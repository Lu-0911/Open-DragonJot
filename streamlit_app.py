import streamlit as st 
import cv2
import numpy as np
import tempfile
import os
import shutil
from pathlib import Path
import json
from collections import deque
from filterpy.kalman import KalmanFilter
import statistics
from PIL import Image, ImageDraw, ImageFont
import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
import time
import psutil
import base64
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration, WebRtcMode
import av
import threading


import streamlit as st
import threading
import psutil

# ------------------ ğŸ‘¥ å¹¶å‘è®¿é—®æ§åˆ¶é€»è¾‘ ------------------

@st.cache_resource
def get_active_sessions():
    """
    å…¨å±€å…±äº«çš„ä¼šè¯è®¡æ•°å™¨ï¼ˆè·¨æ‰€æœ‰ç”¨æˆ· session å…±äº«ï¼‰ã€‚
    """
    return {"count": 0, "lock": threading.Lock()}

MAX_USERS = 1       # åŒæ—¶å…è®¸çš„æœ€å¤§è®¿é—®äººæ•°
MEM_THRESHOLD = 85  # å†…å­˜å ç”¨ä¸Šé™ï¼ˆç™¾åˆ†æ¯”ï¼‰

def check_user_limit():
    """
    æ£€æŸ¥æ˜¯å¦è¶…è¿‡è®¿é—®äººæ•°æˆ–ç³»ç»Ÿèµ„æºé™åˆ¶ã€‚
    """
    sessions = get_active_sessions()

    # ç³»ç»Ÿèµ„æºæ£€æµ‹ï¼ˆé˜²æ­¢ OOMï¼‰
    mem = psutil.virtual_memory().percent
    if mem > MEM_THRESHOLD:
        st.error(f"âš ï¸ æœåŠ¡å™¨èµ„æºç¹å¿™ï¼ˆå†…å­˜ä½¿ç”¨ {mem:.1f}%ï¼‰ï¼Œè¯·ç¨åå†è¯•ã€‚")
        st.stop()

    # äººæ•°æ£€æµ‹
    with sessions["lock"]:
        if sessions["count"] >= MAX_USERS:
            st.error("ğŸš« å½“å‰è®¿é—®äººæ•°å·²æ»¡ï¼Œè¯·ç¨åå†è¯• ğŸ™")
            st.stop()
        else:
            sessions["count"] += 1
            st.session_state["_registered"] = True
            st.session_state["_user_id"] = id(st.session_state)

def release_user():
    """
    ç”¨æˆ·æ–­å¼€æ—¶é‡Šæ”¾å ç”¨çš„è®¿é—®åé¢ã€‚
    """
    sessions = get_active_sessions()
    with sessions["lock"]:
        if sessions["count"] > 0:
            sessions["count"] -= 1
    print("[INFO] å½“å‰åœ¨çº¿ç”¨æˆ·æ•°:", sessions["count"])

# åˆå§‹åŒ–æ—¶æ£€æµ‹ç”¨æˆ·ä¸Šé™
if "_registered" not in st.session_state:
    check_user_limit()

# ç”¨æˆ·å…³é—­æµè§ˆå™¨æˆ–åˆ·æ–°é¡µé¢æ—¶è‡ªåŠ¨å›æ”¶åé¢
st.on_session_end(release_user)

# åœ¨é¡µé¢é¡¶éƒ¨æ˜¾ç¤ºå½“å‰çŠ¶æ€
with st.sidebar:
    sessions = get_active_sessions()
    st.markdown(f"**ğŸ‘¥ å½“å‰åœ¨çº¿ç”¨æˆ·æ•°ï¼š** {sessions['count']} / {MAX_USERS}")


# ---------------------- è·¯å¾„é…ç½® ----------------------
SCRIPT_DIR = Path(os.path.dirname(os.path.abspath(__file__)))
STATIC_DIR = SCRIPT_DIR / "static"
OUTPUT_DIR = SCRIPT_DIR / "temp_output"
MODELS_DIR = SCRIPT_DIR / "model"

for dir_path in [STATIC_DIR, OUTPUT_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

def clean_output_dir():
    if OUTPUT_DIR.exists():
        for item in OUTPUT_DIR.iterdir():
            try:
                if item.is_file():
                    item.unlink()
                elif item.is_dir():
                    shutil.rmtree(item)
            except Exception as e:
                print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

def get_model_path(relative_path):
    """å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„"""
    model_path = Path(relative_path)
    if not model_path.is_absolute():
        model_path = SCRIPT_DIR / model_path
    return str(model_path)

import atexit
atexit.register(clean_output_dir)

# ---------------------- é€šç”¨é…ç½® ----------------------
DRAGON_KEYPOINT_NAMES = [str(i) for i in range(1, 10)]
DRAGON_SKELETON = [[0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8]]
DEFAULT_NODE_COLORS = [
    (0, 0, 255), (0, 128, 255), (0, 255, 255),
    (0, 255, 0), (255, 255, 0), (255, 128, 0),
    (255, 0, 0), (255, 0, 255), (128, 0, 255)
]
DEFAULT_LINE_COLOR = (200, 200, 200)

# ---------------------- GPUç›‘æ§åŠŸèƒ½ ----------------------
class GPUMonitor:
    def __init__(self):
        self.use_gpu = torch.cuda.is_available()
        self.device = torch.device('cuda' if self.use_gpu else 'cpu')
        self.gpu_name = torch.cuda.get_device_name(0) if self.use_gpu else "N/A"
        self.memory_total = self._get_total_memory()
        self.last_usage = 0
        self.last_time = time.time()
        self.fps_list = []
        
    def _get_total_memory(self):
        if self.use_gpu:
            return torch.cuda.get_device_properties(0).total_memory / (1024 **3)
        return 0
        
    def get_memory_usage(self):
        if not self.use_gpu:
            return 0, 0
        memory_used = torch.cuda.memory_allocated(0) / (1024** 3)
        memory_cached = torch.cuda.memory_reserved(0) / (1024 **3)
        return memory_used, memory_cached
        
    def update_fps(self):
        current_time = time.time()
        elapsed = current_time - self.last_time
        if elapsed > 0:
            fps = 1 / elapsed
            self.fps_list.append(fps)
            if len(self.fps_list) > 30:
                self.fps_list.pop(0)
        self.last_time = current_time
        
    def get_average_fps(self):
        return sum(self.fps_list) / len(self.fps_list) if self.fps_list else 0
        
    def get_cpu_usage(self):
        return psutil.cpu_percent()
        
    def get_system_memory_usage(self):
        mem = psutil.virtual_memory()
        return mem.percent

gpu_monitor = GPUMonitor()

# ---------------------- æ»¤æ³¢ç±»å®šä¹‰ ----------------------
class EMAFilter:
    def __init__(self, alpha=0.3):
        self.alpha = alpha
        self.previous_points = None
    
    def update(self, current_points):
        if self.previous_points is None:
            self.previous_points = current_points.copy()
            return current_points
        
        mask = ~np.isnan(current_points)
        smoothed = np.where(mask, 
                           self.alpha * current_points + (1 - self.alpha) * self.previous_points,
                           self.previous_points)
        self.previous_points = smoothed.copy()
        return smoothed

class KalmanFilterWrapper:
    def __init__(self, num_points, dt=0.033, process_noise=0.1, measurement_noise=5.0):
        self.filters = []
        for _ in range(num_points):
            kf = KalmanFilter(dim_x=4, dim_z=2)
            kf.F = np.array([[1, 0, dt, 0],
                            [0, 1, 0, dt],
                            [0, 0, 1, 0],
                            [0, 0, 0, 1]])
            kf.H = np.array([[1, 0, 0, 0],
                            [0, 1, 0, 0]])
            kf.P = np.eye(4) * 1000
            kf.Q = np.eye(4) * process_noise
            kf.R = np.eye(2) * measurement_noise
            self.filters.append(kf)
    
    def update(self, points):
        smoothed_points = []
        for i, (x, y) in enumerate(points):
            if np.isnan(x) or np.isnan(y):
                self.filters[i].predict()
                smoothed = self.filters[i].x[:2]
            else:
                self.filters[i].predict()
                self.filters[i].update([x, y])
                smoothed = self.filters[i].x[:2]
            smoothed_points.append(smoothed)
        return np.array(smoothed_points).squeeze(-1)

# ---------------------- å¹³æ»‘å‡½æ•° ----------------------
def smooth_keypoints(kpts_buffer, conf_buffer=None, method="ewm", weights=None):
    if method == "none":
        if kpts_buffer:
            last_kpts = kpts_buffer[-1]
            last_conf = conf_buffer[-1] if conf_buffer else None
            return last_kpts, last_conf
        else:
            return None, None

    def to_array_with_nan(buffer, shape_expected):
        arr = []
        for k in buffer:
            if k is None:
                arr.append(np.full(shape_expected, np.nan))
            else:
                pad = [(0, shape_expected[i] - k.shape[i]) for i in range(len(shape_expected))]
                arr.append(np.pad(k, pad, mode='constant', constant_values=np.nan))
        return np.array(arr)

    num_frames = len(kpts_buffer)
    max_instances = max([0 if k is None else k.shape[0] for k in kpts_buffer])
    num_kpts = max([0 if k is None else k.shape[1] for k in kpts_buffer])
    shape_expected = (max_instances, num_kpts, 2)
    kpts_arr = to_array_with_nan(kpts_buffer, shape_expected)
    conf_arr = to_array_with_nan(conf_buffer, (max_instances, num_kpts)) if conf_buffer else None

    if method == "mean":
        smoothed_kpts = np.nanmean(kpts_arr, axis=0)
        smoothed_conf = np.nanmean(conf_arr, axis=0) if conf_buffer else None
    elif method == "weighted_mean":
        weights = np.array(weights if weights is not None else np.ones(num_frames)).reshape(-1, 1, 1, 1)
        smoothed_kpts = np.nansum(kpts_arr * weights, axis=0) / np.sum(weights * (~np.isnan(kpts_arr)), axis=0)
        smoothed_conf = np.nansum(conf_arr * weights.squeeze(-1), axis=0) / np.sum(weights.squeeze(-1) * (~np.isnan(conf_arr)), axis=0) if conf_buffer else None
    elif method == "ewm":
        alpha = 2 / (num_frames + 1)
        smoothed_kpts = kpts_arr[0].copy()
        smoothed_conf = conf_arr[0].copy() if conf_buffer else None
        for f in range(1, num_frames):
            smoothed_kpts = alpha * np.nan_to_num(kpts_arr[f]) + (1 - alpha) * smoothed_kpts
            if conf_buffer:
                smoothed_conf = alpha * np.nan_to_num(conf_arr[f]) + (1 - alpha) * smoothed_conf
    else:
        raise ValueError(f"Unknown smoothing method: {method}")

    return smoothed_kpts, smoothed_conf

# ---------------------- åŠ¨ä½œåˆ†ç±»æ¨¡å‹ ----------------------
class PoseCNN(nn.Module):
    def __init__(self, num_classes=6):
        super().__init__()
        self.person_conv1 = nn.Conv1d(3, 32, kernel_size=3, padding=1)
        self.person_conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)
        self.person_len = 170

        self.dragon_conv1 = nn.Conv1d(3, 16, kernel_size=3, padding=1)
        self.dragon_conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)
        self.dragon_len = 9

        self.fc1 = nn.Linear(64 * self.person_len + 32 * self.dragon_len, 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.dropout = nn.Dropout(0.3)

    def forward(self, person_x, dragon_x):
        person_flat = person_x.reshape(person_x.size(0), 10*17, 3)
        dragon_flat = dragon_x.reshape(dragon_x.size(0), 1*9, 3)

        person_x = person_flat.transpose(1, 2)
        dragon_x = dragon_flat.transpose(1, 2)

        p = F.relu(self.person_conv1(person_x))
        p = F.relu(self.person_conv2(p))
        p = p.flatten(start_dim=1)

        d = F.relu(self.dragon_conv1(dragon_x))
        d = F.relu(self.dragon_conv2(d))
        d = d.flatten(start_dim=1)

        x = torch.cat([p, d], dim=1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

def load_classification_model(model_path, device):
    model_path = Path(model_path)
    if not model_path.is_absolute():
        model_path = SCRIPT_DIR / model_path
    
    checkpoint = torch.load(model_path, map_location=device)
    model = PoseCNN(num_classes=len(checkpoint['classes']))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    return model, checkpoint['classes']

def build_class_inputs(person_results, dragon_results, frame_wh,
                       num_person=10, num_person_kpts=17, num_dragon_kpts=9):
    w, h = frame_wh
    person_array = np.zeros((num_person, num_person_kpts, 3), dtype=np.float32)
    dragon_array = np.zeros((1, num_dragon_kpts, 3), dtype=np.float32)

    if person_results is not None and len(person_results) > 0 and person_results[0].keypoints is not None:
        kpts_all = person_results[0].keypoints
        xy = kpts_all.xy.cpu().numpy()
        confs = kpts_all.conf.cpu().numpy()

        mean_conf = np.nanmean(confs, axis=1)
        order = np.argsort(mean_conf)[::-1]

        for i, idx in enumerate(order[:num_person]):
            k_xy = xy[idx]
            k_conf = confs[idx]
            x_norm = (k_xy[:, 0] / float(w)).astype(np.float32)
            y_norm = (k_xy[:, 1] / float(h)).astype(np.float32)
            v = k_conf.astype(np.float32)
            k_stack = np.stack([x_norm, y_norm, v], axis=1)
            L = min(num_person_kpts, k_stack.shape[0])
            person_array[i, :L, :] = k_stack[:L, :]

    if dragon_results is not None and len(dragon_results) > 0 and dragon_results[0].keypoints is not None:
        kpts_all = dragon_results[0].keypoints
        xy = kpts_all.xy.cpu().numpy()
        confs = kpts_all.conf.cpu().numpy()
        boxes = getattr(dragon_results[0], 'boxes', None)
        if boxes is not None and len(boxes) > 0 and hasattr(boxes, 'conf'):
            inst_scores = boxes.conf.cpu().numpy()
            best_idx = int(np.argmax(inst_scores))
        else:
            mean_conf = np.nanmean(confs, axis=1)
            best_idx = int(np.argmax(mean_conf))

        k_xy = xy[best_idx]
        k_conf = confs[best_idx]
        x_norm = (k_xy[:, 0] / float(w)).astype(np.float32)
        y_norm = (k_xy[:, 1] / float(h)).astype(np.float32)
        v = k_conf.astype(np.float32)
        k_stack = np.stack([x_norm, y_norm, v], axis=1)
        L = min(num_dragon_kpts, k_stack.shape[0])
        dragon_array[0, :L, :] = k_stack[:L, :]

    return person_array, dragon_array

def classify_action(model, classes, person_array, dragon_array, device):
    if person_array.shape != (10, 17, 3):
        raise ValueError(f"person_array å½¢çŠ¶é”™è¯¯ï¼Œåº”ä¸º (10, 17, 3)ï¼Œå½“å‰ {person_array.shape}")
    if dragon_array.shape != (1, 9, 3):
        raise ValueError(f"dragon_array å½¢çŠ¶é”™è¯¯ï¼Œåº”ä¸º (1, 9, 3)ï¼Œå½“å‰ {dragon_array.shape}")

    classes_dict = {"BZ": "å…«å­—ç±»", "DC":"å•ä¾§ç±»", "CT": "ç©¿è…¾ç±»", "FG": "ç¿»æ»šç±»", "YL": "æ¸¸é¾™ç±»", "ZX": "é€ å‹ç±»"}
    classes = [classes_dict.get(c, c) for c in classes]

    person_tensor = torch.tensor(person_array, dtype=torch.float32).unsqueeze(0).to(device)
    dragon_tensor = torch.tensor(dragon_array, dtype=torch.float32).unsqueeze(0).to(device)

    with torch.no_grad():
        out = model(person_tensor, dragon_tensor)
        probs = F.softmax(out, dim=1)
        pred = torch.argmax(probs, dim=1).item()

    return classes[pred]

def put_chinese_text(img, text, pos=(30,80), color=(0,255,0), font_size=32):
    try:
        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        try:
            font = ImageFont.truetype("simsun.ttc", font_size)
        except:
            font = ImageFont.load_default()
        draw.text(pos, text, font=font, fill=color)
        img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
    except Exception as e:
        cv2.putText(img, text, pos, cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
    return img


# ---------------------- å›¾åƒæ£€æµ‹ ----------------------
def process_image(person_model, dragon_model, img_path, confs, device,
                  save_json=False, save_txt=False, 
                  single_dragon=True, only_person=False, only_dragon=False,
                  classify=False, classify_model=None, verbose=False,
                  node_colors=None, node_size=10, line_color=None, line_thickness=6):
    node_colors = node_colors or DEFAULT_NODE_COLORS
    line_color = line_color or DEFAULT_LINE_COLOR
    
    person_conf, dragon_conf, person_kpt_conf, dragon_kpt_conf = confs

    img = cv2.imread(img_path)
    if img is None:
        raise FileNotFoundError(f"æ— æ³•è¯»å–å›¾ç‰‡: {img_path}")
    
    person_results = None
    dragon_results = None
    

    person_model_path = Path(person_model)
    if not person_model_path.is_absolute():
        person_model_path = MODELS_DIR / person_model_path
    
    dragon_model_path = Path(dragon_model)
    if not dragon_model_path.is_absolute():
        dragon_model_path = MODELS_DIR / dragon_model_path

    if not only_dragon:
        model_person = YOLO(str(person_model_path))
        model_person.to(device)
        person_results = model_person(img, conf=person_conf, verbose=verbose)

    if not only_person:
        model_dragon = YOLO(str(dragon_model_path))
        model_dragon.to(device)
        dragon_results = model_dragon(img, conf=dragon_conf, verbose=verbose)
    
    if only_dragon or only_person:
        classify = False

    img_out = img.copy()
    if person_results is not None:
        img_out = person_results[0].plot(boxes=False)

    if dragon_results and dragon_results[0].keypoints is not None:
        boxes = dragon_results[0].boxes

        if boxes is not None and len(boxes) > 0:
            kpts = dragon_results[0].keypoints.xy.cpu().numpy()
            conf = dragon_results[0].keypoints.conf.cpu().numpy()
            if single_dragon:
                best_idx = np.argmax(boxes.conf.cpu().numpy())
                kpts = kpts[best_idx:best_idx+1]
                conf = conf[best_idx:best_idx+1]

            for i, kp_set in enumerate(kpts):
                for j, ((x, y), c) in enumerate(zip(kp_set, conf[i])):
                    if c > dragon_kpt_conf:
                        color = node_colors[j % len(node_colors)]
                        cv2.circle(img_out, (int(x), int(y)), node_size, color, -1, lineType=cv2.LINE_AA)
                        cv2.putText(img_out, str(j + 1), (int(x) + 5, int(y) - 5),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)
                for a, b in DRAGON_SKELETON:
                    if conf[i][a] > dragon_kpt_conf and conf[i][b] > dragon_kpt_conf:
                        pt1, pt2 = tuple(map(int, kp_set[a])), tuple(map(int, kp_set[b]))
                        cv2.line(img_out, pt1, pt2, line_color, line_thickness)

    if classify and classify_model is not None:
        classify_model_obj, classes = load_classification_model(MODELS_DIR / classify_model, device)
        num_person, num_person_kpts = 10, 17
        num_dragon_kpts = 9
        person_array = np.zeros((num_person, num_person_kpts, 3), dtype=np.float32)
        dragon_array = np.zeros((1, num_dragon_kpts, 3), dtype=np.float32)

        if person_results is not None and len(person_results[0].keypoints) > 0:
            kpts_all = person_results[0].keypoints
            confs = kpts_all.conf.cpu().numpy()
            xy = kpts_all.xy.cpu().numpy()
            mean_conf = confs.mean(axis=1)
            order = np.argsort(mean_conf)[::-1]
            for i, idx in enumerate(order[:num_person]):
                x = xy[idx, :, 0]
                y = xy[idx, :, 1]
                v = confs[idx]
                person_array[i, :len(x), :] = np.stack([x, y, v], axis=1)

        if dragon_results is not None and len(dragon_results[0].keypoints) > 0:
            kpts = dragon_results[0].keypoints.xy.cpu().numpy()[0]
            conf = dragon_results[0].keypoints.conf.cpu().numpy()[0]
            dragon_array[0, :len(kpts), :] = np.stack([kpts[:, 0], kpts[:, 1], conf], axis=1)

        action_label = classify_action(classify_model_obj, classes, person_array, dragon_array, device)
        img_out = put_chinese_text(img_out, f"Action: {action_label}")

    output_img_path = OUTPUT_DIR / "output_image.jpg"
    cv2.imwrite(str(output_img_path), img_out)

    if save_json:
        if person_results is not None:
            person_labels = json.loads(person_results[0].to_json())
            (OUTPUT_DIR / "person.json").write_text(json.dumps(person_labels, indent=2), encoding='utf-8')
        if dragon_results is not None:
            dragon_labels = json.loads(dragon_results[0].to_json())
            (OUTPUT_DIR / "dragon.json").write_text(json.dumps(dragon_labels, indent=2), encoding='utf-8')

    if save_txt:
        if person_results is not None:
            person_results[0].save_txt(str(OUTPUT_DIR / "person.txt"))
        if dragon_results is not None:
            dragon_results[0].save_txt(str(OUTPUT_DIR / "dragon.txt"))

    return img_out, output_img_path

# ---------------------- è§†é¢‘æ£€æµ‹ ----------------------
def process_video(person_model, dragon_model, video_path, confs, realtime_filter_method, smooth, device,
                  save_json=False, save_txt=False, show_preview=True,
                  single_dragon=True, only_person=False, only_dragon=False, 
                  classify=False, classify_model=None, save_video=False, verbose=False,
                  node_colors=None, node_size=10, line_color=None, line_thickness=6,
                  gpu_monitor=None, status_callback=None):
    node_colors = node_colors or DEFAULT_NODE_COLORS
    line_color = line_color or DEFAULT_LINE_COLOR
    
    person_conf, dragon_conf, person_kpt_conf, dragon_kpt_conf = confs

    person_model_path = Path(person_model)
    if not person_model_path.is_absolute():
        person_model_path = MODELS_DIR / person_model_path
    
    dragon_model_path = Path(dragon_model)
    if not dragon_model_path.is_absolute():
        dragon_model_path = MODELS_DIR / dragon_model_path

    model_person = YOLO(str(person_model_path)).to(device) if not only_dragon else None
    model_dragon = YOLO(str(dragon_model_path)).to(device) if not only_person else None

    realtime_filter = None
    if realtime_filter_method == 'ema':
        realtime_filter = EMAFilter(alpha=0.3)
    elif realtime_filter_method == 'kalman':
        realtime_filter = KalmanFilterWrapper(
            num_points=len(DRAGON_KEYPOINT_NAMES),
            dt=1/30.0,
            process_noise=0.2,
            measurement_noise=10.0
        )

    smooth_window = 3
    frame_id = 0
    dragon_kpts_buffer = deque(maxlen=2 * smooth_window + 1)
    dragon_conf_buffer = deque(maxlen=2 * smooth_window + 1)
    frame_buffer = deque(maxlen=2 * smooth_window + 1)
    
    current_smoothed_kpts = None
    current_smoothed_conf = None

    if only_dragon or only_person:
        classify = False

    classify_model_obj = None
    classes = None
    if classify and classify_model is not None:
        classify_model_obj, classes = load_classification_model(MODELS_DIR / classify_model, device)
        class_buffer = deque(maxlen=30)
        display_class = None

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise FileNotFoundError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

    w, h = int(cap.get(3)), int(cap.get(4))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    output_video_path = None
    
    if save_video:
        fps = cap.get(cv2.CAP_PROP_FPS)
        # ç¡®ä¿FPSæœ‰æ•ˆ
        if fps <= 0:
            fps = 30  # ä½¿ç”¨é»˜è®¤FPS
            
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        output_video_path = OUTPUT_DIR / "output_video.mp4"
        try:
            out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (w, h))
            if not out.isOpened():
                raise Exception(f"æ— æ³•åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨ï¼Œå¯èƒ½æ˜¯ç¼–è§£ç å™¨é—®é¢˜: {output_video_path}")
        except Exception as e:
            st.error(f"è§†é¢‘å†™å…¥åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return None

    # é¢„è§ˆå ä½ç¬¦
    preview_placeholder = st.empty() if show_preview else None
    gpu_status_placeholder = st.empty() if gpu_monitor and gpu_monitor.use_gpu else None

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            # æ›´æ–°è¿›åº¦
            if status_callback:
                progress = min(frame_id / total_frames, 1.0)
                status_callback(f"å¤„ç†è¿›åº¦: {frame_id}/{total_frames} å¸§", progress)

            # æ›´æ–°GPUç›‘æ§
            if gpu_monitor and gpu_status_placeholder:
                gpu_monitor.update_fps()
                mem_used, mem_cached = gpu_monitor.get_memory_usage()
                cpu_usage = gpu_monitor.get_cpu_usage()
                sys_mem_usage = gpu_monitor.get_system_memory_usage()
                avg_fps = gpu_monitor.get_average_fps()
                
                gpu_status = f"""
                **GPUçŠ¶æ€ç›‘æ§**  
                GPU: {gpu_monitor.gpu_name}  
                æ˜¾å­˜ä½¿ç”¨: {mem_used:.2f}GB / {gpu_monitor.memory_total:.2f}GB  
                ç¼“å­˜æ˜¾å­˜: {mem_cached:.2f}GB  
                å¹³å‡å¸§ç‡: {avg_fps:.1f} FPS  
                CPUä½¿ç”¨ç‡: {cpu_usage}%  
                ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡: {sys_mem_usage}%
                """
                gpu_status_placeholder.markdown(gpu_status)

            frame_buffer.append(frame.copy())
            person_results = model_person(frame, conf=person_conf, verbose=verbose) if model_person else None
            dragon_results = model_dragon(frame, conf=dragon_conf, verbose=verbose) if model_dragon else None

            if dragon_results and dragon_results[0].keypoints is not None: 
                boxes = dragon_results[0].boxes
                if boxes is not None and len(boxes) > 0:
                    kpts = dragon_results[0].keypoints.xy.cpu().numpy()
                    conf = dragon_results[0].keypoints.conf.cpu().numpy()
                    if single_dragon:
                        best_idx = np.argmax(boxes.conf.cpu().numpy())
                        kpts = kpts[best_idx:best_idx+1]
                        conf = conf[best_idx:best_idx+1]
                    
                    if realtime_filter is not None and len(kpts) > 0:
                        current_kpts = kpts[0]
                        smoothed_kpts = realtime_filter.update(current_kpts)
                        current_smoothed_kpts = smoothed_kpts.reshape(1, -1, 2)
                        current_smoothed_conf = conf
                    else:
                        current_smoothed_kpts = kpts
                        current_smoothed_conf = conf
                    
                    dragon_kpts_buffer.append(current_smoothed_kpts)
                    dragon_conf_buffer.append(current_smoothed_conf)

            else:
                empty_kpts = np.full((1, len(DRAGON_KEYPOINT_NAMES), 2), np.nan)
                empty_conf = np.full((1, len(DRAGON_KEYPOINT_NAMES)), np.nan)
                dragon_kpts_buffer.append(empty_kpts)
                dragon_conf_buffer.append(empty_conf)
                current_smoothed_kpts = None
                current_smoothed_conf = None

            img = frame.copy()
            if person_results is not None:
                img = person_results[0].plot(boxes=False)

            draw_kpts = None
            draw_conf = None

            if len(dragon_kpts_buffer) == dragon_kpts_buffer.maxlen and smooth != "none":
                smoothed_kpts, smoothed_conf = smooth_keypoints(dragon_kpts_buffer, dragon_conf_buffer, method=smooth)
                if smoothed_kpts is not None and len(smoothed_kpts) > 0:
                    draw_kpts = smoothed_kpts
                    draw_conf = smoothed_conf
            elif current_smoothed_kpts is not None and len(current_smoothed_kpts) > 0:
                draw_kpts = current_smoothed_kpts
                draw_conf = current_smoothed_conf
            elif dragon_kpts_buffer:
                draw_kpts = dragon_kpts_buffer[-1]
                draw_conf = dragon_conf_buffer[-1] if dragon_conf_buffer else None

            if draw_kpts is not None and len(draw_kpts) > 0:
                kp_set, conf_set = draw_kpts[0], draw_conf[0]
                for j, ((x, y), c) in enumerate(zip(kp_set, conf_set)):
                    if not np.isnan(x) and not np.isnan(y) and c > dragon_kpt_conf:
                        color = node_colors[j % len(node_colors)]
                        cv2.circle(img, (int(x), int(y)), node_size, color, -1)
                        cv2.putText(img, str(j + 1), (int(x)+5, int(y)-5),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                for a, b in DRAGON_SKELETON:
                    if (conf_set[a] > dragon_kpt_conf and conf_set[b] > dragon_kpt_conf and
                        not np.isnan(kp_set[a][0]) and not np.isnan(kp_set[b][0])):
                        cv2.line(img, tuple(map(int, kp_set[a])), tuple(map(int, kp_set[b])), line_color, line_thickness)

                if classify and classes is not None and draw_kpts is not None:
                    try:
                        frame_wh = (w, h)
                        person_array, dragon_array = build_class_inputs(person_results, dragon_results, frame_wh)
                        label = classify_action(classify_model_obj, classes, person_array, dragon_array, device)
                    except Exception as e:
                        label = None

                    if label is not None:
                        class_buffer.append(label)

                    if len(class_buffer) == 30:
                        stable_class = statistics.mode(class_buffer)
                        display_class = stable_class

            if classify and display_class is not None:
                img = put_chinese_text(img, f"Action: {display_class}")

            # å®æ—¶é¢„è§ˆ
            if show_preview and preview_placeholder is not None:
                preview_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                preview_placeholder.image(preview_img, caption=f"å¸§ {frame_id}", width='stretch')

            if save_video and 'out' in locals() and out.isOpened():
                try:
                    out.write(img)
                except Exception as e:
                    st.error(f"å†™å…¥è§†é¢‘å¸§å¤±è´¥ (å¸§ {frame_id}): {str(e)}")
                    st.error(f"å¸§å°ºå¯¸: {img.shape}, è§†é¢‘å°ºå¯¸: ({w}, {h})")

            frame_id += 1

    except Exception as e:
        st.error(f"è§†é¢‘å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
    finally:
        if save_video and 'out' in locals():
            # ç¡®ä¿æ‰€æœ‰å¸§éƒ½è¢«å†™å…¥
            for i in range(smooth_window):
                if i < len(frame_buffer):
                    try:
                        out.write(frame_buffer[i])
                    except:
                        pass
            out.release()
            # ç¡®ä¿æ–‡ä»¶æ­£ç¡®å…³é—­
            cv2.destroyAllWindows()
            
            # éªŒè¯è§†é¢‘æ–‡ä»¶
            if output_video_path.exists():
                cap_check = cv2.VideoCapture(str(output_video_path))
                if not cap_check.isOpened():
                    st.error(f"ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶æ— æ³•æ‰“å¼€ï¼Œå¯èƒ½å·²æŸå: {output_video_path}")
                else:
                    check_ret, _ = cap_check.read()
                    if not check_ret:
                        st.error(f"ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶ä¸ºç©ºæˆ–æŸåï¼Œæ— æ³•è¯»å–å¸§: {output_video_path}")
                    cap_check.release()
            else:
                st.error(f"è§†é¢‘æ–‡ä»¶æœªç”Ÿæˆ: {output_video_path}")

        cap.release()
        if gpu_status_placeholder:
            gpu_status_placeholder.empty()
            
    return output_video_path

# ---------------------- æ‘„åƒå¤´æ£€æµ‹ ----------------------
def process_camera(person_model, dragon_model, cam_id, confs, realtime_filter_method, device,
                   single_dragon=True, only_person=False, only_dragon=False,
                   classify=False, classify_model=None, save_video=False,
                   preview_placeholder=None, stop_flag=None,
                   node_colors=None, node_size=10, line_color=None, line_thickness=6,
                   gpu_monitor=None):
    node_colors = node_colors or DEFAULT_NODE_COLORS
    line_color = line_color or DEFAULT_LINE_COLOR
    
    person_conf, dragon_conf, person_kpt_conf, dragon_kpt_conf = confs
    
    person_model_path = Path(person_model)
    if not person_model_path.is_absolute():
        person_model_path = MODELS_DIR / person_model_path
    
    dragon_model_path = Path(dragon_model)
    if not dragon_model_path.is_absolute():
        dragon_model_path = MODELS_DIR / dragon_model_path

    model_person = YOLO(str(person_model_path)).to(device) if not only_dragon else None
    model_dragon = YOLO(str(dragon_model_path)).to(device) if not only_person else None

    realtime_filter = None
    if realtime_filter_method == 'ema':
        realtime_filter = EMAFilter(alpha=0.4)
    elif realtime_filter_method == 'kalman':
        realtime_filter = KalmanFilterWrapper(
            num_points=len(DRAGON_KEYPOINT_NAMES),
            dt=1/30.0,
            process_noise=0.1,
            measurement_noise=5.0
        )

    if only_dragon or only_person:
        classify = False

    classify_model_obj = None
    classes = None
    if classify and classify_model is not None:
        classify_model_obj, classes = load_classification_model(MODELS_DIR / classify_model, device)
        class_buffer = deque(maxlen=30)
        display_class = None

    cap = cv2.VideoCapture(cam_id)
    if not cap.isOpened():
        preview_placeholder.error(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {cam_id}ï¼Œè¯·æ£€æŸ¥æ‘„åƒå¤´è¿æ¥")
        return
    
    # è§†é¢‘å½•åˆ¶è®¾ç½®
    video_writer = None
    output_video_path = None
    if save_video:
        output_video_path = OUTPUT_DIR / "camera_output.mp4"
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0:
            fps = 20  # ä½¿ç”¨é»˜è®¤FPS
        
        try:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            video_writer = cv2.VideoWriter(str(output_video_path), fourcc, fps, (frame_width, frame_height))
            if not video_writer.isOpened():
                raise Exception(f"æ— æ³•åˆå§‹åŒ–æ‘„åƒå¤´è§†é¢‘å†™å…¥å™¨ï¼Œå¯èƒ½æ˜¯ç¼–è§£ç å™¨é—®é¢˜")
        except Exception as e:
            st.error(f"æ‘„åƒå¤´è§†é¢‘å†™å…¥åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            save_video = False

    # çŠ¶æ€æ˜¾ç¤º
    status_text = st.empty()
    status_text.info("æ‘„åƒå¤´è¿è¡Œä¸­...")
    
    # GPUç›‘æ§å ä½ç¬¦
    gpu_status_placeholder = st.empty() if gpu_monitor and gpu_monitor.use_gpu else None

    try:
        while True:
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if stop_flag and stop_flag():
                break
                
            # æ›´æ–°GPUç›‘æ§
            if gpu_monitor and gpu_status_placeholder:
                gpu_monitor.update_fps()
                mem_used, mem_cached = gpu_monitor.get_memory_usage()
                cpu_usage = gpu_monitor.get_cpu_usage()
                sys_mem_usage = gpu_monitor.get_system_memory_usage()
                avg_fps = gpu_monitor.get_average_fps()
                
                gpu_status = f"""
                **GPUçŠ¶æ€ç›‘æ§**  
                GPU: {gpu_monitor.gpu_name}  
                æ˜¾å­˜ä½¿ç”¨: {mem_used:.2f}GB / {gpu_monitor.memory_total:.2f}GB  
                ç¼“å­˜æ˜¾å­˜: {mem_cached:.2f}GB  
                å¹³å‡å¸§ç‡: {avg_fps:.1f} FPS  
                CPUä½¿ç”¨ç‡: {cpu_usage}%  
                ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡: {sys_mem_usage}%
                """
                gpu_status_placeholder.markdown(gpu_status)

            ret, frame = cap.read()
            if not ret:
                preview_placeholder.error("æ— æ³•ä»æ‘„åƒå¤´è¯»å–å¸§")
                break
                
            person_results = model_person(frame, conf=person_conf, verbose=False) if model_person else None
            dragon_results = model_dragon(frame, conf=dragon_conf, verbose=False) if model_dragon else None

            img = frame.copy()
            if person_results is not None:
                img = person_results[0].plot(boxes=False)

            if dragon_results and dragon_results[0].keypoints is not None:
                boxes = dragon_results[0].boxes
                if boxes is not None and len(boxes) > 0:
                    kpts = dragon_results[0].keypoints.xy.cpu().numpy()
                    conf = dragon_results[0].keypoints.conf.cpu().numpy()
                    if single_dragon:
                        best_idx = np.argmax(boxes.conf.cpu().numpy())
                        kpts = kpts[best_idx:best_idx+1]
                        conf = conf[best_idx:best_idx+1]
                    
                    if realtime_filter is not None and len(kpts) > 0:
                        current_kpts = kpts[0]
                        smoothed_kpts = realtime_filter.update(current_kpts)
                        kpts[0] = smoothed_kpts.reshape(1, -1, 2)
                    
                    for i, kp_set in enumerate(kpts):
                        for j, ((x, y), c) in enumerate(zip(kp_set, conf[i])):
                            if c > dragon_kpt_conf:
                                color = node_colors[j % len(node_colors)]
                                cv2.circle(img, (int(x), int(y)), node_size, color, -1)
                                cv2.putText(img, str(j + 1), (int(x)+5, int(y)-5),
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                        for (a, b) in DRAGON_SKELETON:
                            if conf[i][a] > dragon_kpt_conf and conf[i][b] > dragon_kpt_conf:
                                pt1, pt2 = tuple(map(int, kp_set[a])), tuple(map(int, kp_set[b]))
                                cv2.line(img, pt1, pt2, line_color, line_thickness)
                            
                    if classify and classes is not None:
                        try:
                            frame_wh = (frame.shape[1], frame.shape[0])
                            person_array, dragon_array = build_class_inputs(person_results, dragon_results, frame_wh)
                            label = classify_action(classify_model_obj, classes, person_array, dragon_array, device)
                        except Exception as e:
                            label = None

                        if label is not None:
                            class_buffer.append(label)

                        if len(class_buffer) == 30:
                            stable_class = statistics.mode(class_buffer)
                            display_class = stable_class
            
            if classify and display_class is not None:
                img = put_chinese_text(img, f"Action: {display_class}")

            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            preview_placeholder.image(img_rgb, channels="RGB", width='stretch')

            if save_video and video_writer is not None and video_writer.isOpened():
                try:
                    video_writer.write(img)
                except Exception as e:
                    st.error(f"å†™å…¥æ‘„åƒå¤´å¸§å¤±è´¥: {str(e)}")
                    st.error(f"å¸§å°ºå¯¸: {img.shape}, è§†é¢‘å°ºå¯¸: ({frame_width}, {frame_height})")

            # å°å»¶è¿Ÿé˜²æ­¢UIå¡é¡¿
            time.sleep(0.01)

    except Exception as e:
        preview_placeholder.error(f"æ‘„åƒå¤´å¤„ç†å‡ºé”™: {str(e)}")
    
    finally:
        cap.release()
        if video_writer is not None:
            video_writer.release()
            cv2.destroyAllWindows()
            
            # éªŒè¯è§†é¢‘æ–‡ä»¶
            if output_video_path and output_video_path.exists():
                cap_check = cv2.VideoCapture(str(output_video_path))
                if not cap_check.isOpened():
                    st.error(f"ç”Ÿæˆçš„æ‘„åƒå¤´è§†é¢‘æ–‡ä»¶æ— æ³•æ‰“å¼€ï¼Œå¯èƒ½å·²æŸå: {output_video_path}")
                else:
                    check_ret, _ = cap_check.read()
                    if not check_ret:
                        st.error(f"ç”Ÿæˆçš„æ‘„åƒå¤´è§†é¢‘æ–‡ä»¶ä¸ºç©ºæˆ–æŸåï¼Œæ— æ³•è¯»å–å¸§: {output_video_path}")
                    cap_check.release()
            elif save_video:
                st.error(f"æ‘„åƒå¤´è§†é¢‘æ–‡ä»¶æœªç”Ÿæˆ: {output_video_path}")
        
        status_text.success("æ‘„åƒå¤´å·²åœæ­¢")
        if gpu_status_placeholder:
            gpu_status_placeholder.empty()
            
        return output_video_path
    

def process_camera_stream(params, gpu_monitor=None):
    """å®Œæ•´æ‘„åƒå¤´è¯†åˆ«é€»è¾‘"""
    RTC_CONFIGURATION = RTCConfiguration({
        "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
    })

    class VideoProcessor(VideoProcessorBase):
        def __init__(self):
            # --- åˆå§‹åŒ–å‚æ•° ---
            self.model_person = None
            self.model_dragon = None
            self.device = params['device']
            self.node_colors = params['node_colors']
            self.line_color = params['line_color']
            self.node_size = params['node_size']
            self.line_thickness = params['line_thickness']
            self.person_conf, self.dragon_conf, self.person_kpt_conf, self.dragon_kpt_conf = params['confs']
            self.single_dragon = params.get('single_dragon', True)
            self.only_person = params.get('only_person', False)
            self.only_dragon = params.get('only_dragon', False)

            # --- åˆ†ç±»æ¨¡å‹ ---
            self.classify = params.get('classify', False)
            self.classify_model = params.get('classify_model', None)
            self.classify_model_obj = None
            self.classes = None
            self.class_buffer = deque(maxlen=30)
            self.display_class = None

            # --- å®æ—¶å¹³æ»‘æ»¤æ³¢å™¨ ---
            self.realtime_filter_method = params.get('realtime_filter_method', None)
            if self.realtime_filter_method == 'ema':
                self.realtime_filter = EMAFilter(alpha=0.4)
            elif self.realtime_filter_method == 'kalman':
                self.realtime_filter = KalmanFilterWrapper(
                    num_points=len(DRAGON_KEYPOINT_NAMES),
                    dt=1 / 30.0,
                    process_noise=0.1,
                    measurement_noise=5.0,
                )
            else:
                self.realtime_filter = None

            # --- è§†é¢‘ä¿å­˜ ---
            self.save_video = params.get('save_video', False)
            self.output_video_path = OUTPUT_DIR / "camera_output.mp4"
            self.video_writer = None
            self.fps = 0.0
            self.last_time = time.time()

        def _init_models(self):
            """æ‡’åŠ è½½æ¨¡å‹"""
            if self.model_person is None and not self.only_dragon:
                person_model_path = Path(params['person_model'])
                if not person_model_path.is_absolute():
                    person_model_path = MODELS_DIR / person_model_path
                self.model_person = YOLO(str(person_model_path)).to(self.device)

            if self.model_dragon is None and not self.only_person:
                dragon_model_path = Path(params['dragon_model'])
                if not dragon_model_path.is_absolute():
                    dragon_model_path = MODELS_DIR / dragon_model_path
                self.model_dragon = YOLO(str(dragon_model_path)).to(self.device)

            if self.classify and self.classify_model_obj is None and self.classify_model:
                self.classify_model_obj, self.classes = load_classification_model(
                    MODELS_DIR / self.classify_model, self.device
                )

            print("[INFO] æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¯å¼€å§‹å®æ—¶æ£€æµ‹")

        def _init_video_writer(self, frame_shape):
            if not self.save_video:
                return
            h, w = frame_shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            self.video_writer = cv2.VideoWriter(str(self.output_video_path), fourcc, 20.0, (w, h))

        def recv(self, frame):
            # --- æ¨¡å‹åˆå§‹åŒ– ---
            self._init_models()
            img = frame.to_ndarray(format="bgr24")

            # --- FPS è®¡ç®— ---
            now = time.time()
            dt = now - self.last_time
            if dt > 0:
                self.fps = 1.0 / dt
            self.last_time = now

            # --- äººä½“æ£€æµ‹ ---
            results_person = None
            if self.model_person:
                results_person = self.model_person(img, conf=self.person_conf, verbose=False)
                img = results_person[0].plot(boxes=False)

            # --- é¾™æ£€æµ‹ ---
            results_dragon = None
            if self.model_dragon:
                results_dragon = self.model_dragon(img, conf=self.dragon_conf, verbose=False)

            # --- é¾™å…³é”®ç‚¹ç»˜åˆ¶ ---
            if results_dragon and results_dragon[0].keypoints is not None:
                boxes = results_dragon[0].boxes
                kpts = results_dragon[0].keypoints.xy.cpu().numpy()
                conf = results_dragon[0].keypoints.conf.cpu().numpy()

                if self.single_dragon and boxes is not None and len(boxes) > 0:
                    best_idx = np.argmax(boxes.conf.cpu().numpy())
                    kpts = kpts[best_idx:best_idx + 1]
                    conf = conf[best_idx:best_idx + 1]

                if self.realtime_filter is not None and len(kpts) > 0:
                    smoothed = self.realtime_filter.update(kpts[0])
                    kpts[0] = smoothed.reshape(-1, 2)

                for i, kp_set in enumerate(kpts):
                    for j, ((x, y), c) in enumerate(zip(kp_set, conf[i])):
                        if c > self.dragon_kpt_conf:
                            color = self.node_colors[j % len(self.node_colors)]
                            cv2.circle(img, (int(x), int(y)), self.node_size, color, -1)
                            cv2.putText(img, str(j + 1), (int(x)+5, int(y)-5),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                    for (a, b) in DRAGON_SKELETON:
                        if conf[i][a] > self.dragon_kpt_conf and conf[i][b] > self.dragon_kpt_conf:
                            pt1, pt2 = tuple(map(int, kp_set[a])), tuple(map(int, kp_set[b]))
                            cv2.line(img, pt1, pt2, self.line_color, self.line_thickness)

            # --- åŠ¨ä½œåˆ†ç±» ---
            if self.classify and self.classify_model_obj is not None:
                try:
                    frame_wh = (img.shape[1], img.shape[0])
                    person_array, dragon_array = build_class_inputs(results_person, results_dragon, frame_wh)
                    label = classify_action(self.classify_model_obj, self.classes, person_array, dragon_array, self.device)
                    if label is not None:
                        self.class_buffer.append(label)
                        if len(self.class_buffer) == self.class_buffer.maxlen:
                            stable_class = statistics.mode(self.class_buffer)
                            if stable_class != self.display_class:
                                self.display_class = stable_class
                except Exception:
                    pass

                if self.display_class:
                    img = put_chinese_text(
                        img, 
                        f"Action:{self.display_class}",
                        pos=(30, 60),
                        color=(0, 255, 0),
                        font_size=32
                    )

            # --- è§†é¢‘ä¿å­˜ ---
            if self.save_video and self.video_writer is None:
                self._init_video_writer(img.shape)
            if self.video_writer:
                try:
                    self.video_writer.write(img)
                except Exception as e:
                    print(f"[WARN] å†™å…¥è§†é¢‘å¤±è´¥: {e}")

            # --- GPUçŠ¶æ€ç›‘æ§ï¼ˆå¯é€‰ï¼‰---
            if gpu_monitor:
                gpu_monitor.update_fps()
                if gpu_monitor.use_gpu:
                    mem_used, mem_cached = gpu_monitor.get_memory_usage()
                    print(f"[GPU] FPS {self.fps:.1f}, Mem {mem_used:.2f} GB")
                else:
                    print(f"[CPU] FPS {self.fps:.1f}")

            return av.VideoFrame.from_ndarray(img, format="bgr24")

        def on_stop(self):
            if self.video_writer:
                self.video_writer.release()
                print("[INFO] å½•åˆ¶è§†é¢‘å·²ä¿å­˜:", self.output_video_path)

    # --- å¯åŠ¨ WebRTC ---
    webrtc_streamer(
        key="camera",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=RTC_CONFIGURATION,
        video_processor_factory=VideoProcessor,
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )



     
# ---------------------- æ£€æµ‹å¯ç”¨æ‘„åƒå¤´ ----------------------
@st.cache_resource(show_spinner=False)
def get_available_cameras(max_test=5):
    available_cameras = []
    for i in range(max_test):
        cap = cv2.VideoCapture(i)
        if cap.isOpened():
            ret, frame = cap.read()
            if ret:
                available_cameras.append(i)
            cap.release()
    return available_cameras


# ---------------------- Streamlit Webç•Œé¢ ----------------------

def main():
    st.set_page_config(
        page_title="Open DragonJot - èˆé¾™åŠ¨ä½œè¯†åˆ«æ£€æµ‹ç³»ç»Ÿ",
        page_icon="ğŸ‰",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'current_tab' not in st.session_state:
        st.session_state.current_tab = "åŠŸèƒ½æ¼”ç¤º"
    if 'analysis_running' not in st.session_state:
        st.session_state.analysis_running = False
    if 'stop_flag' not in st.session_state:
        st.session_state.stop_flag = False
    if 'output_video_path' not in st.session_state:
        st.session_state.output_video_path = None

    # ä¾§è¾¹æ  - é¡¶éƒ¨Logoå’Œæ–‡å­—
    st.sidebar.markdown("""
    <div style="text-align: center; padding: 20px 0;">
    """, unsafe_allow_html=True)
    
    try:
        st.sidebar.image(str(STATIC_DIR / "logo.jpg"), width='stretch')
    except Exception as e:
        st.sidebar.warning(f"æœªæ‰¾åˆ°Logoæ–‡ä»¶: {STATIC_DIR / 'logo.jpg'}")
    
    # æ·»åŠ Logoä¸‹æ–¹çš„æ–‡å­—ä¿¡æ¯
    st.sidebar.markdown("""
    <h3 style="text-align: center; margin-top: 10px; margin-bottom: 5px;">Open DragonJot</h3>
    <p style="text-align: center; margin-top: 0; color: #666;">ç‚¹ç›AIå¼€æºç‰ˆ</p>
    </div>
    """, unsafe_allow_html=True)

    # ä¾§è¾¹æ  - é€‰é¡¹å¡é€‰æ‹©
    st.sidebar.title("å¯¼èˆªèœå•")
    # ä¸ºæ¯ä¸ªæŒ‰é’®æ·»åŠ å”¯ä¸€çš„keyå‚æ•°
    if st.sidebar.button("åŠŸèƒ½æ¼”ç¤º", width='stretch', 
                         type="primary" if st.session_state.current_tab == "åŠŸèƒ½æ¼”ç¤º" else "secondary",
                         key="btn_feature_demo"):
        st.session_state.current_tab = "åŠŸèƒ½æ¼”ç¤º"
    
    if st.sidebar.button("å¿«é€Ÿä½“éªŒ", width='stretch', 
                         type="primary" if st.session_state.current_tab == "å¿«é€Ÿä½“éªŒ" else "secondary",
                         key="btn_quick_start"):
        st.session_state.current_tab = "å¿«é€Ÿä½“éªŒ"
    
    # æ–°å¢å‚æ•°è®¾ç½®ç‹¬ç«‹æ¿å—
    if st.sidebar.button("å‚æ•°è®¾ç½®", width='stretch', 
                         type="primary" if st.session_state.current_tab == "å‚æ•°è®¾ç½®" else "secondary",
                         key="btn_settings"):
        st.session_state.current_tab = "å‚æ•°è®¾ç½®"
    
    # æ–°å¢å¼€å‘å›¢é˜Ÿæ¿å—
    if st.sidebar.button("å¼€å‘å›¢é˜Ÿ", width='stretch', 
                         type="primary" if st.session_state.current_tab == "å¼€å‘å›¢é˜Ÿ" else "secondary",
                         key="btn_team"):
        st.session_state.current_tab = "å¼€å‘å›¢é˜Ÿ"

    # åŠŸèƒ½æ¼”ç¤ºé¡µé¢
    if st.session_state.current_tab == "åŠŸèƒ½æ¼”ç¤º":
        st.markdown("<h1 style='text-align: center;'>Open DragonJot - åŠŸèƒ½æ¼”ç¤º</h1>", unsafe_allow_html=True)
        
        # å±•ç¤ºé™æ€æ–‡ä»¶å¤¹ä¸­çš„GIF
        st.subheader("  1. äººé¾™ä¸€ä½“å§¿æ€è¯†åˆ«")
        try:
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                # è¯»å– GIF æ–‡ä»¶å¹¶ç¼–ç 
                gif_path = STATIC_DIR / "skeleton_demo.gif"
                if os.path.exists(gif_path):
                    with open(gif_path, "rb") as f:
                        gif_data = f.read()
                        gif_base64 = base64.b64encode(gif_data).decode()
                    
                    st.markdown(
                        f"""
                        <div style="display: flex; justify-content: center; flex-direction: column; align-items: center;">
                            <img src="data:image/gif;base64,{gif_base64}" style="width: 450px; max-width: 100%;">
                            <p style="text-align: center; font-size: 14px; color: #6B7280;">æœ±ä¿Šé¹æ•™ç»ƒåœ¨æœ±å®¶è§’æ¯å¼€åœºè¡¨æ¼”</p>
                            <p style="text-align: center; font-size: 14px; color: #6B7280;">é¾™éª¨æ¶å…³é”®ç‚¹å®æ—¶è¯†åˆ«æ¼”ç¤º</p>
                        </div>
                        """, 
                        unsafe_allow_html=True
                    )
                else:
                    st.warning(f"æœªæ‰¾åˆ°æ¼”ç¤ºæ–‡ä»¶: {gif_path}")
        except Exception as e:
            st.warning(f"åŠ è½½æ¼”ç¤ºæ–‡ä»¶å‡ºé”™: {e}")

        st.subheader("  2. äº”å¤§åŠ¨ä½œåˆ†ç±»")
        try:
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                # è¯»å– GIF æ–‡ä»¶å¹¶ç¼–ç 
                gif_path = STATIC_DIR / "classification_demo.gif"
                if os.path.exists(gif_path):
                    with open(gif_path, "rb") as f:
                        gif_data = f.read()
                        gif_base64 = base64.b64encode(gif_data).decode()
                    
                    st.markdown(
                        f"""
                        <div style="display: flex; justify-content: center; flex-direction: column; align-items: center;">
                            <img src="data:image/gif;base64,{gif_base64}" style="width: 450px; max-width: 100%;">
                            <p style="text-align: center; font-size: 14px; color: #6B7280;">å¤æ—¦é¾™ç‹®åä¼šæ—¥å¸¸è®­ç»ƒ</p>
                            <p style="text-align: center; font-size: 14px; color: #6B7280;">èˆé¾™åŠ¨ä½œè‡ªåŠ¨åˆ†ç±»æ¼”ç¤º</p>
                        </div>
                        """, 
                        unsafe_allow_html=True
                    )
                else:
                    st.warning(f"æœªæ‰¾åˆ°æ¼”ç¤ºæ–‡ä»¶: {gif_path}")
        except Exception as e:
            st.warning(f"åŠ è½½æ¼”ç¤ºæ–‡ä»¶å‡ºé”™: {e}")

        # st.subheader("  3. åŠ¨ä½œå®æ—¶è¯„åˆ†")
        # try:
        #     col1, col2, col3 = st.columns([1, 2, 1])
        #     with col2:
        #         # è¯»å– GIF æ–‡ä»¶å¹¶ç¼–ç 
        #         gif_path = STATIC_DIR / "score_demo.gif"
        #         if os.path.exists(gif_path):
        #             with open(gif_path, "rb") as f:
        #                 gif_data = f.read()
        #                 gif_base64 = base64.b64encode(gif_data).decode()
                    
        #             st.markdown(
        #                 f"""
        #                 <div style="display: flex; justify-content: center; flex-direction: column; align-items: center;">
        #                     <img src="data:image/gif;base64,{gif_base64}" style="width: 450px; max-width: 100%;">
        #                     <p style="text-align: center; font-size: 14px; color: #6B7280;">ç‚¹ç›AIä¸“ä¸šç‰ˆ</p>
        #                     <p style="text-align: center; margin-top: 10px; font-size: 14px; color: #6B7280;">èˆé¾™åŠ¨ä½œAIæ‰“åˆ†æ¼”ç¤º</p>
        #                 </div>
        #                 """, 
        #                 unsafe_allow_html=True
        #             )
        #         else:
        #             st.warning(f"æœªæ‰¾åˆ°æ¼”ç¤ºæ–‡ä»¶: {gif_path}")
        # except Exception as e:
        #     st.warning(f"åŠ è½½æ¼”ç¤ºæ–‡ä»¶å‡ºé”™: {e}")
        
        st.markdown("""
        ### åŠŸèƒ½è¯´æ˜
        - ç³»ç»Ÿå¯å®æ—¶è¯†åˆ«é¾™éª¨æ¶çš„9ä¸ªå…³é”®èŠ‚ç‚¹
        - æ”¯æŒäº”å¤§ç±»èˆé¾™åŠ¨ä½œçš„è‡ªåŠ¨åˆ†ç±»
        - æ”¯æŒå›¾åƒã€è§†é¢‘ã€æ‘„åƒå¤´è¾“å…¥
        - æä¾›èŠ‚ç‚¹é¢œè‰²ã€å¤§å°ç­‰ä¸ªæ€§åŒ–è®¾ç½®
        - å¯ä½¿ç”¨GPUåŠ é€Ÿæé«˜å¤„ç†æ•ˆç‡
        
        ç‚¹å‡»å·¦ä¾§"å¿«é€Ÿä½“éªŒ"å¼€å§‹ä½¿ç”¨ç³»ç»ŸåŠŸèƒ½ã€‚
        """)

        st.markdown("""
        `å¼€æºç‰ˆæœ¬æš‚æœªå¼€æ”¾åŠ¨ä½œè¯„åˆ†åŠŸèƒ½ï¼Œæœ‰å…´è¶£å¯è®¿é—®ç‚¹ç›AIä¸“ä¸šç‰ˆ`
        `å’¨è¯¢çƒ­çº¿ï¼š15398180360`
        """)

    # å¿«é€Ÿä½“éªŒé¡µé¢
    elif st.session_state.current_tab == "å¿«é€Ÿä½“éªŒ":
        st.markdown("<h1 style='text-align: center;'>Open DragonJot</h1>", unsafe_allow_html=True)
        
        # æ¨ªå‘æ¨¡å¼é€‰æ‹©
        mode = st.radio(
            "é€‰æ‹©æ£€æµ‹æ¨¡å¼:",
            ["ğŸ“· å›¾ç‰‡æ¨¡å¼", "ğŸ¥ è§†é¢‘æ¨¡å¼", "ğŸ“¹ æ‘„åƒå¤´æ¨¡å¼"],
            horizontal=True,
            label_visibility="collapsed"
        )
        
        # ç®€åŒ–æ¨¡å¼æ˜ å°„
        mode_mapping = {
            "ğŸ“· å›¾ç‰‡æ¨¡å¼": "image",
            "ğŸ¥ è§†é¢‘æ¨¡å¼": "video", 
            "ğŸ“¹ æ‘„åƒå¤´æ¨¡å¼": "camera"
        }
        current_mode = mode_mapping[mode]
        
        # ä¸»å†…å®¹åŒºåŸŸ - å›¾ç‰‡æ¨¡å¼
        if current_mode == "image":
            uploaded_file = st.file_uploader("é€‰æ‹©å›¾ç‰‡æ–‡ä»¶", type=['jpg', 'jpeg', 'png'], key="upload_image")
            
            if uploaded_file is not None:
                # æ˜¾ç¤ºé¢„è§ˆå’Œç»“æœåœ¨åŒä¸€åŒºåŸŸ
                preview_placeholder = st.empty()
                
                # ä¸´æ—¶ä¿å­˜æ–‡ä»¶å¹¶æ˜¾ç¤ºåŸå›¾é¢„è§ˆ
                with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    img_path = tmp_file.name
                
                # æ˜¾ç¤ºåŸå›¾é¢„è§ˆ
                original_img = cv2.imread(img_path)
                preview_placeholder.image(
                    cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB), 
                    caption="åŸå›¾é¢„è§ˆ",
                    width='stretch'
                )
                
                if st.button("å¼€å§‹æ£€æµ‹å›¾ç‰‡", width='stretch', type="primary", key="btn_process_image"):
                    with st.spinner("æ­£åœ¨å¤„ç†å›¾ç‰‡..."):
                        # è·å–å‚æ•°è®¾ç½®
                        params = get_params()
                        
                        # æ˜¾ç¤ºGPUçŠ¶æ€ï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
                        gpu_status_placeholder = None
                        if params['device'].type == 'cuda':
                            gpu_status_placeholder = st.empty()
                            mem_used, mem_cached = gpu_monitor.get_memory_usage()
                            gpu_status = f"""
                            **GPUçŠ¶æ€**  
                            æ˜¾å­˜ä½¿ç”¨: {mem_used:.2f}GB / {gpu_monitor.memory_total:.2f}GB  
                            """
                            gpu_status_placeholder.markdown(gpu_status)
                        
                        result_img, output_img_path = process_image(
                            params['person_model'], params['dragon_model'], img_path, params['confs'], params['device'],
                            save_json=params['save_json'], save_txt=params['save_txt'],
                            single_dragon=params['single_dragon'], only_person=params['only_person'], only_dragon=params['only_dragon'],
                            classify=params['classify'], classify_model=params['classify_model'], verbose=False,
                            node_colors=params['node_colors'], node_size=params['node_size'],
                            line_color=params['line_color'], line_thickness=params['line_thickness']
                        )
                        
                        if gpu_status_placeholder:
                            gpu_status_placeholder.empty()
                        
                        # æ›´æ–°æ˜¾ç¤ºç»“æœ
                        preview_placeholder.image(
                            cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), 
                            caption="æ£€æµ‹ç»“æœ",
                            width='stretch'
                        )
                        
                        # æç¤ºç”¨æˆ·åŠæ—¶ä¿å­˜
                        st.success("å¤„ç†å®Œæˆï¼è¯·åŠæ—¶ä¿å­˜è¾“å‡ºç»“æœï¼Œå…³é—­æµè§ˆå™¨åæ–‡ä»¶å°†è‡ªåŠ¨æ¸…é™¤ã€‚")
                        
                        # æä¾›ä¸‹è½½é€‰é¡¹
                        download_cols = st.columns(4)
                        
                        if params['save_json']:
                            with download_cols[0]:
                                with open(OUTPUT_DIR / "person.json", "rb") as file:
                                    st.download_button(
                                        label="ğŸ“¥ äººä½“JSON",
                                        data=file,
                                        file_name="person_detection.json",
                                        mime="application/json",
                                        width='stretch',
                                        key="download_person_json"
                                    )
                            with download_cols[1]:
                                with open(OUTPUT_DIR / "dragon.json", "rb") as file:
                                    st.download_button(
                                        label="ğŸ“¥ é¾™éª¨æ¶JSON",
                                        data=file,
                                        file_name="dragon_detection.json",
                                        mime="application/json",
                                        width='stretch',
                                        key="download_dragon_json"
                                    )
                        
                        if params['save_txt']:
                            with download_cols[2]:
                                with open(OUTPUT_DIR / "person.txt", "rb") as file:
                                    st.download_button(
                                        label="ğŸ“¥ äººä½“TXT",
                                        data=file,
                                        file_name="person_detection.txt",
                                        mime="text/plain",
                                        width='stretch',
                                        key="download_person_txt"
                                    )
                            with download_cols[3]:
                                with open(OUTPUT_DIR / "dragon.txt", "rb") as file:
                                    st.download_button(
                                        label="ğŸ“¥ é¾™éª¨æ¶TXT",
                                        data=file,
                                        file_name="dragon_detection.txt",
                                        mime="text/plain",
                                        width='stretch',
                                        key="download_dragon_txt"
                                    )
                        
                        # å›¾ç‰‡ä¸‹è½½æŒ‰é’®
                        with open(output_img_path, "rb") as file:
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½æ£€æµ‹ç»“æœå›¾ç‰‡",
                                data=file,
                                file_name="detection_result.jpg",
                                mime="image/jpeg",
                                width='stretch',
                                key="download_result_image"
                            )

        # ä¸»å†…å®¹åŒºåŸŸ - è§†é¢‘æ¨¡å¼
        elif current_mode == "video":
            # åˆå§‹åŒ–è§†é¢‘æ’­æ”¾ç›¸å…³çŠ¶æ€
            if 'video_played' not in st.session_state:
                st.session_state.video_played = False
                
            uploaded_file = st.file_uploader("é€‰æ‹©è§†é¢‘æ–‡ä»¶", type=['mp4', 'mov', 'avi', 'mkv'], key="upload_video")
            
            if uploaded_file is not None and not st.session_state.analysis_running:
                # ä¿å­˜ä¸Šä¼ çš„è§†é¢‘åˆ°ä¸´æ—¶ç›®å½•
                with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    video_path = tmp_file.name
                
                # æ˜¾ç¤ºä¸Šä¼ çš„è§†é¢‘ä¿¡æ¯
                try:
                    cap_info = cv2.VideoCapture(video_path)
                    if cap_info.isOpened():
                        width = int(cap_info.get(cv2.CAP_PROP_FRAME_WIDTH))
                        height = int(cap_info.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        fps = cap_info.get(cv2.CAP_PROP_FPS)
                        frame_count = int(cap_info.get(cv2.CAP_PROP_FRAME_COUNT))
                        st.info(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps:.1f} FPS, {frame_count} å¸§")
                    cap_info.release()
                except Exception as e:
                    st.warning(f"æ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {str(e)}")
                
                # æ§åˆ¶æŒ‰é’®
                col1, col2 = st.columns(2)
                with col1:
                    start_button = st.button("å¼€å§‹å¤„ç†è§†é¢‘", width='stretch', type="primary", key="btn_start_video")
                with col2:
                    stop_button = st.button("ç»ˆæ­¢å¤„ç†", width='stretch', disabled=True, key="btn_stop_video")
                
                if start_button:
                    st.session_state.analysis_running = True
                    st.session_state.stop_flag = False
                    st.session_state.video_played = False
                    
                    # è·å–å‚æ•°è®¾ç½®
                    params = get_params()
                    
                    # çŠ¶æ€å’Œè¿›åº¦æ˜¾ç¤º
                    status_text = st.empty()
                    progress_bar = st.progress(0)
                    
                    def update_status(text, progress=0):
                        status_text.text(text)
                        progress_bar.progress(progress)
                    
                    # ç›´æ¥åœ¨ä¸»çº¿ç¨‹è¿è¡Œè§†é¢‘å¤„ç†
                    output_video_path = process_video(
                        params['person_model'], params['dragon_model'], video_path, params['confs'], 
                        params['realtime_filter_method'], params['smooth'], params['device'],
                        save_json=params['save_json'], save_txt=params['save_txt'],
                        show_preview=params['show_preview'], single_dragon=params['single_dragon'], 
                        only_person=params['only_person'], only_dragon=params['only_dragon'],
                        classify=params['classify'], classify_model=params['classify_model'],
                        save_video=params['save_video'], verbose=False,
                        node_colors=params['node_colors'], node_size=params['node_size'],
                        line_color=params['line_color'], line_thickness=params['line_thickness'],
                        gpu_monitor=gpu_monitor,
                        status_callback=update_status
                    )
                    
                    st.session_state.analysis_running = False
                    st.session_state.output_video_path = output_video_path
                    st.session_state.video_played = True
                    update_status("è§†é¢‘å¤„ç†å®Œæˆï¼", 1.0)
                    
                    # æ˜¾ç¤ºå¤„ç†ç»“æœ
                    if output_video_path and output_video_path.exists():
                        st.subheader("å¤„ç†ç»“æœé¢„è§ˆ")
                        # è¯»å–è§†é¢‘æ–‡ä»¶å†…å®¹
                        try:
                            with open(output_video_path, "rb") as file:
                                video_bytes = file.read()
                                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                                if len(video_bytes) < 1024:  # å°äº1KBçš„è§†é¢‘æ–‡ä»¶å¯èƒ½ä¸ºç©º
                                    st.error("ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶è¿‡å°ï¼Œå¯èƒ½ä¸ºç©ºæˆ–æŸå")
                                else:
                                    # ä½¿ç”¨Streamlitçš„è§†é¢‘æ’­æ”¾å™¨
                                    st.video(video_bytes, format="video/mp4")
                        except Exception as e:
                            st.error(f"æ— æ³•è¯»å–è§†é¢‘æ–‡ä»¶: {str(e)}")
                        
                        # éªŒè¯è§†é¢‘æ–‡ä»¶
                        cap = cv2.VideoCapture(str(output_video_path))
                        if not cap.isOpened():
                            st.error("ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶æ— æ³•æ‰“å¼€ï¼Œå¯èƒ½å·²æŸå")
                        else:
                            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                            fps = cap.get(cv2.CAP_PROP_FPS)
                            st.info(f"è§†é¢‘ä¿¡æ¯: {frame_count} å¸§, {fps:.1f} FPS")
                            cap.release()
                        
                        # æç¤ºç”¨æˆ·åŠæ—¶ä¿å­˜
                        st.success("å¤„ç†å®Œæˆï¼è¯·åŠæ—¶ä¿å­˜è¾“å‡ºç»“æœï¼Œå…³é—­æµè§ˆå™¨åæ–‡ä»¶å°†è‡ªåŠ¨æ¸…é™¤ã€‚")
                        
                        # æä¾›ä¸‹è½½é€‰é¡¹
                        download_cols = st.columns(4)
                        
                        if params['save_json']:
                            with download_cols[0]:
                                with open(OUTPUT_DIR / "person.json", "rb") as file:
                                    st.download_button(
                                        label="ğŸ“¥ äººä½“JSON",
                                        data=file,
                                        file_name="person_detection.json",
                                        mime="application/json",
                                        width='stretch',
                                        key="download_video_person_json"
                                    )
                            with download_cols[1]:
                                with open(OUTPUT_DIR / "dragon.json", "rb") as file:
                                    st.download_button(
                                        label="ğŸ“¥ é¾™éª¨æ¶JSON",
                                        data=file,
                                        file_name="dragon_detection.json",
                                        mime="application/json",
                                        width='stretch',
                                        key="download_video_dragon_json"
                                    )
                        
                        if params['save_txt']:
                            with download_cols[2]:
                                with open(OUTPUT_DIR / "person.txt", "rb") as file:
                                    st.download_button(
                                        label="ğŸ“¥ äººä½“TXT",
                                        data=file,
                                        file_name="person_detection.txt",
                                        mime="text/plain",
                                        width='stretch',
                                        key="download_video_person_txt"
                                    )
                            with download_cols[3]:
                                with open(OUTPUT_DIR / "dragon.txt", "rb") as file:
                                    st.download_button(
                                        label="ğŸ“¥ é¾™éª¨æ¶TXT",
                                        data=file,
                                        file_name="dragon_detection.txt",
                                        mime="text/plain",
                                        width='stretch',
                                        key="download_video_dragon_txt"
                                    )
                        
                        # è§†é¢‘ä¸‹è½½æŒ‰é’®
                        with open(output_video_path, "rb") as file:
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½å¤„ç†åçš„è§†é¢‘",
                                data=file,
                                file_name="processed_video.mp4",
                                mime="video/mp4",
                                width='stretch',
                                key="download_processed_video"
                            )
                    elif output_video_path:
                        st.error(f"è§†é¢‘å¤„ç†å¤±è´¥ï¼Œæ–‡ä»¶æœªç”Ÿæˆ: {output_video_path}")
                    else:
                        st.error("è§†é¢‘å¤„ç†å¤±è´¥ï¼Œæœªç”Ÿæˆè¾“å‡ºæ–‡ä»¶")

        # ä¸»å†…å®¹åŒºåŸŸ - æ‘„åƒå¤´æ¨¡å¼
        elif current_mode == "camera":
            # è·å–æ£€æµ‹å‚æ•°
            params = get_params()

            # è°ƒç”¨æ–°çš„ WebRTC æ‘„åƒå¤´å¤„ç†é€»è¾‘
            process_camera_stream(params, gpu_monitor)

    # å‚æ•°è®¾ç½®ç‹¬ç«‹é¡µé¢
    elif st.session_state.current_tab == "å‚æ•°è®¾ç½®":
        st.markdown("<h1 style='text-align: center;'>å‚æ•°è®¾ç½®</h1>", unsafe_allow_html=True)
        st.write("åœ¨è¿™é‡Œé…ç½®ç³»ç»Ÿçš„å„é¡¹å‚æ•°ï¼Œè®¾ç½®å°†åº”ç”¨äºæ‰€æœ‰æ£€æµ‹æ¨¡å¼ã€‚")
        
        # ä½¿ç”¨session_stateå­˜å‚¨å‚æ•°ï¼Œç¡®ä¿åœ¨é¡µé¢é—´ä¿æŒä¸€è‡´
        if 'params_initialized' not in st.session_state:
            # åˆå§‹åŒ–å‚æ•°
            st.session_state.use_gpu = gpu_monitor.use_gpu
            st.session_state.single_dragon = True
            st.session_state.only_person = False
            st.session_state.only_dragon = False
            st.session_state.classify = True
            st.session_state.show_preview = True
            st.session_state.save_video = True
            st.session_state.save_json = False
            st.session_state.save_txt = False
            st.session_state.person_model = "yolov8m-pose.pt"
            st.session_state.dragon_model = "YoGon-Pose-v2.pt"
            st.session_state.classify_model = "YoGon-Clas-v2.pth"
            st.session_state.node_size = 10
            st.session_state.line_thickness = 6
            st.session_state.person_conf = 0.3
            st.session_state.dragon_conf = 0.3
            st.session_state.person_kpt_conf = 0.5
            st.session_state.dragon_kpt_conf = 0.5
            st.session_state.realtime_filter_method = "none"
            st.session_state.smooth = "none"
            
            # åˆå§‹åŒ–èŠ‚ç‚¹é¢œè‰²
            hex_to_rgb = lambda h: tuple(int(h.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))
            for i in range(9):
                default_color = '#%02x%02x%02x' % (
                    DEFAULT_NODE_COLORS[i][2], 
                    DEFAULT_NODE_COLORS[i][1], 
                    DEFAULT_NODE_COLORS[i][0]
                )
                st.session_state[f"node_color_{i}"] = default_color
            
            st.session_state.line_color = "#C8C8C8"
            st.session_state.params_initialized = True
        
        # è®¾å¤‡é…ç½®
        with st.expander("ğŸ’» è®¾å¤‡é…ç½®", expanded=True):
            st.session_state.use_gpu = st.checkbox(
                "ä½¿ç”¨GPUåŠ é€Ÿ", 
                value=st.session_state.use_gpu, 
                disabled=not gpu_monitor.use_gpu,
                key="cb_use_gpu"
            )
            device = torch.device('cuda' if (st.session_state.use_gpu and gpu_monitor.use_gpu) else 'cpu')
            
            # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
            if device.type == 'cuda':
                st.success(f"ä½¿ç”¨GPU: {gpu_monitor.gpu_name}")
                st.info(f"æ€»æ˜¾å­˜: {gpu_monitor.memory_total:.2f} GB")
            else:
                st.warning("ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—ï¼Œå¯èƒ½è¾ƒæ…¢")
        
        # æ£€æµ‹é€‰é¡¹
        with st.expander("ğŸ” æ£€æµ‹é€‰é¡¹", expanded=True):
            def on_detection_param_change():
                if st.session_state.analysis_running:
                    st.warning("æ£€æµ‹å‚æ•°å·²ä¿®æ”¹ï¼Œå½“å‰åˆ†æå°†ç»ˆæ­¢å¹¶éœ€è¦é‡æ–°å¼€å§‹")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            st.session_state.single_dragon = st.checkbox(
                "ä»…æ£€æµ‹å•æ¡é¾™", 
                value=st.session_state.single_dragon, 
                on_change=on_detection_param_change,
                key="cb_single_dragon"
            )
            st.session_state.only_person = st.checkbox(
                "ä»…æ£€æµ‹äººä½“", 
                value=st.session_state.only_person, 
                on_change=on_detection_param_change,
                key="cb_only_person"
            )
            st.session_state.only_dragon = st.checkbox(
                "ä»…æ£€æµ‹é¾™éª¨æ¶", 
                value=st.session_state.only_dragon, 
                on_change=on_detection_param_change,
                key="cb_only_dragon"
            )
            st.session_state.classify = st.checkbox(
                "å¯ç”¨åŠ¨ä½œåˆ†ç±»", 
                value=st.session_state.classify, 
                on_change=on_detection_param_change,
                key="cb_classify"
            )

        # è¾“å‡ºé€‰é¡¹
        with st.expander("ğŸ’¾ è¾“å‡ºé€‰é¡¹", expanded=True):
            def on_output_param_change():
                if st.session_state.analysis_running:
                    st.warning("è¾“å‡ºå‚æ•°å·²ä¿®æ”¹ï¼Œå½“å‰åˆ†æå°†ç»ˆæ­¢å¹¶éœ€è¦é‡æ–°å¼€å§‹")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            st.session_state.show_preview = st.checkbox(
                "å®æ—¶é¢„è§ˆ", 
                value=st.session_state.show_preview, 
                on_change=on_output_param_change,
                key="cb_show_preview"
            )
            st.session_state.save_video = st.checkbox(
                "ä¿å­˜è§†é¢‘", 
                value=st.session_state.save_video, 
                on_change=on_output_param_change,
                key="cb_save_video"
            )
            st.session_state.save_json = st.checkbox(
                "ä¿å­˜JSONæ–‡ä»¶", 
                value=st.session_state.save_json, 
                on_change=on_output_param_change,
                key="cb_save_json"
            )
            st.session_state.save_txt = st.checkbox(
                "ä¿å­˜TXTæ–‡ä»¶", 
                value=st.session_state.save_txt, 
                on_change=on_output_param_change,
                key="cb_save_txt"
            )
        
        # æ¨¡å‹é…ç½®
        with st.expander("ğŸ¤– æ¨¡å‹é…ç½®", expanded=True):
            def on_model_param_change():
                if st.session_state.analysis_running:
                    st.warning("æ¨¡å‹å‚æ•°å·²ä¿®æ”¹ï¼Œå½“å‰åˆ†æå°†ç»ˆæ­¢å¹¶éœ€è¦é‡æ–°å¼€å§‹")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            st.session_state.person_model = st.text_input(
                "äººä½“å§¿æ€æ¨¡å‹è·¯å¾„", 
                value=st.session_state.person_model, 
                on_change=on_model_param_change,
                key="txt_person_model"
            )
            st.session_state.dragon_model = st.text_input(
                "é¾™éª¨æ¶æ¨¡å‹è·¯å¾„", 
                value=st.session_state.dragon_model, 
                on_change=on_model_param_change,
                key="txt_dragon_model"
            )
            st.session_state.classify_model = st.text_input(
                "åŠ¨ä½œåˆ†ç±»æ¨¡å‹è·¯å¾„", 
                value=st.session_state.classify_model, 
                on_change=on_model_param_change,
                key="txt_classify_model"
            )


        # æ ·å¼è‡ªå®šä¹‰é…ç½®
        with st.expander("ğŸ¨ æ ·å¼è‡ªå®šä¹‰", expanded=False):
            def on_style_param_change():
                if st.session_state.analysis_running:
                    st.warning("æ ·å¼å‚æ•°å·²ä¿®æ”¹ï¼Œå½“å‰åˆ†æå°†ç»ˆæ­¢å¹¶éœ€è¦é‡æ–°å¼€å§‹")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            # èŠ‚ç‚¹æ ·å¼
            st.subheader("èŠ‚ç‚¹æ ·å¼")
            st.session_state.node_size = st.slider(
                "èŠ‚ç‚¹å¤§å°", 
                1, 20, st.session_state.node_size, 
                on_change=on_style_param_change,
                key="slider_node_size"
            )
            
            # æ¯ä¸ªèŠ‚ç‚¹å•ç‹¬çš„é¢œè‰²é€‰æ‹©
            hex_to_rgb = lambda h: tuple(int(h.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))
            for i in range(9):  # 1-9èŠ‚ç‚¹
                st.session_state[f"node_color_{i}"] = st.color_picker(
                    f"èŠ‚ç‚¹ {i+1} é¢œè‰²", 
                    st.session_state[f"node_color_{i}"], 
                    key=f"cp_node_{i}",
                    on_change=on_style_param_change
                )
            
            # è¿çº¿æ ·å¼
            st.subheader("è¿çº¿æ ·å¼")
            st.session_state.line_thickness = st.slider(
                "è¿çº¿ç²—ç»†", 
                1, 10, st.session_state.line_thickness, 
                on_change=on_style_param_change,
                key="slider_line_thickness"
            )
            st.session_state.line_color = st.color_picker(
                "è¿çº¿é¢œè‰²", 
                st.session_state.line_color, 
                on_change=on_style_param_change,
                key="cp_line_color"
            )

        # ç½®ä¿¡åº¦é…ç½®
        with st.expander("ğŸ¯ ç½®ä¿¡åº¦é˜ˆå€¼", expanded=False):
            def on_conf_param_change():
                if st.session_state.analysis_running:
                    st.warning("ç½®ä¿¡åº¦å‚æ•°å·²ä¿®æ”¹ï¼Œå½“å‰åˆ†æå°†ç»ˆæ­¢å¹¶éœ€è¦é‡æ–°å¼€å§‹")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            st.session_state.person_conf = st.slider(
                "äººä½“æ£€æµ‹ç½®ä¿¡åº¦", 
                0.1, 1.0, st.session_state.person_conf, 0.05, 
                on_change=on_conf_param_change,
                key="slider_person_conf"
            )
            st.session_state.dragon_conf = st.slider(
                "é¾™éª¨æ¶æ£€æµ‹ç½®ä¿¡åº¦", 
                0.1, 1.0, st.session_state.dragon_conf, 0.05, 
                on_change=on_conf_param_change,
                key="slider_dragon_conf"
            )
            st.session_state.person_kpt_conf = st.slider(
                "äººä½“å…³é”®ç‚¹ç½®ä¿¡åº¦", 
                0.1, 1.0, st.session_state.person_kpt_conf, 0.05, 
                on_change=on_conf_param_change,
                key="slider_person_kpt_conf"
            )
            st.session_state.dragon_kpt_conf = st.slider(
                "é¾™éª¨æ¶å…³é”®ç‚¹ç½®ä¿¡åº¦", 
                0.1, 1.0, st.session_state.dragon_kpt_conf, 0.05, 
                on_change=on_conf_param_change,
                key="slider_dragon_kpt_conf"
            )

        # æ»¤æ³¢é…ç½®
        with st.expander("ğŸ“Š æ»¤æ³¢è®¾ç½®", expanded=False):
            def on_filter_param_change():
                if st.session_state.analysis_running:
                    st.warning("æ»¤æ³¢å‚æ•°å·²ä¿®æ”¹ï¼Œå½“å‰åˆ†æå°†ç»ˆæ­¢å¹¶éœ€è¦é‡æ–°å¼€å§‹")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            st.session_state.realtime_filter_method = st.selectbox(
                "å®æ—¶æ»¤æ³¢æ–¹æ³•",
                ["none", "ema", "kalman"],
                index=["none", "ema", "kalman"].index(st.session_state.realtime_filter_method),
                on_change=on_filter_param_change,
                key="select_realtime_filter"
            )
            st.session_state.smooth = st.selectbox(
                "è§†é¢‘å¹³æ»‘æ–¹æ¡ˆ",
                ["none", "mean", "weighted_mean", "ewm"],
                index=["none", "mean", "weighted_mean", "ewm"].index(st.session_state.smooth),
                on_change=on_filter_param_change,
                key="select_smooth"
            )
        
        if st.button("ä¿å­˜å‚æ•°è®¾ç½®", width='stretch', type="primary", key="btn_save_params"):
            st.success("å‚æ•°è®¾ç½®å·²ä¿å­˜ï¼")

    # å¼€å‘å›¢é˜Ÿé¡µé¢
    elif st.session_state.current_tab == "å¼€å‘å›¢é˜Ÿ":
        st.markdown("<h1 style='text-align: center;'>å¼€å‘å›¢é˜Ÿ</h1>", unsafe_allow_html=True)
        
        # å›¢é˜Ÿä»‹ç»
        st.subheader("é¡¹ç›®ç®€ä»‹")
        st.write("""
        ç‚¹ç›AIï¼ˆDragonJotï¼‰æ˜¯å›½å†…é¦–ä¸ªä¸“æ³¨äºèˆé¾™è¿åŠ¨çš„AIæ™ºèƒ½è¯„åˆ¤ä¸è®­ç»ƒè¾…åŠ©ç³»ç»Ÿã€‚é’ˆå¯¹ä¼ ç»Ÿèˆé¾™èµ›äº‹è¯„åˆ¤ä¸»è§‚æ€§å¼ºã€è®­ç»ƒåé¦ˆç¼ºä¹é‡åŒ–ä¾æ®ã€æ•™å­¦èµ„æºä¸è¶³ç­‰è¡Œä¸šç—›ç‚¹ï¼Œæˆ‘ä»¬åˆ›æ–°æå‡º"äººé¾™ä¸€ä½“"ååŒåŠ¨ä½œè¯†åˆ«æ¨¡å‹ã€‚è¯¥ç³»ç»ŸåŸºäºYOLOå§¿æ€è¯†åˆ«æ¨¡å‹ä¸CNNåŠ¨ä½œåˆ†ç±»ç®—æ³•ï¼Œç»“åˆè‡ªä¸»æ„å»ºçš„èˆé¾™éª¨éª¼æ•°æ®é›†ï¼Œå®ç°äº†å¯¹äººä½“å§¿æ€ä¸é¾™å½¢è½¨è¿¹çš„åŒæ­¥è¯†åˆ«ä¸åˆ†æã€‚

        ç³»ç»Ÿæ”¯æŒå¤šè·¯è§†é¢‘è¾“å…¥ï¼ˆåŒ…æ‹¬æ‰‹æœºç›´æ’­ä¸æœ¬åœ°è§†é¢‘ï¼‰ï¼Œå…·å¤‡å®æ—¶åŠ¨ä½œæ•æ‰ã€åŠ¨ä½œåˆ†ç±»ã€å®æ—¶åˆ†æä¸é‡åŒ–è¯„ä¼°ç­‰æ ¸å¿ƒåŠŸèƒ½ï¼Œå…¨é¢è¦†ç›–èµ›äº‹è¯„åˆ¤ã€ä¸“ä¸šè®­ç»ƒä¸å¤§ä¼—æ•™å­¦ä¸‰å¤§åº”ç”¨åœºæ™¯ã€‚

        Open DragonJotä½œä¸ºç‚¹ç›AIçš„å¼€æºäº§å“ï¼Œæä¾›æ ¸å¿ƒåŠŸèƒ½çš„ä½“éªŒç‰ˆï¼ŒåŒ…å«äººä½“å§¿æ€è¯†åˆ«ã€é¾™è¿åŠ¨è½¨è¿¹è¯†åˆ«å’Œèˆé¾™åŠ¨ä½œåˆ†ç±»åŠŸèƒ½ã€‚å¼€æºä»£ç å·²å‘å¸ƒäºGitHubï¼Œäº§å“é¢„è§ˆç½‘é¡µéƒ¨ç½²åœ¨Streamlitå¹³å°ï¼Œæ¬¢è¿ä½“éªŒä¸äº¤æµã€‚
        """)
        
        # å›¢é˜Ÿæˆå‘˜
        st.subheader("æ ¸å¿ƒå¼€å‘å›¢é˜Ÿ")
        
        # æˆå‘˜1
        col1, col2 = st.columns([1, 9])  # è°ƒæ•´æ¯”ä¾‹ä½¿å¤´åƒåŒºåŸŸæ›´å°
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "HC.jpg"), caption="é¡¹ç›®è´Ÿè´£äºº", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="é¡¹ç›®è´Ÿè´£äºº", width='stretch')
        with col2:
            st.markdown("""
            ### éœç•… - é¡¹ç›®è´Ÿè´£äºº
            è´Ÿè´£å›¢é˜Ÿç»Ÿç­¹ä¸æ ¸å¿ƒè§„åˆ’ï¼Œæå‡ºé¡¹ç›®æ ¸å¿ƒåˆ›æ„ï¼Œä¸»å¯¼åº”ç”¨å¼€å‘ä¸è½åœ°ã€‚
            """)
        
        # æˆå‘˜2
        col1, col2 = st.columns([1, 9])
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "LZC.jpg"), caption="ç®—æ³•å·¥ç¨‹å¸ˆ", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="ç®—æ³•å·¥ç¨‹å¸ˆ", width='stretch')
        with col2:
            st.markdown("""
            ### å¢å­è¯š - ç®—æ³•å·¥ç¨‹å¸ˆ
            è´Ÿè´£æŠ€æœ¯è°ƒç ”ã€AIæ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–ï¼Œä»¥åŠæ ¸å¿ƒç®—æ³•çš„å¼€å‘è½åœ°ã€‚
            """)
        
        # æˆå‘˜3
        col1, col2 = st.columns([1, 9])
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "FZY.jpg"), caption="UI/UXè®¾è®¡å¸ˆ", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="UI/UXè®¾è®¡å¸ˆ", width='stretch')
        with col2:
            st.markdown("""
            ### æ–¹å­å«£ - UI/UXè®¾è®¡å¸ˆ
            è´Ÿè´£äº§å“åˆ›æ„ç­–åˆ’ã€ç”¨æˆ·ç•Œé¢ä¸ä½“éªŒè®¾è®¡ï¼Œå¡‘é€ äº§å“çš„æ•´ä½“è§†è§‰é£æ ¼ã€‚
            """)
        
        # æˆå‘˜4
        col1, col2 = st.columns([1, 9])
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "DWY.jpg"), caption="æ–‡æ¡ˆç­–åˆ’", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="æ–‡æ¡ˆç­–åˆ’", width='stretch')
        with col2:
            st.markdown("""
            ### ä¸é—»ç¥ - åˆ›æ„å†…å®¹æ€»ç›‘
            è´Ÿè´£å¸‚åœºè°ƒç ”ï¼Œå¹¶æ€»è´Ÿè´£äº§å“æŠ¥å‘Šã€è½åœ°æ¨å¹¿æ–¹æ¡ˆç­‰å…³é”®æ–‡ç¨¿çš„æ’°å†™ã€‚
            """)

        # æˆå‘˜5
        col1, col2 = st.columns([1, 9])
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "ZMH.jpg"), caption="æ•°æ®å·¥ç¨‹å¸ˆ", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="æ•°æ®å·¥ç¨‹å¸ˆ", width='stretch')
        with col2:
            st.markdown("""
            ### å¼ æ˜æ¶µ - æ•°æ®å·¥ç¨‹å¸ˆ
            ä¸»å¯¼èˆé¾™æ•°æ®é›†çš„è§„åˆ’ã€é‡‡é›†ä¸æ„å»ºå·¥ä½œï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›æ ¸å¿ƒæ•°æ®åŸºç¡€ã€‚
            """)

        # æˆå‘˜6
        col1, col2 = st.columns([1, 9])
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "ZKX.jpg"), caption="äº§å“å®£ä¼ ", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="äº§å“å®£ä¼ ", uwidth='stretch')
        with col2:
            st.markdown("""
            ### èµµåº·è¥¿ â€“ äº§å“å¯è§†åŒ–ä¸“å‘˜
            è´Ÿè´£äº§å“æ¼”ç¤ºè§†é¢‘çš„å‰ªè¾‘åˆ¶ä½œã€æ•°æ®æ”¶é›†å’Œéƒ¨åˆ†å‰ç«¯é¡µé¢å¼€å‘ã€‚
            """)
        
        # è”ç³»æ–¹å¼
        st.subheader("è”ç³»æ–¹å¼")
        st.write("""
        å¦‚æœæ‚¨å¯¹"ç‚¹ç›AI"é¡¹ç›®æ„Ÿå…´è¶£ï¼Œæˆ–æœ‰ä»»ä½•ç–‘é—®ã€å»ºè®®ä¸åˆä½œæ„å‘ï¼Œæ¬¢è¿é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸æˆ‘ä»¬å–å¾—è”ç³»ï¼š
        
        é‚®ç®±ï¼š22307110080@m.fudan.edu.cn
        
        é¡¹ç›®åœ°å€ï¼šxxxx
        
        æœŸå¾…æ‚¨çš„åé¦ˆä¸äº¤æµï¼
        """)
        st.write("   ")
        st.write("   ")
        st.write("   ")
        st.write("   ")
        st.markdown("<div style='height: 100px;'></div>", unsafe_allow_html=True)
        

        # è‡´è°¢ï¼ˆå±…ä¸­æ˜¾ç¤ºåœ¨æœ«å°¾ï¼‰
        st.markdown("""
    <div style='text-align: center; margin: 20px 0;'>
        <h3>è‡´ è°¢</h3>
        <p style='margin: 15px 0; line-height: 1.8;'>
            ä¸€è·¯èµ°æ¥ï¼Œ<br>"ç‚¹ç›AI"çš„æ¯ä¸€æ­¥æˆé•¿éƒ½ç¦»ä¸å¼€ä¼—å¤šæ”¯æŒè€…çš„é¼åŠ›ç›¸åŠ©ã€‚<br>åœ¨æ­¤ï¼Œæˆ‘ä»¬è°¨å‘æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®<br>
            å€¾æ³¨å¿ƒè¡€ã€ç»™äºˆæŒ‡å¯¼çš„æœºæ„ä¸ä¸ªäººï¼Œ<br>è‡´ä»¥æœ€è¯šæŒšçš„æ•¬æ„ä¸æœ€è¡·å¿ƒçš„æ„Ÿè°¢ï¼
        </p>
        <p style='margin: 15px 0; line-height: 1.8;'>
            ç‰¹åˆ«é¸£è°¢ <strong style='font-size: 1.1em;'>å¤æ—¦å¤§å­¦ä½“è‚²å‘å±•éƒ¨çš„å¾ç‡•å‹¤è€å¸ˆ</strong>ï¼Œä»¥åŠæ‰€æœ‰<br>
            <strong style='font-size: 1.1em;'>ç¬¬ä¸€å±Š"å¤æ—¦AI+ä½“è‚²åˆ›å˜è¥"</strong> çš„è€å¸ˆä¸åŒå­¦ä»¬â€”â€”<br>
            æ˜¯ä½ ä»¬æ­å»ºçš„åˆ›æ–°å¹³å°ï¼Œè®©è¿™ä¸ªæºäºå¯¹ä¼ ç»Ÿæ–‡åŒ–çƒ­çˆ±çš„é¡¹ç›®ç§å­å¾—ä»¥èŒå‘ã€æˆé•¿ã€‚
        </p>
        <p style='margin: 15px 0; line-height: 1.8;'>
            ç‰¹åˆ«æ„Ÿè°¢ <strong style='font-size: 1.1em;'>ä¸Šæµ·é¾™ç‹®åä¼šæœ±ä¿Šé¹æ•™ç»ƒ</strong> çš„ä¸“ä¸šå¼•é¢†â€”â€”<br>
            æ‚¨ä¸ä»…ç²¾å‡†ç‚¹å‡ºè¡Œä¸šç—›ç‚¹ï¼Œæ›´æ— ç§åˆ†äº«å®è´µçš„è¡Œä¸šèµ„æºï¼Œä¸ºæˆ‘ä»¬çš„æŠ€æœ¯è½åœ°æŒ‡æ˜äº†æ–¹å‘ã€‚
        </p>
        <p style='margin: 15px 0; line-height: 1.8;'>
            æ·±æ·±æ„Ÿæ¿€ <strong style='font-size: 1.1em;'>å¤æ—¦é¾™ç‹®åä¼š</strong> çš„åŒå­¦ä»¬â€”â€”<br>
            ä½ ä»¬ç”¨ä¸“ä¸šçš„è¡¨æ¼”ä¸çƒ­æƒ…çš„æŠ•å…¥ï¼Œä¸ºé¡¹ç›®æä¾›äº†é«˜è´¨é‡çš„æ•°æ®é›†ä¸æ¼”ç¤ºç´ æï¼Œ<br>
            æˆä¸º"ç‚¹ç›AI"èƒ½å¤Ÿæ ©æ ©å¦‚ç”Ÿçš„æ ¸å¿ƒåŸºçŸ³ã€‚
        </p>
        <p style='margin: 15px 0; line-height: 1.8;'>
            å› ä¸ºæœ‰ä½ ä»¬ï¼Œä¼ ç»Ÿæ–‡åŒ–ä¸ç°ä»£ç§‘æŠ€çš„ç¢°æ’æ‰å¦‚æ­¤ç²¾å½©ã€‚<br>
            è¿™ä»½æ”¯æŒï¼Œå°†æ¿€åŠ±æˆ‘ä»¬ç»§ç»­ç”¨æŠ€æœ¯èµ‹èƒ½ä¼ ç»Ÿï¼Œè®©èˆé¾™æ–‡åŒ–ç„•å‘æ–°çš„ç”Ÿæœºï¼
        </p>
    </div>
    """, unsafe_allow_html=True)

# å‚æ•°è·å–å‡½æ•°ï¼Œä¾›å„æ¨¡å—ä½¿ç”¨ç»Ÿä¸€çš„å‚æ•°è®¾ç½®
def get_params():
    # ç¡®ä¿å‚æ•°å·²åˆå§‹åŒ–
    if 'params_initialized' not in st.session_state:
        # é¿å…é€’å½’è°ƒç”¨main()ï¼Œç›´æ¥åˆå§‹åŒ–å‚æ•°
        st.session_state.use_gpu = gpu_monitor.use_gpu
        st.session_state.single_dragon = True
        st.session_state.only_person = False
        st.session_state.only_dragon = False
        st.session_state.classify = True
        st.session_state.show_preview = True
        st.session_state.save_video = True
        st.session_state.save_json = False
        st.session_state.save_txt = False
        st.session_state.person_model = "yolov8m-pose.pt"
        st.session_state.dragon_model = "YoGon-Pose-v2.pt"
        st.session_state.classify_model = "YoGon-Clas-v2.pth"
        st.session_state.node_size = 10
        st.session_state.line_thickness = 6
        st.session_state.person_conf = 0.3
        st.session_state.dragon_conf = 0.3
        st.session_state.person_kpt_conf = 0.5
        st.session_state.dragon_kpt_conf = 0.5
        st.session_state.realtime_filter_method = "none"
        st.session_state.smooth = "none"
        
        # åˆå§‹åŒ–èŠ‚ç‚¹é¢œè‰²
        hex_to_rgb = lambda h: tuple(int(h.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))
        for i in range(9):
            default_color = '#%02x%02x%02x' % (
                DEFAULT_NODE_COLORS[i][2], 
                DEFAULT_NODE_COLORS[i][1], 
                DEFAULT_NODE_COLORS[i][0]
            )
            st.session_state[f"node_color_{i}"] = default_color
        
        st.session_state.line_color = "#C8C8C8"
        st.session_state.params_initialized = True
    
    # è®¾å¤‡é…ç½®
    device = torch.device(
        'cuda' if (st.session_state.use_gpu and gpu_monitor.use_gpu) else 'cpu'
    )
    
    # ç½®ä¿¡åº¦å‚æ•°
    confs = [
        st.session_state.person_conf, 
        st.session_state.dragon_conf, 
        st.session_state.person_kpt_conf, 
        st.session_state.dragon_kpt_conf
    ]
    
    # èŠ‚ç‚¹é¢œè‰²å¤„ç†
    hex_to_rgb = lambda h: tuple(int(h.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))
    node_colors = []
    for i in range(9):
        r, g, b = hex_to_rgb(st.session_state[f"node_color_{i}"])
        node_colors.append((b, g, r))  # è½¬æ¢ä¸ºBGRæ ¼å¼
    
    # è¿çº¿é¢œè‰²å¤„ç†
    line_r, line_g, line_b = hex_to_rgb(st.session_state.line_color)
    line_color = (line_b, line_g, line_r)  # è½¬æ¢ä¸ºBGRæ ¼å¼
    
    return {
        'device': device,
        'single_dragon': st.session_state.single_dragon,
        'only_person': st.session_state.only_person,
        'only_dragon': st.session_state.only_dragon,
        'classify': st.session_state.classify,
        'show_preview': st.session_state.show_preview,
        'save_video': st.session_state.save_video,
        'save_json': st.session_state.save_json,
        'save_txt': st.session_state.save_txt,
        'person_model': st.session_state.person_model,
        'dragon_model': st.session_state.dragon_model,
        'classify_model': st.session_state.classify_model,
        'node_size': st.session_state.node_size,
        'line_thickness': st.session_state.line_thickness,
        'confs': confs,
        'realtime_filter_method': st.session_state.realtime_filter_method,
        'smooth': st.session_state.smooth,
        'node_colors': node_colors,
        'line_color': line_color
    }

if __name__ == "__main__":
    main()
