import streamlit as st 
import cv2
import numpy as np
import tempfile
import os
import shutil
from pathlib import Path
import json
from collections import deque
from filterpy.kalman import KalmanFilter
import statistics
from PIL import Image, ImageDraw, ImageFont
import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
import time
import psutil
import base64
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration, WebRtcMode
import av
import threading

import logging
logging.getLogger("streamlit.runtime.scriptrunner.script_runner").setLevel(logging.ERROR)


# ------------------ Âπ∂ÂèëËÆøÈóÆÊéßÂà∂ÈÄªËæë ------------------

@st.cache_resource
def get_active_sessions():
    """
    ÂÖ®Â±ÄÂÖ±‰∫´ÁöÑ‰ºöËØùËÆ°Êï∞Âô®ÔºàË∑®ÊâÄÊúâÁî®Êà∑ session ÂÖ±‰∫´Ôºâ„ÄÇ
    """
    return {"count": 0, "lock": threading.Lock()}

MAX_USERS = 2       # ÂêåÊó∂ÂÖÅËÆ∏ÁöÑÊúÄÂ§ßËÆøÈóÆ‰∫∫Êï∞
MEM_THRESHOLD = 85  # ÂÜÖÂ≠òÂç†Áî®‰∏äÈôêÔºàÁôæÂàÜÊØîÔºâ

def check_user_limit():
    """
    Ê£ÄÊü•ÊòØÂê¶Ë∂ÖËøáËÆøÈóÆ‰∫∫Êï∞ÊàñÁ≥ªÁªüËµÑÊ∫êÈôêÂà∂„ÄÇ
    """
    sessions = get_active_sessions()

    # Á≥ªÁªüËµÑÊ∫êÊ£ÄÊµãÔºàÈò≤Ê≠¢ OOMÔºâ
    mem = psutil.virtual_memory().percent
    if mem > MEM_THRESHOLD:
        st.error(f"‚ö†Ô∏è ÊúçÂä°Âô®ËµÑÊ∫êÁπÅÂøôÔºàÂÜÖÂ≠ò‰ΩøÁî® {mem:.1f}%ÔºâÔºåËØ∑Á®çÂêéÂÜçËØï„ÄÇ")
        st.stop()

    # ‰∫∫Êï∞Ê£ÄÊµã
    with sessions["lock"]:
        if sessions["count"] >= MAX_USERS:
            st.error("üö´ ÂΩìÂâçËÆøÈóÆ‰∫∫Êï∞Â∑≤Êª°ÔºåËØ∑Á®çÂêéÂÜçËØï üôè")
            st.stop()
        else:
            sessions["count"] += 1
            st.session_state["_registered"] = True
            st.session_state["_user_id"] = id(st.session_state)
            print(f"[INFO] Êñ∞Áî®Êà∑ËøõÂÖ•ÔºåÂΩìÂâçÂú®Á∫ø‰∫∫Êï∞: {sessions['count']}")

def release_user():
    """
    Áî®Êà∑Êñ≠ÂºÄÊàñÂà∑Êñ∞Êó∂ÈáäÊîæËÆøÈóÆÂêçÈ¢ù„ÄÇ
    """
    sessions = get_active_sessions()
    with sessions["lock"]:
        if sessions["count"] > 0:
            sessions["count"] -= 1
    print(f"[INFO] Áî®Êà∑Á¶ªÂºÄÔºåÂΩìÂâçÂú®Á∫ø‰∫∫Êï∞: {sessions['count']}")

def user_session_cleanup():
    """
    Ê®°Êãü on_session_endÔºöÂêéÂè∞Á∫øÁ®ãÊ£ÄÊµã session ÊòØÂê¶‰∏≠Êñ≠„ÄÇ
    """
    while True:
        time.sleep(5)
        # Â¶ÇÊûúÁî®Êà∑ session Ë¢´Ê†áËÆ∞‰∏∫Â∑≤ÁªìÊùüÔºåÂ∞±ÈáäÊîæÂêçÈ¢ù
        if "_registered" in st.session_state and not st.session_state._is_running_with_streamlit:
            release_user()
            break

# ÂàùÂßãÂåñÊó∂Ê£ÄÊµãÁî®Êà∑‰∏äÈôê
if "_registered" not in st.session_state:
    check_user_limit()

# ÂêéÂè∞ÁõëÊµãÁ∫øÁ®ãÔºåÊ®°Êãü session ÂÖ≥Èó≠Ê£ÄÊµãÔºàÈùûÈòªÂ°ûÔºâ
if "_cleanup_started" not in st.session_state:
    threading.Thread(target=user_session_cleanup, daemon=True).start()
    st.session_state["_cleanup_started"] = True

# Âú®È°µÈù¢‰æßËæπÊ†èÊòæÁ§∫ÂΩìÂâçÁä∂ÊÄÅ
with st.sidebar:
    sessions = get_active_sessions()
    st.markdown(f"**ÂΩìÂâçÂú®Á∫øÁî®Êà∑Êï∞Ôºö** {sessions['count']} / {MAX_USERS}")

# ---------------------- Ë∑ØÂæÑÈÖçÁΩÆ ----------------------
SCRIPT_DIR = Path(os.path.dirname(os.path.abspath(__file__)))
STATIC_DIR = SCRIPT_DIR / "static"
OUTPUT_DIR = SCRIPT_DIR / "temp_output"
MODELS_DIR = SCRIPT_DIR / "model"

for dir_path in [STATIC_DIR, OUTPUT_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

def clean_output_dir():
    if OUTPUT_DIR.exists():
        for item in OUTPUT_DIR.iterdir():
            try:
                if item.is_file():
                    item.unlink()
                elif item.is_dir():
                    shutil.rmtree(item)
            except Exception as e:
                print(f"Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂Â§±Ë¥•: {e}")

def get_model_path(relative_path):
    """Â∞ÜÁõ∏ÂØπË∑ØÂæÑËΩ¨Êç¢‰∏∫ÁªùÂØπË∑ØÂæÑ"""
    model_path = Path(relative_path)
    if not model_path.is_absolute():
        model_path = SCRIPT_DIR / model_path
    return str(model_path)

import atexit
atexit.register(clean_output_dir)

# ---------------------- ÈÄöÁî®ÈÖçÁΩÆ ----------------------
DRAGON_KEYPOINT_NAMES = [str(i) for i in range(1, 10)]
DRAGON_SKELETON = [[0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8]]
DEFAULT_NODE_COLORS = [
    (0, 0, 255), (0, 128, 255), (0, 255, 255),
    (0, 255, 0), (255, 255, 0), (255, 128, 0),
    (255, 0, 0), (255, 0, 255), (128, 0, 255)
]
DEFAULT_LINE_COLOR = (200, 200, 200)

# ---------------------- GPUÁõëÊéßÂäüËÉΩ ----------------------
class GPUMonitor:
    def __init__(self):
        self.use_gpu = torch.cuda.is_available()
        self.device = torch.device('cuda' if self.use_gpu else 'cpu')
        self.gpu_name = torch.cuda.get_device_name(0) if self.use_gpu else "N/A"
        self.memory_total = self._get_total_memory()
        self.last_usage = 0
        self.last_time = time.time()
        self.fps_list = []
        
    def _get_total_memory(self):
        if self.use_gpu:
            return torch.cuda.get_device_properties(0).total_memory / (1024 **3)
        return 0
        
    def get_memory_usage(self):
        if not self.use_gpu:
            return 0, 0
        memory_used = torch.cuda.memory_allocated(0) / (1024** 3)
        memory_cached = torch.cuda.memory_reserved(0) / (1024 **3)
        return memory_used, memory_cached
        
    def update_fps(self):
        current_time = time.time()
        elapsed = current_time - self.last_time
        if elapsed > 0:
            fps = 1 / elapsed
            self.fps_list.append(fps)
            if len(self.fps_list) > 30:
                self.fps_list.pop(0)
        self.last_time = current_time
        
    def get_average_fps(self):
        return sum(self.fps_list) / len(self.fps_list) if self.fps_list else 0
        
    def get_cpu_usage(self):
        return psutil.cpu_percent()
        
    def get_system_memory_usage(self):
        mem = psutil.virtual_memory()
        return mem.percent

gpu_monitor = GPUMonitor()

# ---------------------- Êª§Ê≥¢Á±ªÂÆö‰πâ ----------------------
class EMAFilter:
    def __init__(self, alpha=0.3):
        self.alpha = alpha
        self.previous_points = None
    
    def update(self, current_points):
        if self.previous_points is None:
            self.previous_points = current_points.copy()
            return current_points
        
        mask = ~np.isnan(current_points)
        smoothed = np.where(mask, 
                           self.alpha * current_points + (1 - self.alpha) * self.previous_points,
                           self.previous_points)
        self.previous_points = smoothed.copy()
        return smoothed

class KalmanFilterWrapper:
    def __init__(self, num_points, dt=0.033, process_noise=0.1, measurement_noise=5.0):
        self.filters = []
        for _ in range(num_points):
            kf = KalmanFilter(dim_x=4, dim_z=2)
            kf.F = np.array([[1, 0, dt, 0],
                            [0, 1, 0, dt],
                            [0, 0, 1, 0],
                            [0, 0, 0, 1]])
            kf.H = np.array([[1, 0, 0, 0],
                            [0, 1, 0, 0]])
            kf.P = np.eye(4) * 1000
            kf.Q = np.eye(4) * process_noise
            kf.R = np.eye(2) * measurement_noise
            self.filters.append(kf)
    
    def update(self, points):
        smoothed_points = []
        for i, (x, y) in enumerate(points):
            if np.isnan(x) or np.isnan(y):
                self.filters[i].predict()
                smoothed = self.filters[i].x[:2]
            else:
                self.filters[i].predict()
                self.filters[i].update([x, y])
                smoothed = self.filters[i].x[:2]
            smoothed_points.append(smoothed)
        return np.array(smoothed_points).squeeze(-1)

# ---------------------- Âπ≥ÊªëÂáΩÊï∞ ----------------------
def smooth_keypoints(kpts_buffer, conf_buffer=None, method="ewm", weights=None):
    if method == "none":
        if kpts_buffer:
            last_kpts = kpts_buffer[-1]
            last_conf = conf_buffer[-1] if conf_buffer else None
            return last_kpts, last_conf
        else:
            return None, None

    def to_array_with_nan(buffer, shape_expected):
        arr = []
        for k in buffer:
            if k is None:
                arr.append(np.full(shape_expected, np.nan))
            else:
                pad = [(0, shape_expected[i] - k.shape[i]) for i in range(len(shape_expected))]
                arr.append(np.pad(k, pad, mode='constant', constant_values=np.nan))
        return np.array(arr)

    num_frames = len(kpts_buffer)
    max_instances = max([0 if k is None else k.shape[0] for k in kpts_buffer])
    num_kpts = max([0 if k is None else k.shape[1] for k in kpts_buffer])
    shape_expected = (max_instances, num_kpts, 2)
    kpts_arr = to_array_with_nan(kpts_buffer, shape_expected)
    conf_arr = to_array_with_nan(conf_buffer, (max_instances, num_kpts)) if conf_buffer else None

    if method == "mean":
        smoothed_kpts = np.nanmean(kpts_arr, axis=0)
        smoothed_conf = np.nanmean(conf_arr, axis=0) if conf_buffer else None
    elif method == "weighted_mean":
        weights = np.array(weights if weights is not None else np.ones(num_frames)).reshape(-1, 1, 1, 1)
        smoothed_kpts = np.nansum(kpts_arr * weights, axis=0) / np.sum(weights * (~np.isnan(kpts_arr)), axis=0)
        smoothed_conf = np.nansum(conf_arr * weights.squeeze(-1), axis=0) / np.sum(weights.squeeze(-1) * (~np.isnan(conf_arr)), axis=0) if conf_buffer else None
    elif method == "ewm":
        alpha = 2 / (num_frames + 1)
        smoothed_kpts = kpts_arr[0].copy()
        smoothed_conf = conf_arr[0].copy() if conf_buffer else None
        for f in range(1, num_frames):
            smoothed_kpts = alpha * np.nan_to_num(kpts_arr[f]) + (1 - alpha) * smoothed_kpts
            if conf_buffer:
                smoothed_conf = alpha * np.nan_to_num(conf_arr[f]) + (1 - alpha) * smoothed_conf
    else:
        raise ValueError(f"Unknown smoothing method: {method}")

    return smoothed_kpts, smoothed_conf

# ---------------------- Âä®‰ΩúÂàÜÁ±ªÊ®°Âûã ----------------------
class PoseCNN(nn.Module):
    def __init__(self, num_classes=6):
        super().__init__()
        self.person_conv1 = nn.Conv1d(3, 32, kernel_size=3, padding=1)
        self.person_conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)
        self.person_len = 170

        self.dragon_conv1 = nn.Conv1d(3, 16, kernel_size=3, padding=1)
        self.dragon_conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)
        self.dragon_len = 9

        self.fc1 = nn.Linear(64 * self.person_len + 32 * self.dragon_len, 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.dropout = nn.Dropout(0.3)

    def forward(self, person_x, dragon_x):
        person_flat = person_x.reshape(person_x.size(0), 10*17, 3)
        dragon_flat = dragon_x.reshape(dragon_x.size(0), 1*9, 3)

        person_x = person_flat.transpose(1, 2)
        dragon_x = dragon_flat.transpose(1, 2)

        p = F.relu(self.person_conv1(person_x))
        p = F.relu(self.person_conv2(p))
        p = p.flatten(start_dim=1)

        d = F.relu(self.dragon_conv1(dragon_x))
        d = F.relu(self.dragon_conv2(d))
        d = d.flatten(start_dim=1)

        x = torch.cat([p, d], dim=1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

def load_classification_model(model_path, device):
    model_path = Path(model_path)
    if not model_path.is_absolute():
        model_path = SCRIPT_DIR / model_path
    
    checkpoint = torch.load(model_path, map_location=device)
    model = PoseCNN(num_classes=len(checkpoint['classes']))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    return model, checkpoint['classes']

def build_class_inputs(person_results, dragon_results, frame_wh,
                       num_person=10, num_person_kpts=17, num_dragon_kpts=9):
    w, h = frame_wh
    person_array = np.zeros((num_person, num_person_kpts, 3), dtype=np.float32)
    dragon_array = np.zeros((1, num_dragon_kpts, 3), dtype=np.float32)

    if person_results is not None and len(person_results) > 0 and person_results[0].keypoints is not None:
        kpts_all = person_results[0].keypoints
        xy = kpts_all.xy.cpu().numpy()
        confs = kpts_all.conf.cpu().numpy()

        mean_conf = np.nanmean(confs, axis=1)
        order = np.argsort(mean_conf)[::-1]

        for i, idx in enumerate(order[:num_person]):
            k_xy = xy[idx]
            k_conf = confs[idx]
            x_norm = (k_xy[:, 0] / float(w)).astype(np.float32)
            y_norm = (k_xy[:, 1] / float(h)).astype(np.float32)
            v = k_conf.astype(np.float32)
            k_stack = np.stack([x_norm, y_norm, v], axis=1)
            L = min(num_person_kpts, k_stack.shape[0])
            person_array[i, :L, :] = k_stack[:L, :]

    if dragon_results is not None and len(dragon_results) > 0 and dragon_results[0].keypoints is not None:
        kpts_all = dragon_results[0].keypoints
        xy = kpts_all.xy.cpu().numpy()
        confs = kpts_all.conf.cpu().numpy()
        boxes = getattr(dragon_results[0], 'boxes', None)
        if boxes is not None and len(boxes) > 0 and hasattr(boxes, 'conf'):
            inst_scores = boxes.conf.cpu().numpy()
            best_idx = int(np.argmax(inst_scores))
        else:
            mean_conf = np.nanmean(confs, axis=1)
            best_idx = int(np.argmax(mean_conf))

        k_xy = xy[best_idx]
        k_conf = confs[best_idx]
        x_norm = (k_xy[:, 0] / float(w)).astype(np.float32)
        y_norm = (k_xy[:, 1] / float(h)).astype(np.float32)
        v = k_conf.astype(np.float32)
        k_stack = np.stack([x_norm, y_norm, v], axis=1)
        L = min(num_dragon_kpts, k_stack.shape[0])
        dragon_array[0, :L, :] = k_stack[:L, :]

    return person_array, dragon_array

def classify_action(model, classes, person_array, dragon_array, device):
    if person_array.shape != (10, 17, 3):
        raise ValueError(f"person_array ÂΩ¢Áä∂ÈîôËØØÔºåÂ∫î‰∏∫ (10, 17, 3)ÔºåÂΩìÂâç {person_array.shape}")
    if dragon_array.shape != (1, 9, 3):
        raise ValueError(f"dragon_array ÂΩ¢Áä∂ÈîôËØØÔºåÂ∫î‰∏∫ (1, 9, 3)ÔºåÂΩìÂâç {dragon_array.shape}")

    classes_dict = {"BZ": "ÂÖ´Â≠óÁ±ª", "DC":"Âçï‰æßÁ±ª", "CT": "Á©øËÖæÁ±ª", "FG": "ÁøªÊªöÁ±ª", "YL": "Ê∏∏ÈæôÁ±ª", "ZX": "ÈÄ†ÂûãÁ±ª"}
    classes = [classes_dict.get(c, c) for c in classes]

    person_tensor = torch.tensor(person_array, dtype=torch.float32).unsqueeze(0).to(device)
    dragon_tensor = torch.tensor(dragon_array, dtype=torch.float32).unsqueeze(0).to(device)

    with torch.no_grad():
        out = model(person_tensor, dragon_tensor)
        probs = F.softmax(out, dim=1)
        pred = torch.argmax(probs, dim=1).item()

    return classes[pred]

def put_chinese_text(img, text, pos=(30,80), color=(0,255,0), font_size=32):
    try:
        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        try:
            font = ImageFont.truetype("simsun.ttc", font_size)
        except:
            font = ImageFont.load_default()
        draw.text(pos, text, font=font, fill=color)
        img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
    except Exception as e:
        cv2.putText(img, text, pos, cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
    return img


# ---------------------- ÂõæÂÉèÊ£ÄÊµã ----------------------
def process_image(person_model, dragon_model, img_path, confs, device,
                  save_json=False, save_txt=False, 
                  single_dragon=True, only_person=False, only_dragon=False,
                  classify=False, classify_model=None, verbose=False,
                  node_colors=None, node_size=10, line_color=None, line_thickness=6):
    node_colors = node_colors or DEFAULT_NODE_COLORS
    line_color = line_color or DEFAULT_LINE_COLOR
    
    person_conf, dragon_conf, person_kpt_conf, dragon_kpt_conf = confs

    img = cv2.imread(img_path)
    if img is None:
        raise FileNotFoundError(f"Êó†Ê≥ïËØªÂèñÂõæÁâá: {img_path}")
    
    person_results = None
    dragon_results = None
    

    person_model_path = Path(person_model)
    if not person_model_path.is_absolute():
        person_model_path = MODELS_DIR / person_model_path
    
    dragon_model_path = Path(dragon_model)
    if not dragon_model_path.is_absolute():
        dragon_model_path = MODELS_DIR / dragon_model_path

    if not only_dragon:
        model_person = YOLO(str(person_model_path))
        model_person.to(device)
        person_results = model_person(img, conf=person_conf, verbose=verbose)

    if not only_person:
        model_dragon = YOLO(str(dragon_model_path))
        model_dragon.to(device)
        dragon_results = model_dragon(img, conf=dragon_conf, verbose=verbose)
    
    if only_dragon or only_person:
        classify = False

    img_out = img.copy()
    if person_results is not None:
        img_out = person_results[0].plot(boxes=False)

    if dragon_results and dragon_results[0].keypoints is not None:
        boxes = dragon_results[0].boxes

        if boxes is not None and len(boxes) > 0:
            kpts = dragon_results[0].keypoints.xy.cpu().numpy()
            conf = dragon_results[0].keypoints.conf.cpu().numpy()
            if single_dragon:
                best_idx = np.argmax(boxes.conf.cpu().numpy())
                kpts = kpts[best_idx:best_idx+1]
                conf = conf[best_idx:best_idx+1]

            for i, kp_set in enumerate(kpts):
                for j, ((x, y), c) in enumerate(zip(kp_set, conf[i])):
                    if c > dragon_kpt_conf:
                        color = node_colors[j % len(node_colors)]
                        cv2.circle(img_out, (int(x), int(y)), node_size, color, -1, lineType=cv2.LINE_AA)
                        cv2.putText(img_out, str(j + 1), (int(x) + 5, int(y) - 5),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)
                for a, b in DRAGON_SKELETON:
                    if conf[i][a] > dragon_kpt_conf and conf[i][b] > dragon_kpt_conf:
                        pt1, pt2 = tuple(map(int, kp_set[a])), tuple(map(int, kp_set[b]))
                        cv2.line(img_out, pt1, pt2, line_color, line_thickness)

    if classify and classify_model is not None:
        classify_model_obj, classes = load_classification_model(MODELS_DIR / classify_model, device)
        num_person, num_person_kpts = 10, 17
        num_dragon_kpts = 9
        person_array = np.zeros((num_person, num_person_kpts, 3), dtype=np.float32)
        dragon_array = np.zeros((1, num_dragon_kpts, 3), dtype=np.float32)

        if person_results is not None and len(person_results[0].keypoints) > 0:
            kpts_all = person_results[0].keypoints
            confs = kpts_all.conf.cpu().numpy()
            xy = kpts_all.xy.cpu().numpy()
            mean_conf = confs.mean(axis=1)
            order = np.argsort(mean_conf)[::-1]
            for i, idx in enumerate(order[:num_person]):
                x = xy[idx, :, 0]
                y = xy[idx, :, 1]
                v = confs[idx]
                person_array[i, :len(x), :] = np.stack([x, y, v], axis=1)

        if dragon_results is not None and len(dragon_results[0].keypoints) > 0:
            kpts = dragon_results[0].keypoints.xy.cpu().numpy()[0]
            conf = dragon_results[0].keypoints.conf.cpu().numpy()[0]
            dragon_array[0, :len(kpts), :] = np.stack([kpts[:, 0], kpts[:, 1], conf], axis=1)

        action_label = classify_action(classify_model_obj, classes, person_array, dragon_array, device)
        img_out = put_chinese_text(img_out, f"Action: {action_label}")

    output_img_path = OUTPUT_DIR / "output_image.jpg"
    cv2.imwrite(str(output_img_path), img_out)

    if save_json:
        if person_results is not None:
            person_labels = json.loads(person_results[0].to_json())
            (OUTPUT_DIR / "person.json").write_text(json.dumps(person_labels, indent=2), encoding='utf-8')
        if dragon_results is not None:
            dragon_labels = json.loads(dragon_results[0].to_json())
            (OUTPUT_DIR / "dragon.json").write_text(json.dumps(dragon_labels, indent=2), encoding='utf-8')

    if save_txt:
        if person_results is not None:
            person_results[0].save_txt(str(OUTPUT_DIR / "person.txt"))
        if dragon_results is not None:
            dragon_results[0].save_txt(str(OUTPUT_DIR / "dragon.txt"))

    return img_out, output_img_path

# ---------------------- ËßÜÈ¢ëÊ£ÄÊµã ----------------------
def process_video(person_model, dragon_model, video_path, confs, realtime_filter_method, smooth, device,
                  save_json=False, save_txt=False, show_preview=True,
                  single_dragon=True, only_person=False, only_dragon=False, 
                  classify=False, classify_model=None, save_video=False, verbose=False,
                  node_colors=None, node_size=10, line_color=None, line_thickness=6,
                  gpu_monitor=None, status_callback=None):
    node_colors = node_colors or DEFAULT_NODE_COLORS
    line_color = line_color or DEFAULT_LINE_COLOR
    
    person_conf, dragon_conf, person_kpt_conf, dragon_kpt_conf = confs

    person_model_path = Path(person_model)
    if not person_model_path.is_absolute():
        person_model_path = MODELS_DIR / person_model_path
    
    dragon_model_path = Path(dragon_model)
    if not dragon_model_path.is_absolute():
        dragon_model_path = MODELS_DIR / dragon_model_path

    model_person = YOLO(str(person_model_path)).to(device) if not only_dragon else None
    model_dragon = YOLO(str(dragon_model_path)).to(device) if not only_person else None

    realtime_filter = None
    if realtime_filter_method == 'ema':
        realtime_filter = EMAFilter(alpha=0.3)
    elif realtime_filter_method == 'kalman':
        realtime_filter = KalmanFilterWrapper(
            num_points=len(DRAGON_KEYPOINT_NAMES),
            dt=1/30.0,
            process_noise=0.2,
            measurement_noise=10.0
        )

    smooth_window = 3
    frame_id = 0
    dragon_kpts_buffer = deque(maxlen=2 * smooth_window + 1)
    dragon_conf_buffer = deque(maxlen=2 * smooth_window + 1)
    frame_buffer = deque(maxlen=2 * smooth_window + 1)
    
    current_smoothed_kpts = None
    current_smoothed_conf = None

    if only_dragon or only_person:
        classify = False

    classify_model_obj = None
    classes = None
    if classify and classify_model is not None:
        classify_model_obj, classes = load_classification_model(MODELS_DIR / classify_model, device)
        class_buffer = deque(maxlen=30)
        display_class = None

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise FileNotFoundError(f"Êó†Ê≥ïÊâìÂºÄËßÜÈ¢ëÊñá‰ª∂: {video_path}")

    w, h = int(cap.get(3)), int(cap.get(4))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    output_video_path = None
    
    if save_video:
        fps = cap.get(cv2.CAP_PROP_FPS)
        # Á°Æ‰øùFPSÊúâÊïà
        if fps <= 0:
            fps = 30  # ‰ΩøÁî®ÈªòËÆ§FPS
            
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        output_video_path = OUTPUT_DIR / "output_video.mp4"
        try:
            out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (w, h))
            if not out.isOpened():
                raise Exception(f"Êó†Ê≥ïÂàùÂßãÂåñËßÜÈ¢ëÂÜôÂÖ•Âô®ÔºåÂèØËÉΩÊòØÁºñËß£Á†ÅÂô®ÈóÆÈ¢ò: {output_video_path}")
        except Exception as e:
            st.error(f"ËßÜÈ¢ëÂÜôÂÖ•ÂàùÂßãÂåñÂ§±Ë¥•: {str(e)}")
            return None

    # È¢ÑËßàÂç†‰ΩçÁ¨¶
    preview_placeholder = st.empty() if show_preview else None
    gpu_status_placeholder = st.empty() if gpu_monitor and gpu_monitor.use_gpu else None

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            # Êõ¥Êñ∞ËøõÂ∫¶
            if status_callback:
                progress = min(frame_id / total_frames, 1.0)
                status_callback(f"Â§ÑÁêÜËøõÂ∫¶: {frame_id}/{total_frames} Â∏ß", progress)

            # Êõ¥Êñ∞GPUÁõëÊéß
            if gpu_monitor and gpu_status_placeholder:
                gpu_monitor.update_fps()
                mem_used, mem_cached = gpu_monitor.get_memory_usage()
                cpu_usage = gpu_monitor.get_cpu_usage()
                sys_mem_usage = gpu_monitor.get_system_memory_usage()
                avg_fps = gpu_monitor.get_average_fps()
                
                gpu_status = f"""
                **GPUÁä∂ÊÄÅÁõëÊéß**  
                GPU: {gpu_monitor.gpu_name}  
                ÊòæÂ≠ò‰ΩøÁî®: {mem_used:.2f}GB / {gpu_monitor.memory_total:.2f}GB  
                ÁºìÂ≠òÊòæÂ≠ò: {mem_cached:.2f}GB  
                Âπ≥ÂùáÂ∏ßÁéá: {avg_fps:.1f} FPS  
                CPU‰ΩøÁî®Áéá: {cpu_usage}%  
                Á≥ªÁªüÂÜÖÂ≠ò‰ΩøÁî®Áéá: {sys_mem_usage}%
                """
                gpu_status_placeholder.markdown(gpu_status)

            frame_buffer.append(frame.copy())
            person_results = model_person(frame, conf=person_conf, verbose=verbose) if model_person else None
            dragon_results = model_dragon(frame, conf=dragon_conf, verbose=verbose) if model_dragon else None

            if dragon_results and dragon_results[0].keypoints is not None: 
                boxes = dragon_results[0].boxes
                if boxes is not None and len(boxes) > 0:
                    kpts = dragon_results[0].keypoints.xy.cpu().numpy()
                    conf = dragon_results[0].keypoints.conf.cpu().numpy()
                    if single_dragon:
                        best_idx = np.argmax(boxes.conf.cpu().numpy())
                        kpts = kpts[best_idx:best_idx+1]
                        conf = conf[best_idx:best_idx+1]
                    
                    if realtime_filter is not None and len(kpts) > 0:
                        current_kpts = kpts[0]
                        smoothed_kpts = realtime_filter.update(current_kpts)
                        current_smoothed_kpts = smoothed_kpts.reshape(1, -1, 2)
                        current_smoothed_conf = conf
                    else:
                        current_smoothed_kpts = kpts
                        current_smoothed_conf = conf
                    
                    dragon_kpts_buffer.append(current_smoothed_kpts)
                    dragon_conf_buffer.append(current_smoothed_conf)

            else:
                empty_kpts = np.full((1, len(DRAGON_KEYPOINT_NAMES), 2), np.nan)
                empty_conf = np.full((1, len(DRAGON_KEYPOINT_NAMES)), np.nan)
                dragon_kpts_buffer.append(empty_kpts)
                dragon_conf_buffer.append(empty_conf)
                current_smoothed_kpts = None
                current_smoothed_conf = None

            img = frame.copy()
            if person_results is not None:
                img = person_results[0].plot(boxes=False)

            draw_kpts = None
            draw_conf = None

            if len(dragon_kpts_buffer) == dragon_kpts_buffer.maxlen and smooth != "none":
                smoothed_kpts, smoothed_conf = smooth_keypoints(dragon_kpts_buffer, dragon_conf_buffer, method=smooth)
                if smoothed_kpts is not None and len(smoothed_kpts) > 0:
                    draw_kpts = smoothed_kpts
                    draw_conf = smoothed_conf
            elif current_smoothed_kpts is not None and len(current_smoothed_kpts) > 0:
                draw_kpts = current_smoothed_kpts
                draw_conf = current_smoothed_conf
            elif dragon_kpts_buffer:
                draw_kpts = dragon_kpts_buffer[-1]
                draw_conf = dragon_conf_buffer[-1] if dragon_conf_buffer else None

            if draw_kpts is not None and len(draw_kpts) > 0:
                kp_set, conf_set = draw_kpts[0], draw_conf[0]
                for j, ((x, y), c) in enumerate(zip(kp_set, conf_set)):
                    if not np.isnan(x) and not np.isnan(y) and c > dragon_kpt_conf:
                        color = node_colors[j % len(node_colors)]
                        cv2.circle(img, (int(x), int(y)), node_size, color, -1)
                        cv2.putText(img, str(j + 1), (int(x)+5, int(y)-5),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                for a, b in DRAGON_SKELETON:
                    if (conf_set[a] > dragon_kpt_conf and conf_set[b] > dragon_kpt_conf and
                        not np.isnan(kp_set[a][0]) and not np.isnan(kp_set[b][0])):
                        cv2.line(img, tuple(map(int, kp_set[a])), tuple(map(int, kp_set[b])), line_color, line_thickness)

                if classify and classes is not None and draw_kpts is not None:
                    try:
                        frame_wh = (w, h)
                        person_array, dragon_array = build_class_inputs(person_results, dragon_results, frame_wh)
                        label = classify_action(classify_model_obj, classes, person_array, dragon_array, device)
                    except Exception as e:
                        label = None

                    if label is not None:
                        class_buffer.append(label)

                    if len(class_buffer) == 30:
                        stable_class = statistics.mode(class_buffer)
                        display_class = stable_class

            if classify and display_class is not None:
                img = put_chinese_text(img, f"Action: {display_class}")

            # ÂÆûÊó∂È¢ÑËßà
            if show_preview and preview_placeholder is not None:
                preview_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                preview_placeholder.image(preview_img, caption=f"Â∏ß {frame_id}", width='stretch')

            if save_video and 'out' in locals() and out.isOpened():
                try:
                    out.write(img)
                except Exception as e:
                    st.error(f"ÂÜôÂÖ•ËßÜÈ¢ëÂ∏ßÂ§±Ë¥• (Â∏ß {frame_id}): {str(e)}")
                    st.error(f"Â∏ßÂ∞∫ÂØ∏: {img.shape}, ËßÜÈ¢ëÂ∞∫ÂØ∏: ({w}, {h})")

            frame_id += 1

    except Exception as e:
        st.error(f"ËßÜÈ¢ëÂ§ÑÁêÜËøáÁ®ã‰∏≠Âá∫Èîô: {str(e)}")
    finally:
        if save_video and 'out' in locals():
            # Á°Æ‰øùÊâÄÊúâÂ∏ßÈÉΩË¢´ÂÜôÂÖ•
            for i in range(smooth_window):
                if i < len(frame_buffer):
                    try:
                        out.write(frame_buffer[i])
                    except:
                        pass
            out.release()
            # Á°Æ‰øùÊñá‰ª∂Ê≠£Á°ÆÂÖ≥Èó≠
            cv2.destroyAllWindows()
            
            # È™åËØÅËßÜÈ¢ëÊñá‰ª∂
            if output_video_path.exists():
                cap_check = cv2.VideoCapture(str(output_video_path))
                if not cap_check.isOpened():
                    st.error(f"ÁîüÊàêÁöÑËßÜÈ¢ëÊñá‰ª∂Êó†Ê≥ïÊâìÂºÄÔºåÂèØËÉΩÂ∑≤ÊçüÂùè: {output_video_path}")
                else:
                    check_ret, _ = cap_check.read()
                    if not check_ret:
                        st.error(f"ÁîüÊàêÁöÑËßÜÈ¢ëÊñá‰ª∂‰∏∫Á©∫ÊàñÊçüÂùèÔºåÊó†Ê≥ïËØªÂèñÂ∏ß: {output_video_path}")
                    cap_check.release()
            else:
                st.error(f"ËßÜÈ¢ëÊñá‰ª∂Êú™ÁîüÊàê: {output_video_path}")

        cap.release()
        if gpu_status_placeholder:
            gpu_status_placeholder.empty()
            
    return output_video_path

# ---------------------- ÊëÑÂÉèÂ§¥Ê£ÄÊµã ----------------------
def process_camera(person_model, dragon_model, cam_id, confs, realtime_filter_method, device,
                   single_dragon=True, only_person=False, only_dragon=False,
                   classify=False, classify_model=None, save_video=False,
                   preview_placeholder=None, stop_flag=None,
                   node_colors=None, node_size=10, line_color=None, line_thickness=6,
                   gpu_monitor=None):
    node_colors = node_colors or DEFAULT_NODE_COLORS
    line_color = line_color or DEFAULT_LINE_COLOR
    
    person_conf, dragon_conf, person_kpt_conf, dragon_kpt_conf = confs
    
    person_model_path = Path(person_model)
    if not person_model_path.is_absolute():
        person_model_path = MODELS_DIR / person_model_path
    
    dragon_model_path = Path(dragon_model)
    if not dragon_model_path.is_absolute():
        dragon_model_path = MODELS_DIR / dragon_model_path

    model_person = YOLO(str(person_model_path)).to(device) if not only_dragon else None
    model_dragon = YOLO(str(dragon_model_path)).to(device) if not only_person else None

    realtime_filter = None
    if realtime_filter_method == 'ema':
        realtime_filter = EMAFilter(alpha=0.4)
    elif realtime_filter_method == 'kalman':
        realtime_filter = KalmanFilterWrapper(
            num_points=len(DRAGON_KEYPOINT_NAMES),
            dt=1/30.0,
            process_noise=0.1,
            measurement_noise=5.0
        )

    if only_dragon or only_person:
        classify = False

    classify_model_obj = None
    classes = None
    if classify and classify_model is not None:
        classify_model_obj, classes = load_classification_model(MODELS_DIR / classify_model, device)
        class_buffer = deque(maxlen=30)
        display_class = None

    cap = cv2.VideoCapture(cam_id)
    if not cap.isOpened():
        preview_placeholder.error(f"Êó†Ê≥ïÊâìÂºÄÊëÑÂÉèÂ§¥ {cam_id}ÔºåËØ∑Ê£ÄÊü•ÊëÑÂÉèÂ§¥ËøûÊé•")
        return
    
    # ËßÜÈ¢ëÂΩïÂà∂ËÆæÁΩÆ
    video_writer = None
    output_video_path = None
    if save_video:
        output_video_path = OUTPUT_DIR / "camera_output.mp4"
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0:
            fps = 20  # ‰ΩøÁî®ÈªòËÆ§FPS
        
        try:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            video_writer = cv2.VideoWriter(str(output_video_path), fourcc, fps, (frame_width, frame_height))
            if not video_writer.isOpened():
                raise Exception(f"Êó†Ê≥ïÂàùÂßãÂåñÊëÑÂÉèÂ§¥ËßÜÈ¢ëÂÜôÂÖ•Âô®ÔºåÂèØËÉΩÊòØÁºñËß£Á†ÅÂô®ÈóÆÈ¢ò")
        except Exception as e:
            st.error(f"ÊëÑÂÉèÂ§¥ËßÜÈ¢ëÂÜôÂÖ•ÂàùÂßãÂåñÂ§±Ë¥•: {str(e)}")
            save_video = False

    # Áä∂ÊÄÅÊòæÁ§∫
    status_text = st.empty()
    status_text.info("ÊëÑÂÉèÂ§¥ËøêË°å‰∏≠...")
    
    # GPUÁõëÊéßÂç†‰ΩçÁ¨¶
    gpu_status_placeholder = st.empty() if gpu_monitor and gpu_monitor.use_gpu else None

    try:
        while True:
            # Ê£ÄÊü•ÂÅúÊ≠¢Êù°‰ª∂
            if stop_flag and stop_flag():
                break
                
            # Êõ¥Êñ∞GPUÁõëÊéß
            if gpu_monitor and gpu_status_placeholder:
                gpu_monitor.update_fps()
                mem_used, mem_cached = gpu_monitor.get_memory_usage()
                cpu_usage = gpu_monitor.get_cpu_usage()
                sys_mem_usage = gpu_monitor.get_system_memory_usage()
                avg_fps = gpu_monitor.get_average_fps()
                
                gpu_status = f"""
                **GPUÁä∂ÊÄÅÁõëÊéß**  
                GPU: {gpu_monitor.gpu_name}  
                ÊòæÂ≠ò‰ΩøÁî®: {mem_used:.2f}GB / {gpu_monitor.memory_total:.2f}GB  
                ÁºìÂ≠òÊòæÂ≠ò: {mem_cached:.2f}GB  
                Âπ≥ÂùáÂ∏ßÁéá: {avg_fps:.1f} FPS  
                CPU‰ΩøÁî®Áéá: {cpu_usage}%  
                Á≥ªÁªüÂÜÖÂ≠ò‰ΩøÁî®Áéá: {sys_mem_usage}%
                """
                gpu_status_placeholder.markdown(gpu_status)

            ret, frame = cap.read()
            if not ret:
                preview_placeholder.error("Êó†Ê≥ï‰ªéÊëÑÂÉèÂ§¥ËØªÂèñÂ∏ß")
                break
                
            person_results = model_person(frame, conf=person_conf, verbose=False) if model_person else None
            dragon_results = model_dragon(frame, conf=dragon_conf, verbose=False) if model_dragon else None

            img = frame.copy()
            if person_results is not None:
                img = person_results[0].plot(boxes=False)

            if dragon_results and dragon_results[0].keypoints is not None:
                boxes = dragon_results[0].boxes
                if boxes is not None and len(boxes) > 0:
                    kpts = dragon_results[0].keypoints.xy.cpu().numpy()
                    conf = dragon_results[0].keypoints.conf.cpu().numpy()
                    if single_dragon:
                        best_idx = np.argmax(boxes.conf.cpu().numpy())
                        kpts = kpts[best_idx:best_idx+1]
                        conf = conf[best_idx:best_idx+1]
                    
                    if realtime_filter is not None and len(kpts) > 0:
                        current_kpts = kpts[0]
                        smoothed_kpts = realtime_filter.update(current_kpts)
                        kpts[0] = smoothed_kpts.reshape(1, -1, 2)
                    
                    for i, kp_set in enumerate(kpts):
                        for j, ((x, y), c) in enumerate(zip(kp_set, conf[i])):
                            if c > dragon_kpt_conf:
                                color = node_colors[j % len(node_colors)]
                                cv2.circle(img, (int(x), int(y)), node_size, color, -1)
                                cv2.putText(img, str(j + 1), (int(x)+5, int(y)-5),
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                        for (a, b) in DRAGON_SKELETON:
                            if conf[i][a] > dragon_kpt_conf and conf[i][b] > dragon_kpt_conf:
                                pt1, pt2 = tuple(map(int, kp_set[a])), tuple(map(int, kp_set[b]))
                                cv2.line(img, pt1, pt2, line_color, line_thickness)
                            
                    if classify and classes is not None:
                        try:
                            frame_wh = (frame.shape[1], frame.shape[0])
                            person_array, dragon_array = build_class_inputs(person_results, dragon_results, frame_wh)
                            label = classify_action(classify_model_obj, classes, person_array, dragon_array, device)
                        except Exception as e:
                            label = None

                        if label is not None:
                            class_buffer.append(label)

                        if len(class_buffer) == 30:
                            stable_class = statistics.mode(class_buffer)
                            display_class = stable_class
            
            if classify and display_class is not None:
                img = put_chinese_text(img, f"Action: {display_class}")

            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            preview_placeholder.image(img_rgb, channels="RGB", width='stretch')

            if save_video and video_writer is not None and video_writer.isOpened():
                try:
                    video_writer.write(img)
                except Exception as e:
                    st.error(f"ÂÜôÂÖ•ÊëÑÂÉèÂ§¥Â∏ßÂ§±Ë¥•: {str(e)}")
                    st.error(f"Â∏ßÂ∞∫ÂØ∏: {img.shape}, ËßÜÈ¢ëÂ∞∫ÂØ∏: ({frame_width}, {frame_height})")

            # Â∞èÂª∂ËøüÈò≤Ê≠¢UIÂç°È°ø
            time.sleep(0.01)

    except Exception as e:
        preview_placeholder.error(f"ÊëÑÂÉèÂ§¥Â§ÑÁêÜÂá∫Èîô: {str(e)}")
    
    finally:
        cap.release()
        if video_writer is not None:
            video_writer.release()
            cv2.destroyAllWindows()
            
            # È™åËØÅËßÜÈ¢ëÊñá‰ª∂
            if output_video_path and output_video_path.exists():
                cap_check = cv2.VideoCapture(str(output_video_path))
                if not cap_check.isOpened():
                    st.error(f"ÁîüÊàêÁöÑÊëÑÂÉèÂ§¥ËßÜÈ¢ëÊñá‰ª∂Êó†Ê≥ïÊâìÂºÄÔºåÂèØËÉΩÂ∑≤ÊçüÂùè: {output_video_path}")
                else:
                    check_ret, _ = cap_check.read()
                    if not check_ret:
                        st.error(f"ÁîüÊàêÁöÑÊëÑÂÉèÂ§¥ËßÜÈ¢ëÊñá‰ª∂‰∏∫Á©∫ÊàñÊçüÂùèÔºåÊó†Ê≥ïËØªÂèñÂ∏ß: {output_video_path}")
                    cap_check.release()
            elif save_video:
                st.error(f"ÊëÑÂÉèÂ§¥ËßÜÈ¢ëÊñá‰ª∂Êú™ÁîüÊàê: {output_video_path}")
        
        status_text.success("ÊëÑÂÉèÂ§¥Â∑≤ÂÅúÊ≠¢")
        if gpu_status_placeholder:
            gpu_status_placeholder.empty()
            
        return output_video_path
    

def process_camera_stream(params, gpu_monitor=None):
    """ÂÆåÊï¥ÊëÑÂÉèÂ§¥ËØÜÂà´ÈÄªËæë"""
    RTC_CONFIGURATION = RTCConfiguration({
        "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
    })

    class VideoProcessor(VideoProcessorBase):
        def __init__(self):
            # --- ÂàùÂßãÂåñÂèÇÊï∞ ---
            self.model_person = None
            self.model_dragon = None
            self.device = params['device']
            self.node_colors = params['node_colors']
            self.line_color = params['line_color']
            self.node_size = params['node_size']
            self.line_thickness = params['line_thickness']
            self.person_conf, self.dragon_conf, self.person_kpt_conf, self.dragon_kpt_conf = params['confs']
            self.single_dragon = params.get('single_dragon', True)
            self.only_person = params.get('only_person', False)
            self.only_dragon = params.get('only_dragon', False)

            # --- ÂàÜÁ±ªÊ®°Âûã ---
            self.classify = params.get('classify', False)
            self.classify_model = params.get('classify_model', None)
            self.classify_model_obj = None
            self.classes = None
            self.class_buffer = deque(maxlen=30)
            self.display_class = None

            # --- ÂÆûÊó∂Âπ≥ÊªëÊª§Ê≥¢Âô® ---
            self.realtime_filter_method = params.get('realtime_filter_method', None)
            if self.realtime_filter_method == 'ema':
                self.realtime_filter = EMAFilter(alpha=0.4)
            elif self.realtime_filter_method == 'kalman':
                self.realtime_filter = KalmanFilterWrapper(
                    num_points=len(DRAGON_KEYPOINT_NAMES),
                    dt=1 / 30.0,
                    process_noise=0.1,
                    measurement_noise=5.0,
                )
            else:
                self.realtime_filter = None

            # --- ËßÜÈ¢ë‰øùÂ≠ò ---
            self.save_video = params.get('save_video', False)
            self.output_video_path = OUTPUT_DIR / "camera_output.mp4"
            self.video_writer = None
            self.fps = 0.0
            self.last_time = time.time()

        def _init_models(self):
            """ÊáíÂä†ËΩΩÊ®°Âûã"""
            if self.model_person is None and not self.only_dragon:
                person_model_path = Path(params['person_model'])
                if not person_model_path.is_absolute():
                    person_model_path = MODELS_DIR / person_model_path
                self.model_person = YOLO(str(person_model_path)).to(self.device)

            if self.model_dragon is None and not self.only_person:
                dragon_model_path = Path(params['dragon_model'])
                if not dragon_model_path.is_absolute():
                    dragon_model_path = MODELS_DIR / dragon_model_path
                self.model_dragon = YOLO(str(dragon_model_path)).to(self.device)

            if self.classify and self.classify_model_obj is None and self.classify_model:
                self.classify_model_obj, self.classes = load_classification_model(
                    MODELS_DIR / self.classify_model, self.device
                )

            print("[INFO] Ê®°ÂûãÂä†ËΩΩÂÆåÊàêÔºåÂèØÂºÄÂßãÂÆûÊó∂Ê£ÄÊµã")

        def _init_video_writer(self, frame_shape):
            if not self.save_video:
                return
            h, w = frame_shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            self.video_writer = cv2.VideoWriter(str(self.output_video_path), fourcc, 20.0, (w, h))

        def recv(self, frame):
            # --- Ê®°ÂûãÂàùÂßãÂåñ ---
            self._init_models()
            img = frame.to_ndarray(format="bgr24")

            # --- FPS ËÆ°ÁÆó ---
            now = time.time()
            dt = now - self.last_time
            if dt > 0:
                self.fps = 1.0 / dt
            self.last_time = now

            # --- ‰∫∫‰ΩìÊ£ÄÊµã ---
            results_person = None
            if self.model_person:
                results_person = self.model_person(img, conf=self.person_conf, verbose=False)
                img = results_person[0].plot(boxes=False)

            # --- ÈæôÊ£ÄÊµã ---
            results_dragon = None
            if self.model_dragon:
                results_dragon = self.model_dragon(img, conf=self.dragon_conf, verbose=False)

            # --- ÈæôÂÖ≥ÈîÆÁÇπÁªòÂà∂ ---
            if results_dragon and results_dragon[0].keypoints is not None:
                boxes = results_dragon[0].boxes
                kpts = results_dragon[0].keypoints.xy.cpu().numpy()
                conf = results_dragon[0].keypoints.conf.cpu().numpy()

                if self.single_dragon and boxes is not None and len(boxes) > 0:
                    best_idx = np.argmax(boxes.conf.cpu().numpy())
                    kpts = kpts[best_idx:best_idx + 1]
                    conf = conf[best_idx:best_idx + 1]

                if self.realtime_filter is not None and len(kpts) > 0:
                    smoothed = self.realtime_filter.update(kpts[0])
                    kpts[0] = smoothed.reshape(-1, 2)

                for i, kp_set in enumerate(kpts):
                    for j, ((x, y), c) in enumerate(zip(kp_set, conf[i])):
                        if c > self.dragon_kpt_conf:
                            color = self.node_colors[j % len(self.node_colors)]
                            cv2.circle(img, (int(x), int(y)), self.node_size, color, -1)
                            cv2.putText(img, str(j + 1), (int(x)+5, int(y)-5),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                    for (a, b) in DRAGON_SKELETON:
                        if conf[i][a] > self.dragon_kpt_conf and conf[i][b] > self.dragon_kpt_conf:
                            pt1, pt2 = tuple(map(int, kp_set[a])), tuple(map(int, kp_set[b]))
                            cv2.line(img, pt1, pt2, self.line_color, self.line_thickness)

            # --- Âä®‰ΩúÂàÜÁ±ª ---
            if self.classify and self.classify_model_obj is not None:
                try:
                    frame_wh = (img.shape[1], img.shape[0])
                    person_array, dragon_array = build_class_inputs(results_person, results_dragon, frame_wh)
                    label = classify_action(self.classify_model_obj, self.classes, person_array, dragon_array, self.device)
                    if label is not None:
                        self.class_buffer.append(label)
                        if len(self.class_buffer) == self.class_buffer.maxlen:
                            stable_class = statistics.mode(self.class_buffer)
                            if stable_class != self.display_class:
                                self.display_class = stable_class
                except Exception:
                    pass

                if self.display_class:
                    img = put_chinese_text(
                        img, 
                        f"Action:{self.display_class}",
                        pos=(30, 60),
                        color=(0, 255, 0),
                        font_size=32
                    )

            # --- ËßÜÈ¢ë‰øùÂ≠ò ---
            if self.save_video and self.video_writer is None:
                self._init_video_writer(img.shape)
            if self.video_writer:
                try:
                    self.video_writer.write(img)
                except Exception as e:
                    print(f"[WARN] ÂÜôÂÖ•ËßÜÈ¢ëÂ§±Ë¥•: {e}")

            # --- GPUÁä∂ÊÄÅÁõëÊéßÔºàÂèØÈÄâÔºâ---
            if gpu_monitor:
                gpu_monitor.update_fps()
                if gpu_monitor.use_gpu:
                    mem_used, mem_cached = gpu_monitor.get_memory_usage()
                    print(f"[GPU] FPS {self.fps:.1f}, Mem {mem_used:.2f} GB")
                else:
                    print(f"[CPU] FPS {self.fps:.1f}")

            return av.VideoFrame.from_ndarray(img, format="bgr24")

        def on_stop(self):
            if self.video_writer:
                self.video_writer.release()
                print("[INFO] ÂΩïÂà∂ËßÜÈ¢ëÂ∑≤‰øùÂ≠ò:", self.output_video_path)

    # --- ÂêØÂä® WebRTC ---
    webrtc_streamer(
        key="camera",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=RTC_CONFIGURATION,
        video_processor_factory=VideoProcessor,
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )



     
# ---------------------- Ê£ÄÊµãÂèØÁî®ÊëÑÂÉèÂ§¥ ----------------------
@st.cache_resource(show_spinner=False)
def get_available_cameras(max_test=5):
    available_cameras = []
    for i in range(max_test):
        cap = cv2.VideoCapture(i)
        if cap.isOpened():
            ret, frame = cap.read()
            if ret:
                available_cameras.append(i)
            cap.release()
    return available_cameras


# ---------------------- Streamlit WebÁïåÈù¢ ----------------------

def main():
    st.set_page_config(
        page_title="Open DragonJot - ËàûÈæôÂä®‰ΩúËØÜÂà´Ê£ÄÊµãÁ≥ªÁªü",
        page_icon="üêâ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # ÂàùÂßãÂåñ‰ºöËØùÁä∂ÊÄÅ
    if 'current_tab' not in st.session_state:
        st.session_state.current_tab = "ÂäüËÉΩÊºîÁ§∫"
    if 'analysis_running' not in st.session_state:
        st.session_state.analysis_running = False
    if 'stop_flag' not in st.session_state:
        st.session_state.stop_flag = False
    if 'output_video_path' not in st.session_state:
        st.session_state.output_video_path = None

    # ‰æßËæπÊ†è - È°∂ÈÉ®LogoÂíåÊñáÂ≠ó
    st.sidebar.markdown("""
    <div style="text-align: center; padding: 20px 0;">
    """, unsafe_allow_html=True)
    
    try:
        st.sidebar.image(str(STATIC_DIR / "logo.jpg"), width='stretch')
    except Exception as e:
        st.sidebar.warning(f"Êú™ÊâæÂà∞LogoÊñá‰ª∂: {STATIC_DIR / 'logo.jpg'}")
    
    # Ê∑ªÂä†Logo‰∏ãÊñπÁöÑÊñáÂ≠ó‰ø°ÊÅØ
    st.sidebar.markdown("""
    <h3 style="text-align: center; margin-top: 10px; margin-bottom: 5px;">Open DragonJot</h3>
    <p style="text-align: center; margin-top: 0; color: #666;">ÁÇπÁùõAIÂºÄÊ∫êÁâà</p>
    </div>
    """, unsafe_allow_html=True)

    # ‰æßËæπÊ†è - ÈÄâÈ°πÂç°ÈÄâÊã©
    st.sidebar.title("ÂØºËà™ËèúÂçï")
    # ‰∏∫ÊØè‰∏™ÊåâÈíÆÊ∑ªÂä†ÂîØ‰∏ÄÁöÑkeyÂèÇÊï∞
    if st.sidebar.button("ÂäüËÉΩÊºîÁ§∫", width='stretch', 
                         type="primary" if st.session_state.current_tab == "ÂäüËÉΩÊºîÁ§∫" else "secondary",
                         key="btn_feature_demo"):
        st.session_state.current_tab = "ÂäüËÉΩÊºîÁ§∫"
    
    if st.sidebar.button("Âø´ÈÄü‰ΩìÈ™å", width='stretch', 
                         type="primary" if st.session_state.current_tab == "Âø´ÈÄü‰ΩìÈ™å" else "secondary",
                         key="btn_quick_start"):
        st.session_state.current_tab = "Âø´ÈÄü‰ΩìÈ™å"
    
    # Êñ∞Â¢ûÂèÇÊï∞ËÆæÁΩÆÁã¨Á´ãÊùøÂùó
    if st.sidebar.button("ÂèÇÊï∞ËÆæÁΩÆ", width='stretch', 
                         type="primary" if st.session_state.current_tab == "ÂèÇÊï∞ËÆæÁΩÆ" else "secondary",
                         key="btn_settings"):
        st.session_state.current_tab = "ÂèÇÊï∞ËÆæÁΩÆ"
    
    # Êñ∞Â¢ûÂºÄÂèëÂõ¢ÈòüÊùøÂùó
    if st.sidebar.button("ÂºÄÂèëÂõ¢Èòü", width='stretch', 
                         type="primary" if st.session_state.current_tab == "ÂºÄÂèëÂõ¢Èòü" else "secondary",
                         key="btn_team"):
        st.session_state.current_tab = "ÂºÄÂèëÂõ¢Èòü"

    # ÂäüËÉΩÊºîÁ§∫È°µÈù¢
    if st.session_state.current_tab == "ÂäüËÉΩÊºîÁ§∫":
        st.markdown("<h1 style='text-align: center;'>Open DragonJot - ÂäüËÉΩÊºîÁ§∫</h1>", unsafe_allow_html=True)
        
        # Â±ïÁ§∫ÈùôÊÄÅÊñá‰ª∂Â§π‰∏≠ÁöÑGIF
        st.subheader("  1. ‰∫∫Èæô‰∏Ä‰ΩìÂßøÊÄÅËØÜÂà´")
        try:
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                # ËØªÂèñ GIF Êñá‰ª∂Âπ∂ÁºñÁ†Å
                gif_path = STATIC_DIR / "skeleton_demo.gif"
                if os.path.exists(gif_path):
                    with open(gif_path, "rb") as f:
                        gif_data = f.read()
                        gif_base64 = base64.b64encode(gif_data).decode()
                    
                    st.markdown(
                        f"""
                        <div style="display: flex; justify-content: center; flex-direction: column; align-items: center;">
                            <img src="data:image/gif;base64,{gif_base64}" style="width: 450px; max-width: 100%;">
                            <p style="text-align: center; font-size: 14px; color: #6B7280;">Êú±‰øäÈπèÊïôÁªÉÂú®Êú±ÂÆ∂ËßíÊùØÂºÄÂú∫Ë°®Êºî</p>
                            <p style="text-align: center; font-size: 14px; color: #6B7280;">ÈæôÈ™®Êû∂ÂÖ≥ÈîÆÁÇπÂÆûÊó∂ËØÜÂà´ÊºîÁ§∫</p>
                        </div>
                        """, 
                        unsafe_allow_html=True
                    )
                else:
                    st.warning(f"Êú™ÊâæÂà∞ÊºîÁ§∫Êñá‰ª∂: {gif_path}")
        except Exception as e:
            st.warning(f"Âä†ËΩΩÊºîÁ§∫Êñá‰ª∂Âá∫Èîô: {e}")

        st.subheader("  2. ‰∫îÂ§ßÂä®‰ΩúÂàÜÁ±ª")
        try:
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                # ËØªÂèñ GIF Êñá‰ª∂Âπ∂ÁºñÁ†Å
                gif_path = STATIC_DIR / "classification_demo.gif"
                if os.path.exists(gif_path):
                    with open(gif_path, "rb") as f:
                        gif_data = f.read()
                        gif_base64 = base64.b64encode(gif_data).decode()
                    
                    st.markdown(
                        f"""
                        <div style="display: flex; justify-content: center; flex-direction: column; align-items: center;">
                            <img src="data:image/gif;base64,{gif_base64}" style="width: 450px; max-width: 100%;">
                            <p style="text-align: center; font-size: 14px; color: #6B7280;">Â§çÊó¶ÈæôÁãÆÂçè‰ºöÊó•Â∏∏ËÆ≠ÁªÉ</p>
                            <p style="text-align: center; font-size: 14px; color: #6B7280;">ËàûÈæôÂä®‰ΩúËá™Âä®ÂàÜÁ±ªÊºîÁ§∫</p>
                        </div>
                        """, 
                        unsafe_allow_html=True
                    )
                else:
                    st.warning(f"Êú™ÊâæÂà∞ÊºîÁ§∫Êñá‰ª∂: {gif_path}")
        except Exception as e:
            st.warning(f"Âä†ËΩΩÊºîÁ§∫Êñá‰ª∂Âá∫Èîô: {e}")

        # st.subheader("  3. Âä®‰ΩúÂÆûÊó∂ËØÑÂàÜ")
        # try:
        #     col1, col2, col3 = st.columns([1, 2, 1])
        #     with col2:
        #         # ËØªÂèñ GIF Êñá‰ª∂Âπ∂ÁºñÁ†Å
        #         gif_path = STATIC_DIR / "score_demo.gif"
        #         if os.path.exists(gif_path):
        #             with open(gif_path, "rb") as f:
        #                 gif_data = f.read()
        #                 gif_base64 = base64.b64encode(gif_data).decode()
                    
        #             st.markdown(
        #                 f"""
        #                 <div style="display: flex; justify-content: center; flex-direction: column; align-items: center;">
        #                     <img src="data:image/gif;base64,{gif_base64}" style="width: 450px; max-width: 100%;">
        #                     <p style="text-align: center; font-size: 14px; color: #6B7280;">ÁÇπÁùõAI‰∏ì‰∏öÁâà</p>
        #                     <p style="text-align: center; margin-top: 10px; font-size: 14px; color: #6B7280;">ËàûÈæôÂä®‰ΩúAIÊâìÂàÜÊºîÁ§∫</p>
        #                 </div>
        #                 """, 
        #                 unsafe_allow_html=True
        #             )
        #         else:
        #             st.warning(f"Êú™ÊâæÂà∞ÊºîÁ§∫Êñá‰ª∂: {gif_path}")
        # except Exception as e:
        #     st.warning(f"Âä†ËΩΩÊºîÁ§∫Êñá‰ª∂Âá∫Èîô: {e}")
        
        st.markdown("""
        ### ÂäüËÉΩËØ¥Êòé
        - Á≥ªÁªüÂèØÂÆûÊó∂ËØÜÂà´ÈæôÈ™®Êû∂ÁöÑ9‰∏™ÂÖ≥ÈîÆËäÇÁÇπ
        - ÊîØÊåÅ‰∫îÂ§ßÁ±ªËàûÈæôÂä®‰ΩúÁöÑËá™Âä®ÂàÜÁ±ª
        - ÊîØÊåÅÂõæÂÉè„ÄÅËßÜÈ¢ë„ÄÅÊëÑÂÉèÂ§¥ËæìÂÖ•
        - Êèê‰æõËäÇÁÇπÈ¢úËâ≤„ÄÅÂ§ßÂ∞èÁ≠â‰∏™ÊÄßÂåñËÆæÁΩÆ
        - ÂèØ‰ΩøÁî®GPUÂä†ÈÄüÊèêÈ´òÂ§ÑÁêÜÊïàÁéá
        
        ÁÇπÂáªÂ∑¶‰æß"Âø´ÈÄü‰ΩìÈ™å"ÂºÄÂßã‰ΩøÁî®Á≥ªÁªüÂäüËÉΩ„ÄÇ
        """)

        st.markdown("""
        `ÂºÄÊ∫êÁâàÊú¨ÊöÇÊú™ÂºÄÊîæÂä®‰ΩúËØÑÂàÜÂäüËÉΩÔºåÊúâÂÖ¥Ë∂£ÂèØËÆøÈóÆÁÇπÁùõAI‰∏ì‰∏öÁâà`
        `Âí®ËØ¢ÁÉ≠Á∫øÔºö15398180360`
        """)

    # Âø´ÈÄü‰ΩìÈ™åÈ°µÈù¢
    elif st.session_state.current_tab == "Âø´ÈÄü‰ΩìÈ™å":
        st.markdown("<h1 style='text-align: center;'>Open DragonJot</h1>", unsafe_allow_html=True)
        
        # Ê®™ÂêëÊ®°ÂºèÈÄâÊã©
        mode = st.radio(
            "ÈÄâÊã©Ê£ÄÊµãÊ®°Âºè:",
            ["üì∑ ÂõæÁâáÊ®°Âºè", "üé• ËßÜÈ¢ëÊ®°Âºè", "üìπ ÊëÑÂÉèÂ§¥Ê®°Âºè"],
            horizontal=True,
            label_visibility="collapsed"
        )
        
        # ÁÆÄÂåñÊ®°ÂºèÊò†Â∞Ñ
        mode_mapping = {
            "üì∑ ÂõæÁâáÊ®°Âºè": "image",
            "üé• ËßÜÈ¢ëÊ®°Âºè": "video", 
            "üìπ ÊëÑÂÉèÂ§¥Ê®°Âºè": "camera"
        }
        current_mode = mode_mapping[mode]
        
        # ‰∏ªÂÜÖÂÆπÂå∫Âüü - ÂõæÁâáÊ®°Âºè
        if current_mode == "image":
            uploaded_file = st.file_uploader("ÈÄâÊã©ÂõæÁâáÊñá‰ª∂", type=['jpg', 'jpeg', 'png'], key="upload_image")
            
            if uploaded_file is not None:
                # ÊòæÁ§∫È¢ÑËßàÂíåÁªìÊûúÂú®Âêå‰∏ÄÂå∫Âüü
                preview_placeholder = st.empty()
                
                # ‰∏¥Êó∂‰øùÂ≠òÊñá‰ª∂Âπ∂ÊòæÁ§∫ÂéüÂõæÈ¢ÑËßà
                with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    img_path = tmp_file.name
                
                # ÊòæÁ§∫ÂéüÂõæÈ¢ÑËßà
                original_img = cv2.imread(img_path)
                preview_placeholder.image(
                    cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB), 
                    caption="ÂéüÂõæÈ¢ÑËßà",
                    width='stretch'
                )
                
                if st.button("ÂºÄÂßãÊ£ÄÊµãÂõæÁâá", width='stretch', type="primary", key="btn_process_image"):
                    with st.spinner("Ê≠£Âú®Â§ÑÁêÜÂõæÁâá..."):
                        # Ëé∑ÂèñÂèÇÊï∞ËÆæÁΩÆ
                        params = get_params()
                        
                        # ÊòæÁ§∫GPUÁä∂ÊÄÅÔºàÂ¶ÇÊûú‰ΩøÁî®GPUÔºâ
                        gpu_status_placeholder = None
                        if params['device'].type == 'cuda':
                            gpu_status_placeholder = st.empty()
                            mem_used, mem_cached = gpu_monitor.get_memory_usage()
                            gpu_status = f"""
                            **GPUÁä∂ÊÄÅ**  
                            ÊòæÂ≠ò‰ΩøÁî®: {mem_used:.2f}GB / {gpu_monitor.memory_total:.2f}GB  
                            """
                            gpu_status_placeholder.markdown(gpu_status)
                        
                        result_img, output_img_path = process_image(
                            params['person_model'], params['dragon_model'], img_path, params['confs'], params['device'],
                            save_json=params['save_json'], save_txt=params['save_txt'],
                            single_dragon=params['single_dragon'], only_person=params['only_person'], only_dragon=params['only_dragon'],
                            classify=params['classify'], classify_model=params['classify_model'], verbose=False,
                            node_colors=params['node_colors'], node_size=params['node_size'],
                            line_color=params['line_color'], line_thickness=params['line_thickness']
                        )
                        
                        if gpu_status_placeholder:
                            gpu_status_placeholder.empty()
                        
                        # Êõ¥Êñ∞ÊòæÁ§∫ÁªìÊûú
                        preview_placeholder.image(
                            cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), 
                            caption="Ê£ÄÊµãÁªìÊûú",
                            width='stretch'
                        )
                        
                        # ÊèêÁ§∫Áî®Êà∑ÂèäÊó∂‰øùÂ≠ò
                        st.success("Â§ÑÁêÜÂÆåÊàêÔºÅËØ∑ÂèäÊó∂‰øùÂ≠òËæìÂá∫ÁªìÊûúÔºåÂÖ≥Èó≠ÊµèËßàÂô®ÂêéÊñá‰ª∂Â∞ÜËá™Âä®Ê∏ÖÈô§„ÄÇ")
                        
                        # Êèê‰æõ‰∏ãËΩΩÈÄâÈ°π
                        download_cols = st.columns(4)
                        
                        if params['save_json']:
                            with download_cols[0]:
                                with open(OUTPUT_DIR / "person.json", "rb") as file:
                                    st.download_button(
                                        label="üì• ‰∫∫‰ΩìJSON",
                                        data=file,
                                        file_name="person_detection.json",
                                        mime="application/json",
                                        width='stretch',
                                        key="download_person_json"
                                    )
                            with download_cols[1]:
                                with open(OUTPUT_DIR / "dragon.json", "rb") as file:
                                    st.download_button(
                                        label="üì• ÈæôÈ™®Êû∂JSON",
                                        data=file,
                                        file_name="dragon_detection.json",
                                        mime="application/json",
                                        width='stretch',
                                        key="download_dragon_json"
                                    )
                        
                        if params['save_txt']:
                            with download_cols[2]:
                                with open(OUTPUT_DIR / "person.txt", "rb") as file:
                                    st.download_button(
                                        label="üì• ‰∫∫‰ΩìTXT",
                                        data=file,
                                        file_name="person_detection.txt",
                                        mime="text/plain",
                                        width='stretch',
                                        key="download_person_txt"
                                    )
                            with download_cols[3]:
                                with open(OUTPUT_DIR / "dragon.txt", "rb") as file:
                                    st.download_button(
                                        label="üì• ÈæôÈ™®Êû∂TXT",
                                        data=file,
                                        file_name="dragon_detection.txt",
                                        mime="text/plain",
                                        width='stretch',
                                        key="download_dragon_txt"
                                    )
                        
                        # ÂõæÁâá‰∏ãËΩΩÊåâÈíÆ
                        with open(output_img_path, "rb") as file:
                            st.download_button(
                                label="üì• ‰∏ãËΩΩÊ£ÄÊµãÁªìÊûúÂõæÁâá",
                                data=file,
                                file_name="detection_result.jpg",
                                mime="image/jpeg",
                                width='stretch',
                                key="download_result_image"
                            )

        # ‰∏ªÂÜÖÂÆπÂå∫Âüü - ËßÜÈ¢ëÊ®°Âºè
        elif current_mode == "video":
            # ÂàùÂßãÂåñËßÜÈ¢ëÊí≠ÊîæÁõ∏ÂÖ≥Áä∂ÊÄÅ
            if 'video_played' not in st.session_state:
                st.session_state.video_played = False
                
            uploaded_file = st.file_uploader("ÈÄâÊã©ËßÜÈ¢ëÊñá‰ª∂", type=['mp4', 'mov', 'avi', 'mkv'], key="upload_video")
            
            if uploaded_file is not None and not st.session_state.analysis_running:
                # ‰øùÂ≠ò‰∏ä‰º†ÁöÑËßÜÈ¢ëÂà∞‰∏¥Êó∂ÁõÆÂΩï
                with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    video_path = tmp_file.name
                
                # ÊòæÁ§∫‰∏ä‰º†ÁöÑËßÜÈ¢ë‰ø°ÊÅØ
                try:
                    cap_info = cv2.VideoCapture(video_path)
                    if cap_info.isOpened():
                        width = int(cap_info.get(cv2.CAP_PROP_FRAME_WIDTH))
                        height = int(cap_info.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        fps = cap_info.get(cv2.CAP_PROP_FPS)
                        frame_count = int(cap_info.get(cv2.CAP_PROP_FRAME_COUNT))
                        st.info(f"ËßÜÈ¢ë‰ø°ÊÅØ: {width}x{height}, {fps:.1f} FPS, {frame_count} Â∏ß")
                    cap_info.release()
                except Exception as e:
                    st.warning(f"Êó†Ê≥ïËé∑ÂèñËßÜÈ¢ë‰ø°ÊÅØ: {str(e)}")
                
                # ÊéßÂà∂ÊåâÈíÆ
                col1, col2 = st.columns(2)
                with col1:
                    start_button = st.button("ÂºÄÂßãÂ§ÑÁêÜËßÜÈ¢ë", width='stretch', type="primary", key="btn_start_video")
                with col2:
                    stop_button = st.button("ÁªàÊ≠¢Â§ÑÁêÜ", width='stretch', disabled=True, key="btn_stop_video")
                
                if start_button:
                    st.session_state.analysis_running = True
                    st.session_state.stop_flag = False
                    st.session_state.video_played = False
                    
                    # Ëé∑ÂèñÂèÇÊï∞ËÆæÁΩÆ
                    params = get_params()
                    
                    # Áä∂ÊÄÅÂíåËøõÂ∫¶ÊòæÁ§∫
                    status_text = st.empty()
                    progress_bar = st.progress(0)
                    
                    def update_status(text, progress=0):
                        status_text.text(text)
                        progress_bar.progress(progress)
                    
                    # Áõ¥Êé•Âú®‰∏ªÁ∫øÁ®ãËøêË°åËßÜÈ¢ëÂ§ÑÁêÜ
                    output_video_path = process_video(
                        params['person_model'], params['dragon_model'], video_path, params['confs'], 
                        params['realtime_filter_method'], params['smooth'], params['device'],
                        save_json=params['save_json'], save_txt=params['save_txt'],
                        show_preview=params['show_preview'], single_dragon=params['single_dragon'], 
                        only_person=params['only_person'], only_dragon=params['only_dragon'],
                        classify=params['classify'], classify_model=params['classify_model'],
                        save_video=params['save_video'], verbose=False,
                        node_colors=params['node_colors'], node_size=params['node_size'],
                        line_color=params['line_color'], line_thickness=params['line_thickness'],
                        gpu_monitor=gpu_monitor,
                        status_callback=update_status
                    )
                    
                    st.session_state.analysis_running = False
                    st.session_state.output_video_path = output_video_path
                    st.session_state.video_played = True
                    update_status("ËßÜÈ¢ëÂ§ÑÁêÜÂÆåÊàêÔºÅ", 1.0)
                    
                    # ÊòæÁ§∫Â§ÑÁêÜÁªìÊûú
                    if output_video_path and output_video_path.exists():
                        st.subheader("Â§ÑÁêÜÁªìÊûúÈ¢ÑËßà")
                        # ËØªÂèñËßÜÈ¢ëÊñá‰ª∂ÂÜÖÂÆπ
                        try:
                            with open(output_video_path, "rb") as file:
                                video_bytes = file.read()
                                # Ê£ÄÊü•Êñá‰ª∂Â§ßÂ∞è
                                if len(video_bytes) < 1024:  # Â∞è‰∫é1KBÁöÑËßÜÈ¢ëÊñá‰ª∂ÂèØËÉΩ‰∏∫Á©∫
                                    st.error("ÁîüÊàêÁöÑËßÜÈ¢ëÊñá‰ª∂ËøáÂ∞èÔºåÂèØËÉΩ‰∏∫Á©∫ÊàñÊçüÂùè")
                                else:
                                    # ‰ΩøÁî®StreamlitÁöÑËßÜÈ¢ëÊí≠ÊîæÂô®
                                    st.video(video_bytes, format="video/mp4")
                        except Exception as e:
                            st.error(f"Êó†Ê≥ïËØªÂèñËßÜÈ¢ëÊñá‰ª∂: {str(e)}")
                        
                        # È™åËØÅËßÜÈ¢ëÊñá‰ª∂
                        cap = cv2.VideoCapture(str(output_video_path))
                        if not cap.isOpened():
                            st.error("ÁîüÊàêÁöÑËßÜÈ¢ëÊñá‰ª∂Êó†Ê≥ïÊâìÂºÄÔºåÂèØËÉΩÂ∑≤ÊçüÂùè")
                        else:
                            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                            fps = cap.get(cv2.CAP_PROP_FPS)
                            st.info(f"ËßÜÈ¢ë‰ø°ÊÅØ: {frame_count} Â∏ß, {fps:.1f} FPS")
                            cap.release()
                        
                        # ÊèêÁ§∫Áî®Êà∑ÂèäÊó∂‰øùÂ≠ò
                        st.success("Â§ÑÁêÜÂÆåÊàêÔºÅËØ∑ÂèäÊó∂‰øùÂ≠òËæìÂá∫ÁªìÊûúÔºåÂÖ≥Èó≠ÊµèËßàÂô®ÂêéÊñá‰ª∂Â∞ÜËá™Âä®Ê∏ÖÈô§„ÄÇ")
                        
                        # Êèê‰æõ‰∏ãËΩΩÈÄâÈ°π
                        download_cols = st.columns(4)
                        
                        if params['save_json']:
                            with download_cols[0]:
                                with open(OUTPUT_DIR / "person.json", "rb") as file:
                                    st.download_button(
                                        label="üì• ‰∫∫‰ΩìJSON",
                                        data=file,
                                        file_name="person_detection.json",
                                        mime="application/json",
                                        width='stretch',
                                        key="download_video_person_json"
                                    )
                            with download_cols[1]:
                                with open(OUTPUT_DIR / "dragon.json", "rb") as file:
                                    st.download_button(
                                        label="üì• ÈæôÈ™®Êû∂JSON",
                                        data=file,
                                        file_name="dragon_detection.json",
                                        mime="application/json",
                                        width='stretch',
                                        key="download_video_dragon_json"
                                    )
                        
                        if params['save_txt']:
                            with download_cols[2]:
                                with open(OUTPUT_DIR / "person.txt", "rb") as file:
                                    st.download_button(
                                        label="üì• ‰∫∫‰ΩìTXT",
                                        data=file,
                                        file_name="person_detection.txt",
                                        mime="text/plain",
                                        width='stretch',
                                        key="download_video_person_txt"
                                    )
                            with download_cols[3]:
                                with open(OUTPUT_DIR / "dragon.txt", "rb") as file:
                                    st.download_button(
                                        label="üì• ÈæôÈ™®Êû∂TXT",
                                        data=file,
                                        file_name="dragon_detection.txt",
                                        mime="text/plain",
                                        width='stretch',
                                        key="download_video_dragon_txt"
                                    )
                        
                        # ËßÜÈ¢ë‰∏ãËΩΩÊåâÈíÆ
                        with open(output_video_path, "rb") as file:
                            st.download_button(
                                label="üì• ‰∏ãËΩΩÂ§ÑÁêÜÂêéÁöÑËßÜÈ¢ë",
                                data=file,
                                file_name="processed_video.mp4",
                                mime="video/mp4",
                                width='stretch',
                                key="download_processed_video"
                            )
                    elif output_video_path:
                        st.error(f"ËßÜÈ¢ëÂ§ÑÁêÜÂ§±Ë¥•ÔºåÊñá‰ª∂Êú™ÁîüÊàê: {output_video_path}")
                    else:
                        st.error("ËßÜÈ¢ëÂ§ÑÁêÜÂ§±Ë¥•ÔºåÊú™ÁîüÊàêËæìÂá∫Êñá‰ª∂")

        # ‰∏ªÂÜÖÂÆπÂå∫Âüü - ÊëÑÂÉèÂ§¥Ê®°Âºè
        elif current_mode == "camera":
            # Ëé∑ÂèñÊ£ÄÊµãÂèÇÊï∞
            params = get_params()

            # Ë∞ÉÁî®Êñ∞ÁöÑ WebRTC ÊëÑÂÉèÂ§¥Â§ÑÁêÜÈÄªËæë
            process_camera_stream(params, gpu_monitor)

    # ÂèÇÊï∞ËÆæÁΩÆÁã¨Á´ãÈ°µÈù¢
    elif st.session_state.current_tab == "ÂèÇÊï∞ËÆæÁΩÆ":
        st.markdown("<h1 style='text-align: center;'>ÂèÇÊï∞ËÆæÁΩÆ</h1>", unsafe_allow_html=True)
        st.write("Âú®ËøôÈáåÈÖçÁΩÆÁ≥ªÁªüÁöÑÂêÑÈ°πÂèÇÊï∞ÔºåËÆæÁΩÆÂ∞ÜÂ∫îÁî®‰∫éÊâÄÊúâÊ£ÄÊµãÊ®°Âºè„ÄÇ")
        
        # ‰ΩøÁî®session_stateÂ≠òÂÇ®ÂèÇÊï∞ÔºåÁ°Æ‰øùÂú®È°µÈù¢Èó¥‰øùÊåÅ‰∏ÄËá¥
        if 'params_initialized' not in st.session_state:
            # ÂàùÂßãÂåñÂèÇÊï∞
            st.session_state.use_gpu = gpu_monitor.use_gpu
            st.session_state.single_dragon = True
            st.session_state.only_person = False
            st.session_state.only_dragon = False
            st.session_state.classify = True
            st.session_state.show_preview = True
            st.session_state.save_video = True
            st.session_state.save_json = False
            st.session_state.save_txt = False
            st.session_state.person_model = "yolov8m-pose.pt"
            st.session_state.dragon_model = "YoGon-Pose-v2.pt"
            st.session_state.classify_model = "YoGon-Clas-v2.pth"
            st.session_state.node_size = 10
            st.session_state.line_thickness = 6
            st.session_state.person_conf = 0.3
            st.session_state.dragon_conf = 0.3
            st.session_state.person_kpt_conf = 0.5
            st.session_state.dragon_kpt_conf = 0.5
            st.session_state.realtime_filter_method = "none"
            st.session_state.smooth = "none"
            
            # ÂàùÂßãÂåñËäÇÁÇπÈ¢úËâ≤
            hex_to_rgb = lambda h: tuple(int(h.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))
            for i in range(9):
                default_color = '#%02x%02x%02x' % (
                    DEFAULT_NODE_COLORS[i][2], 
                    DEFAULT_NODE_COLORS[i][1], 
                    DEFAULT_NODE_COLORS[i][0]
                )
                st.session_state[f"node_color_{i}"] = default_color
            
            st.session_state.line_color = "#C8C8C8"
            st.session_state.params_initialized = True
        
        # ËÆæÂ§áÈÖçÁΩÆ
        with st.expander("üíª ËÆæÂ§áÈÖçÁΩÆ", expanded=True):
            st.session_state.use_gpu = st.checkbox(
                "‰ΩøÁî®GPUÂä†ÈÄü", 
                value=st.session_state.use_gpu, 
                disabled=not gpu_monitor.use_gpu,
                key="cb_use_gpu"
            )
            device = torch.device('cuda' if (st.session_state.use_gpu and gpu_monitor.use_gpu) else 'cpu')
            
            # ÊòæÁ§∫ËÆæÂ§á‰ø°ÊÅØ
            if device.type == 'cuda':
                st.success(f"‰ΩøÁî®GPU: {gpu_monitor.gpu_name}")
                st.info(f"ÊÄªÊòæÂ≠ò: {gpu_monitor.memory_total:.2f} GB")
            else:
                st.warning("‰ΩøÁî®CPUËøõË°åËÆ°ÁÆóÔºåÂèØËÉΩËæÉÊÖ¢")
        
        # Ê£ÄÊµãÈÄâÈ°π
        with st.expander("üîç Ê£ÄÊµãÈÄâÈ°π", expanded=True):
            def on_detection_param_change():
                if st.session_state.analysis_running:
                    st.warning("Ê£ÄÊµãÂèÇÊï∞Â∑≤‰øÆÊîπÔºåÂΩìÂâçÂàÜÊûêÂ∞ÜÁªàÊ≠¢Âπ∂ÈúÄË¶ÅÈáçÊñ∞ÂºÄÂßã")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            st.session_state.single_dragon = st.checkbox(
                "‰ªÖÊ£ÄÊµãÂçïÊù°Èæô", 
                value=st.session_state.single_dragon, 
                on_change=on_detection_param_change,
                key="cb_single_dragon"
            )
            st.session_state.only_person = st.checkbox(
                "‰ªÖÊ£ÄÊµã‰∫∫‰Ωì", 
                value=st.session_state.only_person, 
                on_change=on_detection_param_change,
                key="cb_only_person"
            )
            st.session_state.only_dragon = st.checkbox(
                "‰ªÖÊ£ÄÊµãÈæôÈ™®Êû∂", 
                value=st.session_state.only_dragon, 
                on_change=on_detection_param_change,
                key="cb_only_dragon"
            )
            st.session_state.classify = st.checkbox(
                "ÂêØÁî®Âä®‰ΩúÂàÜÁ±ª", 
                value=st.session_state.classify, 
                on_change=on_detection_param_change,
                key="cb_classify"
            )

        # ËæìÂá∫ÈÄâÈ°π
        with st.expander("üíæ ËæìÂá∫ÈÄâÈ°π", expanded=True):
            def on_output_param_change():
                if st.session_state.analysis_running:
                    st.warning("ËæìÂá∫ÂèÇÊï∞Â∑≤‰øÆÊîπÔºåÂΩìÂâçÂàÜÊûêÂ∞ÜÁªàÊ≠¢Âπ∂ÈúÄË¶ÅÈáçÊñ∞ÂºÄÂßã")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            st.session_state.show_preview = st.checkbox(
                "ÂÆûÊó∂È¢ÑËßà", 
                value=st.session_state.show_preview, 
                on_change=on_output_param_change,
                key="cb_show_preview"
            )
            st.session_state.save_video = st.checkbox(
                "‰øùÂ≠òËßÜÈ¢ë", 
                value=st.session_state.save_video, 
                on_change=on_output_param_change,
                key="cb_save_video"
            )
            st.session_state.save_json = st.checkbox(
                "‰øùÂ≠òJSONÊñá‰ª∂", 
                value=st.session_state.save_json, 
                on_change=on_output_param_change,
                key="cb_save_json"
            )
            st.session_state.save_txt = st.checkbox(
                "‰øùÂ≠òTXTÊñá‰ª∂", 
                value=st.session_state.save_txt, 
                on_change=on_output_param_change,
                key="cb_save_txt"
            )
        
        # Ê®°ÂûãÈÖçÁΩÆ
        with st.expander("ü§ñ Ê®°ÂûãÈÖçÁΩÆ", expanded=True):
            def on_model_param_change():
                if st.session_state.analysis_running:
                    st.warning("Ê®°ÂûãÂèÇÊï∞Â∑≤‰øÆÊîπÔºåÂΩìÂâçÂàÜÊûêÂ∞ÜÁªàÊ≠¢Âπ∂ÈúÄË¶ÅÈáçÊñ∞ÂºÄÂßã")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            st.session_state.person_model = st.text_input(
                "‰∫∫‰ΩìÂßøÊÄÅÊ®°ÂûãË∑ØÂæÑ", 
                value=st.session_state.person_model, 
                on_change=on_model_param_change,
                key="txt_person_model"
            )
            st.session_state.dragon_model = st.text_input(
                "ÈæôÈ™®Êû∂Ê®°ÂûãË∑ØÂæÑ", 
                value=st.session_state.dragon_model, 
                on_change=on_model_param_change,
                key="txt_dragon_model"
            )
            st.session_state.classify_model = st.text_input(
                "Âä®‰ΩúÂàÜÁ±ªÊ®°ÂûãË∑ØÂæÑ", 
                value=st.session_state.classify_model, 
                on_change=on_model_param_change,
                key="txt_classify_model"
            )


        # Ê†∑ÂºèËá™ÂÆö‰πâÈÖçÁΩÆ
        with st.expander("üé® Ê†∑ÂºèËá™ÂÆö‰πâ", expanded=False):
            def on_style_param_change():
                if st.session_state.analysis_running:
                    st.warning("Ê†∑ÂºèÂèÇÊï∞Â∑≤‰øÆÊîπÔºåÂΩìÂâçÂàÜÊûêÂ∞ÜÁªàÊ≠¢Âπ∂ÈúÄË¶ÅÈáçÊñ∞ÂºÄÂßã")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            # ËäÇÁÇπÊ†∑Âºè
            st.subheader("ËäÇÁÇπÊ†∑Âºè")
            st.session_state.node_size = st.slider(
                "ËäÇÁÇπÂ§ßÂ∞è", 
                1, 20, st.session_state.node_size, 
                on_change=on_style_param_change,
                key="slider_node_size"
            )
            
            # ÊØè‰∏™ËäÇÁÇπÂçïÁã¨ÁöÑÈ¢úËâ≤ÈÄâÊã©
            hex_to_rgb = lambda h: tuple(int(h.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))
            for i in range(9):  # 1-9ËäÇÁÇπ
                st.session_state[f"node_color_{i}"] = st.color_picker(
                    f"ËäÇÁÇπ {i+1} È¢úËâ≤", 
                    st.session_state[f"node_color_{i}"], 
                    key=f"cp_node_{i}",
                    on_change=on_style_param_change
                )
            
            # ËøûÁ∫øÊ†∑Âºè
            st.subheader("ËøûÁ∫øÊ†∑Âºè")
            st.session_state.line_thickness = st.slider(
                "ËøûÁ∫øÁ≤óÁªÜ", 
                1, 10, st.session_state.line_thickness, 
                on_change=on_style_param_change,
                key="slider_line_thickness"
            )
            st.session_state.line_color = st.color_picker(
                "ËøûÁ∫øÈ¢úËâ≤", 
                st.session_state.line_color, 
                on_change=on_style_param_change,
                key="cp_line_color"
            )

        # ÁΩÆ‰ø°Â∫¶ÈÖçÁΩÆ
        with st.expander("üéØ ÁΩÆ‰ø°Â∫¶ÈòàÂÄº", expanded=False):
            def on_conf_param_change():
                if st.session_state.analysis_running:
                    st.warning("ÁΩÆ‰ø°Â∫¶ÂèÇÊï∞Â∑≤‰øÆÊîπÔºåÂΩìÂâçÂàÜÊûêÂ∞ÜÁªàÊ≠¢Âπ∂ÈúÄË¶ÅÈáçÊñ∞ÂºÄÂßã")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            st.session_state.person_conf = st.slider(
                "‰∫∫‰ΩìÊ£ÄÊµãÁΩÆ‰ø°Â∫¶", 
                0.1, 1.0, st.session_state.person_conf, 0.05, 
                on_change=on_conf_param_change,
                key="slider_person_conf"
            )
            st.session_state.dragon_conf = st.slider(
                "ÈæôÈ™®Êû∂Ê£ÄÊµãÁΩÆ‰ø°Â∫¶", 
                0.1, 1.0, st.session_state.dragon_conf, 0.05, 
                on_change=on_conf_param_change,
                key="slider_dragon_conf"
            )
            st.session_state.person_kpt_conf = st.slider(
                "‰∫∫‰ΩìÂÖ≥ÈîÆÁÇπÁΩÆ‰ø°Â∫¶", 
                0.1, 1.0, st.session_state.person_kpt_conf, 0.05, 
                on_change=on_conf_param_change,
                key="slider_person_kpt_conf"
            )
            st.session_state.dragon_kpt_conf = st.slider(
                "ÈæôÈ™®Êû∂ÂÖ≥ÈîÆÁÇπÁΩÆ‰ø°Â∫¶", 
                0.1, 1.0, st.session_state.dragon_kpt_conf, 0.05, 
                on_change=on_conf_param_change,
                key="slider_dragon_kpt_conf"
            )

        # Êª§Ê≥¢ÈÖçÁΩÆ
        with st.expander("üìä Êª§Ê≥¢ËÆæÁΩÆ", expanded=False):
            def on_filter_param_change():
                if st.session_state.analysis_running:
                    st.warning("Êª§Ê≥¢ÂèÇÊï∞Â∑≤‰øÆÊîπÔºåÂΩìÂâçÂàÜÊûêÂ∞ÜÁªàÊ≠¢Âπ∂ÈúÄË¶ÅÈáçÊñ∞ÂºÄÂßã")
                    st.session_state.analysis_running = False
                    st.session_state.stop_flag = True
            
            st.session_state.realtime_filter_method = st.selectbox(
                "ÂÆûÊó∂Êª§Ê≥¢ÊñπÊ≥ï",
                ["none", "ema", "kalman"],
                index=["none", "ema", "kalman"].index(st.session_state.realtime_filter_method),
                on_change=on_filter_param_change,
                key="select_realtime_filter"
            )
            st.session_state.smooth = st.selectbox(
                "ËßÜÈ¢ëÂπ≥ÊªëÊñπÊ°à",
                ["none", "mean", "weighted_mean", "ewm"],
                index=["none", "mean", "weighted_mean", "ewm"].index(st.session_state.smooth),
                on_change=on_filter_param_change,
                key="select_smooth"
            )
        
        if st.button("‰øùÂ≠òÂèÇÊï∞ËÆæÁΩÆ", width='stretch', type="primary", key="btn_save_params"):
            st.success("ÂèÇÊï∞ËÆæÁΩÆÂ∑≤‰øùÂ≠òÔºÅ")

    # ÂºÄÂèëÂõ¢ÈòüÈ°µÈù¢
    elif st.session_state.current_tab == "ÂºÄÂèëÂõ¢Èòü":
        st.markdown("<h1 style='text-align: center;'>ÂºÄÂèëÂõ¢Èòü</h1>", unsafe_allow_html=True)
        
        # Âõ¢Èòü‰ªãÁªç
        st.subheader("È°πÁõÆÁÆÄ‰ªã")
        st.write("""
        ÁÇπÁùõAIÔºàDragonJotÔºâÊòØÂõΩÂÜÖÈ¶ñ‰∏™‰∏ìÊ≥®‰∫éËàûÈæôËøêÂä®ÁöÑAIÊô∫ËÉΩËØÑÂà§‰∏éËÆ≠ÁªÉËæÖÂä©Á≥ªÁªü„ÄÇÈíàÂØπ‰º†ÁªüËàûÈæôËµõ‰∫ãËØÑÂà§‰∏ªËßÇÊÄßÂº∫„ÄÅËÆ≠ÁªÉÂèçÈ¶àÁº∫‰πèÈáèÂåñ‰æùÊçÆ„ÄÅÊïôÂ≠¶ËµÑÊ∫ê‰∏çË∂≥Á≠âË°å‰∏öÁóõÁÇπÔºåÊàë‰ª¨ÂàõÊñ∞ÊèêÂá∫"‰∫∫Èæô‰∏Ä‰Ωì"ÂçèÂêåÂä®‰ΩúËØÜÂà´Ê®°Âûã„ÄÇËØ•Á≥ªÁªüÂü∫‰∫éYOLOÂßøÊÄÅËØÜÂà´Ê®°Âûã‰∏éCNNÂä®‰ΩúÂàÜÁ±ªÁÆóÊ≥ïÔºåÁªìÂêàËá™‰∏ªÊûÑÂª∫ÁöÑËàûÈæôÈ™®È™ºÊï∞ÊçÆÈõÜÔºåÂÆûÁé∞‰∫ÜÂØπ‰∫∫‰ΩìÂßøÊÄÅ‰∏éÈæôÂΩ¢ËΩ®ËøπÁöÑÂêåÊ≠•ËØÜÂà´‰∏éÂàÜÊûê„ÄÇ

        Á≥ªÁªüÊîØÊåÅÂ§öË∑ØËßÜÈ¢ëËæìÂÖ•ÔºàÂåÖÊã¨ÊâãÊú∫Áõ¥Êí≠‰∏éÊú¨Âú∞ËßÜÈ¢ëÔºâÔºåÂÖ∑Â§áÂÆûÊó∂Âä®‰ΩúÊçïÊçâ„ÄÅÂä®‰ΩúÂàÜÁ±ª„ÄÅÂÆûÊó∂ÂàÜÊûê‰∏éÈáèÂåñËØÑ‰º∞Á≠âÊ†∏ÂøÉÂäüËÉΩÔºåÂÖ®Èù¢Ë¶ÜÁõñËµõ‰∫ãËØÑÂà§„ÄÅ‰∏ì‰∏öËÆ≠ÁªÉ‰∏éÂ§ß‰ºóÊïôÂ≠¶‰∏âÂ§ßÂ∫îÁî®Âú∫ÊôØ„ÄÇ

        Open DragonJot‰Ωú‰∏∫ÁÇπÁùõAIÁöÑÂºÄÊ∫ê‰∫ßÂìÅÔºåÊèê‰æõÊ†∏ÂøÉÂäüËÉΩÁöÑ‰ΩìÈ™åÁâàÔºåÂåÖÂê´‰∫∫‰ΩìÂßøÊÄÅËØÜÂà´„ÄÅÈæôËøêÂä®ËΩ®ËøπËØÜÂà´ÂíåËàûÈæôÂä®‰ΩúÂàÜÁ±ªÂäüËÉΩ„ÄÇÂºÄÊ∫ê‰ª£Á†ÅÂ∑≤ÂèëÂ∏É‰∫éGitHubÔºå‰∫ßÂìÅÈ¢ÑËßàÁΩëÈ°µÈÉ®ÁΩ≤Âú®StreamlitÂπ≥Âè∞ÔºåÊ¨¢Ëøé‰ΩìÈ™å‰∏é‰∫§ÊµÅ„ÄÇ
        """)
        
        # Âõ¢ÈòüÊàêÂëò
        st.subheader("Ê†∏ÂøÉÂºÄÂèëÂõ¢Èòü")
        
        # ÊàêÂëò1
        col1, col2 = st.columns([1, 9])  # Ë∞ÉÊï¥ÊØî‰æã‰ΩøÂ§¥ÂÉèÂå∫ÂüüÊõ¥Â∞è
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "HC.jpg"), caption="È°πÁõÆË¥üË¥£‰∫∫", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="È°πÁõÆË¥üË¥£‰∫∫", width='stretch')
        with col2:
            st.markdown("""
            ### ÈúçÁïÖ - È°πÁõÆË¥üË¥£‰∫∫
            Ë¥üË¥£Âõ¢ÈòüÁªüÁ≠π‰∏éÊ†∏ÂøÉËßÑÂàíÔºåÊèêÂá∫È°πÁõÆÊ†∏ÂøÉÂàõÊÑèÔºå‰∏ªÂØºÂ∫îÁî®ÂºÄÂèë‰∏éËêΩÂú∞„ÄÇ
            """)
        
        # ÊàêÂëò2
        col1, col2 = st.columns([1, 9])
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "LZC.jpg"), caption="ÁÆóÊ≥ïÂ∑•Á®ãÂ∏à", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="ÁÆóÊ≥ïÂ∑•Á®ãÂ∏à", width='stretch')
        with col2:
            st.markdown("""
            ### Âç¢Â≠êËØö - ÁÆóÊ≥ïÂ∑•Á®ãÂ∏à
            Ë¥üË¥£ÊäÄÊúØË∞ÉÁ†î„ÄÅAIÊ®°ÂûãËÆ≠ÁªÉ‰∏é‰ºòÂåñÔºå‰ª•ÂèäÊ†∏ÂøÉÁÆóÊ≥ïÁöÑÂºÄÂèëËêΩÂú∞„ÄÇ
            """)
        
        # ÊàêÂëò3
        col1, col2 = st.columns([1, 9])
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "FZY.jpg"), caption="UI/UXËÆæËÆ°Â∏à", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="UI/UXËÆæËÆ°Â∏à", width='stretch')
        with col2:
            st.markdown("""
            ### ÊñπÂ≠êÂ´£ - UI/UXËÆæËÆ°Â∏à
            Ë¥üË¥£‰∫ßÂìÅÂàõÊÑèÁ≠ñÂàí„ÄÅÁî®Êà∑ÁïåÈù¢‰∏é‰ΩìÈ™åËÆæËÆ°ÔºåÂ°ëÈÄ†‰∫ßÂìÅÁöÑÊï¥‰ΩìËßÜËßâÈ£éÊ†º„ÄÇ
            """)
        
        # ÊàêÂëò4
        col1, col2 = st.columns([1, 9])
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "DWY.jpg"), caption="ÊñáÊ°àÁ≠ñÂàí", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="ÊñáÊ°àÁ≠ñÂàí", width='stretch')
        with col2:
            st.markdown("""
            ### ‰∏ÅÈóªÁé• - ÂàõÊÑèÂÜÖÂÆπÊÄªÁõë
            Ë¥üË¥£Â∏ÇÂú∫Ë∞ÉÁ†îÔºåÂπ∂ÊÄªË¥üË¥£‰∫ßÂìÅÊä•Âëä„ÄÅËêΩÂú∞Êé®ÂπøÊñπÊ°àÁ≠âÂÖ≥ÈîÆÊñáÁ®øÁöÑÊí∞ÂÜô„ÄÇ
            """)

        # ÊàêÂëò5
        col1, col2 = st.columns([1, 9])
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "ZMH.jpg"), caption="Êï∞ÊçÆÂ∑•Á®ãÂ∏à", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="Êï∞ÊçÆÂ∑•Á®ãÂ∏à", width='stretch')
        with col2:
            st.markdown("""
            ### Âº†ÊòéÊ∂µ - Êï∞ÊçÆÂ∑•Á®ãÂ∏à
            ‰∏ªÂØºËàûÈæôÊï∞ÊçÆÈõÜÁöÑËßÑÂàí„ÄÅÈááÈõÜ‰∏éÊûÑÂª∫Â∑•‰ΩúÔºå‰∏∫Ê®°ÂûãËÆ≠ÁªÉÊèê‰æõÊ†∏ÂøÉÊï∞ÊçÆÂü∫Á°Ä„ÄÇ
            """)

        # ÊàêÂëò6
        col1, col2 = st.columns([1, 9])
        with col1:
            try:
                st.image(str(STATIC_DIR / "avatars" / "ZKX.jpg"), caption="‰∫ßÂìÅÂÆ£‰º†", width='stretch')
            except:
                st.image("https://via.placeholder.com/100", caption="‰∫ßÂìÅÂÆ£‰º†", uwidth='stretch')
        with col2:
            st.markdown("""
            ### ËµµÂ∫∑Ë•ø ‚Äì ‰∫ßÂìÅÂèØËßÜÂåñ‰∏ìÂëò
            Ë¥üË¥£‰∫ßÂìÅÊºîÁ§∫ËßÜÈ¢ëÁöÑÂâ™ËæëÂà∂‰Ωú„ÄÅÊï∞ÊçÆÊî∂ÈõÜÂíåÈÉ®ÂàÜÂâçÁ´ØÈ°µÈù¢ÂºÄÂèë„ÄÇ
            """)
        
        # ËÅîÁ≥ªÊñπÂºè
        st.subheader("ËÅîÁ≥ªÊñπÂºè")
        st.write("""
        Â¶ÇÊûúÊÇ®ÂØπ"ÁÇπÁùõAI"È°πÁõÆÊÑüÂÖ¥Ë∂£ÔºåÊàñÊúâ‰ªª‰ΩïÁñëÈóÆ„ÄÅÂª∫ËÆÆ‰∏éÂêà‰ΩúÊÑèÂêëÔºåÊ¨¢ËøéÈÄöËøá‰ª•‰∏ãÊñπÂºè‰∏éÊàë‰ª¨ÂèñÂæóËÅîÁ≥ªÔºö
        
        ÈÇÆÁÆ±Ôºö22307110080@m.fudan.edu.cn
        
        È°πÁõÆÂú∞ÂùÄÔºöxxxx
        
        ÊúüÂæÖÊÇ®ÁöÑÂèçÈ¶à‰∏é‰∫§ÊµÅÔºÅ
        """)
        st.write("   ")
        st.write("   ")
        st.write("   ")
        st.write("   ")
        st.markdown("<div style='height: 100px;'></div>", unsafe_allow_html=True)
        

        # Ëá¥Ë∞¢ÔºàÂ±Ö‰∏≠ÊòæÁ§∫Âú®Êú´Â∞æÔºâ
        st.markdown("""
    <div style='text-align: center; margin: 20px 0;'>
        <h3>Ëá¥ Ë∞¢</h3>
        <p style='margin: 15px 0; line-height: 1.8;'>
            ‰∏ÄË∑ØËµ∞Êù•Ôºå<br>"ÁÇπÁùõAI"ÁöÑÊØè‰∏ÄÊ≠•ÊàêÈïøÈÉΩÁ¶ª‰∏çÂºÄ‰ºóÂ§öÊîØÊåÅËÄÖÁöÑÈºéÂäõÁõ∏Âä©„ÄÇ<br>Âú®Ê≠§ÔºåÊàë‰ª¨Ë∞®ÂêëÊâÄÊúâ‰∏∫Êú¨È°πÁõÆ<br>
            ÂÄæÊ≥®ÂøÉË°Ä„ÄÅÁªô‰∫àÊåáÂØºÁöÑÊú∫ÊûÑ‰∏é‰∏™‰∫∫Ôºå<br>Ëá¥‰ª•ÊúÄËØöÊåöÁöÑÊï¨ÊÑè‰∏éÊúÄË°∑ÂøÉÁöÑÊÑüË∞¢ÔºÅ
        </p>
        <p style='margin: 15px 0; line-height: 1.8;'>
            ÁâπÂà´È∏£Ë∞¢ <strong style='font-size: 1.1em;'>Â§çÊó¶Â§ßÂ≠¶‰ΩìËÇ≤ÂèëÂ±ïÈÉ®ÁöÑÂæêÁáïÂã§ËÄÅÂ∏à</strong>Ôºå‰ª•ÂèäÊâÄÊúâ<br>
            <strong style='font-size: 1.1em;'>Á¨¨‰∏ÄÂ±ä"Â§çÊó¶AI+‰ΩìËÇ≤ÂàõÂèòËê•"</strong> ÁöÑËÄÅÂ∏à‰∏éÂêåÂ≠¶‰ª¨‚Äî‚Äî<br>
            ÊòØ‰Ω†‰ª¨Êê≠Âª∫ÁöÑÂàõÊñ∞Âπ≥Âè∞ÔºåËÆ©Ëøô‰∏™Ê∫ê‰∫éÂØπ‰º†ÁªüÊñáÂåñÁÉ≠Áà±ÁöÑÈ°πÁõÆÁßçÂ≠êÂæó‰ª•ËêåÂèë„ÄÅÊàêÈïø„ÄÇ
        </p>
        <p style='margin: 15px 0; line-height: 1.8;'>
            ÁâπÂà´ÊÑüË∞¢ <strong style='font-size: 1.1em;'>‰∏äÊµ∑ÈæôÁãÆÂçè‰ºöÊú±‰øäÈπèÊïôÁªÉ</strong> ÁöÑ‰∏ì‰∏öÂºïÈ¢Ü‚Äî‚Äî<br>
            ÊÇ®‰∏ç‰ªÖÁ≤æÂáÜÁÇπÂá∫Ë°å‰∏öÁóõÁÇπÔºåÊõ¥Êó†ÁßÅÂàÜ‰∫´ÂÆùË¥µÁöÑË°å‰∏öËµÑÊ∫êÔºå‰∏∫Êàë‰ª¨ÁöÑÊäÄÊúØËêΩÂú∞ÊåáÊòé‰∫ÜÊñπÂêë„ÄÇ
        </p>
        <p style='margin: 15px 0; line-height: 1.8;'>
            Ê∑±Ê∑±ÊÑüÊøÄ <strong style='font-size: 1.1em;'>Â§çÊó¶ÈæôÁãÆÂçè‰ºö</strong> ÁöÑÂêåÂ≠¶‰ª¨‚Äî‚Äî<br>
            ‰Ω†‰ª¨Áî®‰∏ì‰∏öÁöÑË°®Êºî‰∏éÁÉ≠ÊÉÖÁöÑÊäïÂÖ•Ôºå‰∏∫È°πÁõÆÊèê‰æõ‰∫ÜÈ´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜ‰∏éÊºîÁ§∫Á¥†ÊùêÔºå<br>
            Êàê‰∏∫"ÁÇπÁùõAI"ËÉΩÂ§üÊ†©Ê†©Â¶ÇÁîüÁöÑÊ†∏ÂøÉÂü∫Áü≥„ÄÇ
        </p>
        <p style='margin: 15px 0; line-height: 1.8;'>
            Âõ†‰∏∫Êúâ‰Ω†‰ª¨Ôºå‰º†ÁªüÊñáÂåñ‰∏éÁé∞‰ª£ÁßëÊäÄÁöÑÁ¢∞ÊíûÊâçÂ¶ÇÊ≠§Á≤æÂΩ©„ÄÇ<br>
            Ëøô‰ªΩÊîØÊåÅÔºåÂ∞ÜÊøÄÂä±Êàë‰ª¨ÁªßÁª≠Áî®ÊäÄÊúØËµãËÉΩ‰º†ÁªüÔºåËÆ©ËàûÈæôÊñáÂåñÁÑïÂèëÊñ∞ÁöÑÁîüÊú∫ÔºÅ
        </p>
    </div>
    """, unsafe_allow_html=True)

# ÂèÇÊï∞Ëé∑ÂèñÂáΩÊï∞Ôºå‰æõÂêÑÊ®°Âùó‰ΩøÁî®Áªü‰∏ÄÁöÑÂèÇÊï∞ËÆæÁΩÆ
def get_params():
    # Á°Æ‰øùÂèÇÊï∞Â∑≤ÂàùÂßãÂåñ
    if 'params_initialized' not in st.session_state:
        # ÈÅøÂÖçÈÄíÂΩíË∞ÉÁî®main()ÔºåÁõ¥Êé•ÂàùÂßãÂåñÂèÇÊï∞
        st.session_state.use_gpu = gpu_monitor.use_gpu
        st.session_state.single_dragon = True
        st.session_state.only_person = False
        st.session_state.only_dragon = False
        st.session_state.classify = True
        st.session_state.show_preview = True
        st.session_state.save_video = True
        st.session_state.save_json = False
        st.session_state.save_txt = False
        st.session_state.person_model = "yolov8m-pose.pt"
        st.session_state.dragon_model = "YoGon-Pose-v2.pt"
        st.session_state.classify_model = "YoGon-Clas-v2.pth"
        st.session_state.node_size = 10
        st.session_state.line_thickness = 6
        st.session_state.person_conf = 0.3
        st.session_state.dragon_conf = 0.3
        st.session_state.person_kpt_conf = 0.5
        st.session_state.dragon_kpt_conf = 0.5
        st.session_state.realtime_filter_method = "none"
        st.session_state.smooth = "none"
        
        # ÂàùÂßãÂåñËäÇÁÇπÈ¢úËâ≤
        hex_to_rgb = lambda h: tuple(int(h.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))
        for i in range(9):
            default_color = '#%02x%02x%02x' % (
                DEFAULT_NODE_COLORS[i][2], 
                DEFAULT_NODE_COLORS[i][1], 
                DEFAULT_NODE_COLORS[i][0]
            )
            st.session_state[f"node_color_{i}"] = default_color
        
        st.session_state.line_color = "#C8C8C8"
        st.session_state.params_initialized = True
    
    # ËÆæÂ§áÈÖçÁΩÆ
    device = torch.device(
        'cuda' if (st.session_state.use_gpu and gpu_monitor.use_gpu) else 'cpu'
    )
    
    # ÁΩÆ‰ø°Â∫¶ÂèÇÊï∞
    confs = [
        st.session_state.person_conf, 
        st.session_state.dragon_conf, 
        st.session_state.person_kpt_conf, 
        st.session_state.dragon_kpt_conf
    ]
    
    # ËäÇÁÇπÈ¢úËâ≤Â§ÑÁêÜ
    hex_to_rgb = lambda h: tuple(int(h.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))
    node_colors = []
    for i in range(9):
        r, g, b = hex_to_rgb(st.session_state[f"node_color_{i}"])
        node_colors.append((b, g, r))  # ËΩ¨Êç¢‰∏∫BGRÊ†ºÂºè
    
    # ËøûÁ∫øÈ¢úËâ≤Â§ÑÁêÜ
    line_r, line_g, line_b = hex_to_rgb(st.session_state.line_color)
    line_color = (line_b, line_g, line_r)  # ËΩ¨Êç¢‰∏∫BGRÊ†ºÂºè
    
    return {
        'device': device,
        'single_dragon': st.session_state.single_dragon,
        'only_person': st.session_state.only_person,
        'only_dragon': st.session_state.only_dragon,
        'classify': st.session_state.classify,
        'show_preview': st.session_state.show_preview,
        'save_video': st.session_state.save_video,
        'save_json': st.session_state.save_json,
        'save_txt': st.session_state.save_txt,
        'person_model': st.session_state.person_model,
        'dragon_model': st.session_state.dragon_model,
        'classify_model': st.session_state.classify_model,
        'node_size': st.session_state.node_size,
        'line_thickness': st.session_state.line_thickness,
        'confs': confs,
        'realtime_filter_method': st.session_state.realtime_filter_method,
        'smooth': st.session_state.smooth,
        'node_colors': node_colors,
        'line_color': line_color
    }

if __name__ == "__main__":
    main()


